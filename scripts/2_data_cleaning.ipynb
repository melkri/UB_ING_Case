{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from functions.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/source/df_joined.csv')\n",
    "df['default'] = df['default'].astype('int')\n",
    "df['obs_date'] = pd.to_datetime(df['obs_date'])\n",
    "df['year'] = df['obs_date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>obs_date</th>\n",
       "      <th>Var_01</th>\n",
       "      <th>Var_02</th>\n",
       "      <th>Var_03</th>\n",
       "      <th>Var_04</th>\n",
       "      <th>Var_05</th>\n",
       "      <th>Var_06</th>\n",
       "      <th>Var_07</th>\n",
       "      <th>Var_08</th>\n",
       "      <th>...</th>\n",
       "      <th>Var_32</th>\n",
       "      <th>Var_33</th>\n",
       "      <th>Var_34</th>\n",
       "      <th>Var_35</th>\n",
       "      <th>Var_36</th>\n",
       "      <th>Var_37</th>\n",
       "      <th>Var_38</th>\n",
       "      <th>Var_39</th>\n",
       "      <th>default</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6311599</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>1.222503e+05</td>\n",
       "      <td>1.319265e+05</td>\n",
       "      <td>8.437827e+04</td>\n",
       "      <td>4.452988</td>\n",
       "      <td>-0.573123</td>\n",
       "      <td>5.392172e+03</td>\n",
       "      <td>0.146342</td>\n",
       "      <td>4.465833e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.437827e+04</td>\n",
       "      <td>-1.196308</td>\n",
       "      <td>1.044730e+05</td>\n",
       "      <td>0.208097</td>\n",
       "      <td>-8.437827e+04</td>\n",
       "      <td>-1.685863</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.479673e+04</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25934835</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>3.889795e+05</td>\n",
       "      <td>4.281552e+05</td>\n",
       "      <td>5.713941e+04</td>\n",
       "      <td>1.517365</td>\n",
       "      <td>0.894176</td>\n",
       "      <td>1.031739e+04</td>\n",
       "      <td>0.056952</td>\n",
       "      <td>6.508156e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>7.471376e+04</td>\n",
       "      <td>0.622329</td>\n",
       "      <td>3.928746e+04</td>\n",
       "      <td>0.877436</td>\n",
       "      <td>7.471376e+04</td>\n",
       "      <td>0.990912</td>\n",
       "      <td>1.187002e+05</td>\n",
       "      <td>1.326275e+05</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70390679</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>1.489137e+09</td>\n",
       "      <td>1.520700e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.112577</td>\n",
       "      <td>0.303833</td>\n",
       "      <td>2.193759e+07</td>\n",
       "      <td>0.008544</td>\n",
       "      <td>1.155258e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>8.929345e+07</td>\n",
       "      <td>0.355066</td>\n",
       "      <td>1.606428e+08</td>\n",
       "      <td>0.894363</td>\n",
       "      <td>8.929345e+07</td>\n",
       "      <td>0.649580</td>\n",
       "      <td>5.724240e+08</td>\n",
       "      <td>1.506798e+08</td>\n",
       "      <td>0</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70390679</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>1.523974e+09</td>\n",
       "      <td>1.549976e+09</td>\n",
       "      <td>2.863042e+06</td>\n",
       "      <td>1.162289</td>\n",
       "      <td>0.088073</td>\n",
       "      <td>2.196899e+07</td>\n",
       "      <td>0.014428</td>\n",
       "      <td>1.877299e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>2.544141e+07</td>\n",
       "      <td>0.092185</td>\n",
       "      <td>2.194908e+08</td>\n",
       "      <td>0.858391</td>\n",
       "      <td>2.544141e+07</td>\n",
       "      <td>0.121324</td>\n",
       "      <td>1.172183e+09</td>\n",
       "      <td>2.127906e+08</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70390679</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>1.795857e+09</td>\n",
       "      <td>1.796012e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.323929</td>\n",
       "      <td>0.086474</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014659</td>\n",
       "      <td>2.086998e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>5.854743e+07</td>\n",
       "      <td>0.180772</td>\n",
       "      <td>4.362834e+08</td>\n",
       "      <td>0.757082</td>\n",
       "      <td>5.854743e+07</td>\n",
       "      <td>0.280534</td>\n",
       "      <td>5.106536e+08</td>\n",
       "      <td>4.393970e+08</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148924</th>\n",
       "      <td>18704864</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>1.569996e+09</td>\n",
       "      <td>1.570014e+09</td>\n",
       "      <td>1.842830e+08</td>\n",
       "      <td>1.457124</td>\n",
       "      <td>-0.128417</td>\n",
       "      <td>1.306725e+05</td>\n",
       "      <td>0.009390</td>\n",
       "      <td>1.336044e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.015468e+08</td>\n",
       "      <td>-0.472967</td>\n",
       "      <td>4.925521e+08</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>-1.015468e+08</td>\n",
       "      <td>-0.759314</td>\n",
       "      <td>9.371602e+08</td>\n",
       "      <td>4.925339e+08</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148925</th>\n",
       "      <td>18704864</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>2.031003e+09</td>\n",
       "      <td>2.031129e+09</td>\n",
       "      <td>9.294428e+07</td>\n",
       "      <td>1.173625</td>\n",
       "      <td>0.152625</td>\n",
       "      <td>2.003555e+05</td>\n",
       "      <td>0.007202</td>\n",
       "      <td>1.252715e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>8.459409e+07</td>\n",
       "      <td>0.365641</td>\n",
       "      <td>3.005910e+08</td>\n",
       "      <td>0.852008</td>\n",
       "      <td>8.459409e+07</td>\n",
       "      <td>0.674208</td>\n",
       "      <td>1.493421e+09</td>\n",
       "      <td>3.004651e+08</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148926</th>\n",
       "      <td>13920314</td>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>4.975412e+05</td>\n",
       "      <td>5.677995e+05</td>\n",
       "      <td>1.533735e+05</td>\n",
       "      <td>1.490486</td>\n",
       "      <td>-0.340115</td>\n",
       "      <td>7.535736e+03</td>\n",
       "      <td>0.042170</td>\n",
       "      <td>1.002575e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.533735e+05</td>\n",
       "      <td>-0.738292</td>\n",
       "      <td>2.339882e+05</td>\n",
       "      <td>0.587904</td>\n",
       "      <td>-1.533735e+05</td>\n",
       "      <td>-1.422849</td>\n",
       "      <td>1.521323e+05</td>\n",
       "      <td>1.637299e+05</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148927</th>\n",
       "      <td>13920314</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>7.674287e+05</td>\n",
       "      <td>7.879210e+05</td>\n",
       "      <td>2.727946e+05</td>\n",
       "      <td>1.270313</td>\n",
       "      <td>-1.587593</td>\n",
       "      <td>4.596592e+03</td>\n",
       "      <td>0.072011</td>\n",
       "      <td>2.153525e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.727946e+05</td>\n",
       "      <td>-0.679821</td>\n",
       "      <td>9.418410e+04</td>\n",
       "      <td>0.880465</td>\n",
       "      <td>-2.727946e+05</td>\n",
       "      <td>-1.240262</td>\n",
       "      <td>3.115334e+05</td>\n",
       "      <td>1.633029e+05</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148928</th>\n",
       "      <td>6311599</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>1.237196e+05</td>\n",
       "      <td>1.410654e+05</td>\n",
       "      <td>8.164743e+04</td>\n",
       "      <td>3.840063</td>\n",
       "      <td>-0.650695</td>\n",
       "      <td>6.359381e+03</td>\n",
       "      <td>0.160019</td>\n",
       "      <td>5.105622e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.164743e+04</td>\n",
       "      <td>-1.233575</td>\n",
       "      <td>1.088473e+05</td>\n",
       "      <td>0.228391</td>\n",
       "      <td>-8.164743e+04</td>\n",
       "      <td>-1.422043</td>\n",
       "      <td>5.803352e+02</td>\n",
       "      <td>9.150146e+04</td>\n",
       "      <td>0</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>148929 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID   obs_date        Var_01        Var_02        Var_03  \\\n",
       "0        6311599 2020-12-31  1.222503e+05  1.319265e+05  8.437827e+04   \n",
       "1       25934835 2015-12-31  3.889795e+05  4.281552e+05  5.713941e+04   \n",
       "2       70390679 2019-12-31  1.489137e+09  1.520700e+09           NaN   \n",
       "3       70390679 2020-12-31  1.523974e+09  1.549976e+09  2.863042e+06   \n",
       "4       70390679 2016-12-31  1.795857e+09  1.796012e+09           NaN   \n",
       "...          ...        ...           ...           ...           ...   \n",
       "148924  18704864 2020-12-31  1.569996e+09  1.570014e+09  1.842830e+08   \n",
       "148925  18704864 2019-12-31  2.031003e+09  2.031129e+09  9.294428e+07   \n",
       "148926  13920314 2017-12-31  4.975412e+05  5.677995e+05  1.533735e+05   \n",
       "148927  13920314 2016-12-31  7.674287e+05  7.879210e+05  2.727946e+05   \n",
       "148928   6311599 2019-12-31  1.237196e+05  1.410654e+05  8.164743e+04   \n",
       "\n",
       "          Var_04    Var_05        Var_06    Var_07        Var_08  ...  \\\n",
       "0       4.452988 -0.573123  5.392172e+03  0.146342  4.465833e+04  ...   \n",
       "1       1.517365  0.894176  1.031739e+04  0.056952  6.508156e+04  ...   \n",
       "2       1.112577  0.303833  2.193759e+07  0.008544  1.155258e+08  ...   \n",
       "3       1.162289  0.088073  2.196899e+07  0.014428  1.877299e+08  ...   \n",
       "4       1.323929  0.086474           NaN  0.014659  2.086998e+08  ...   \n",
       "...          ...       ...           ...       ...           ...  ...   \n",
       "148924  1.457124 -0.128417  1.306725e+05  0.009390  1.336044e+08  ...   \n",
       "148925  1.173625  0.152625  2.003555e+05  0.007202  1.252715e+08  ...   \n",
       "148926  1.490486 -0.340115  7.535736e+03  0.042170  1.002575e+05  ...   \n",
       "148927  1.270313 -1.587593  4.596592e+03  0.072011  2.153525e+05  ...   \n",
       "148928  3.840063 -0.650695  6.359381e+03  0.160019  5.105622e+04  ...   \n",
       "\n",
       "              Var_32    Var_33        Var_34    Var_35        Var_36  \\\n",
       "0      -8.437827e+04 -1.196308  1.044730e+05  0.208097 -8.437827e+04   \n",
       "1       7.471376e+04  0.622329  3.928746e+04  0.877436  7.471376e+04   \n",
       "2       8.929345e+07  0.355066  1.606428e+08  0.894363  8.929345e+07   \n",
       "3       2.544141e+07  0.092185  2.194908e+08  0.858391  2.544141e+07   \n",
       "4       5.854743e+07  0.180772  4.362834e+08  0.757082  5.854743e+07   \n",
       "...              ...       ...           ...       ...           ...   \n",
       "148924 -1.015468e+08 -0.472967  4.925521e+08  0.686275 -1.015468e+08   \n",
       "148925  8.459409e+07  0.365641  3.005910e+08  0.852008  8.459409e+07   \n",
       "148926 -1.533735e+05 -0.738292  2.339882e+05  0.587904 -1.533735e+05   \n",
       "148927 -2.727946e+05 -0.679821  9.418410e+04  0.880465 -2.727946e+05   \n",
       "148928 -8.164743e+04 -1.233575  1.088473e+05  0.228391 -8.164743e+04   \n",
       "\n",
       "          Var_37        Var_38        Var_39  default  year  \n",
       "0      -1.685863           NaN  9.479673e+04        0  2020  \n",
       "1       0.990912  1.187002e+05  1.326275e+05        0  2015  \n",
       "2       0.649580  5.724240e+08  1.506798e+08        0  2019  \n",
       "3       0.121324  1.172183e+09  2.127906e+08        0  2020  \n",
       "4       0.280534  5.106536e+08  4.393970e+08        0  2016  \n",
       "...          ...           ...           ...      ...   ...  \n",
       "148924 -0.759314  9.371602e+08  4.925339e+08        0  2020  \n",
       "148925  0.674208  1.493421e+09  3.004651e+08        1  2019  \n",
       "148926 -1.422849  1.521323e+05  1.637299e+05        0  2017  \n",
       "148927 -1.240262  3.115334e+05  1.633029e+05        0  2016  \n",
       "148928 -1.422043  5.803352e+02  9.150146e+04        0  2019  \n",
       "\n",
       "[148929 rows x 43 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do we need this? can we delete it?\n",
    "df_slice = df[df['ID']==1574]\n",
    "df_slice2 = df[df['ID']==9353]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kacpergruca/Documents/Studia/UW/4_semestr/Understanding Business/UB_ING_Case/scripts/functions/functions.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  no_default_indices['filter_condition'] = True\n"
     ]
    }
   ],
   "source": [
    "df_1 = filter_df(df)\n",
    "# df_1 = df_1[['ID', 'obs_date', 'cutoff_date', 'default', 'new_default']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>obs_date</th>\n",
       "      <th>Var_01</th>\n",
       "      <th>Var_02</th>\n",
       "      <th>Var_03</th>\n",
       "      <th>Var_04</th>\n",
       "      <th>Var_05</th>\n",
       "      <th>Var_06</th>\n",
       "      <th>Var_07</th>\n",
       "      <th>Var_08</th>\n",
       "      <th>...</th>\n",
       "      <th>Var_32</th>\n",
       "      <th>Var_33</th>\n",
       "      <th>Var_34</th>\n",
       "      <th>Var_35</th>\n",
       "      <th>Var_36</th>\n",
       "      <th>Var_37</th>\n",
       "      <th>Var_38</th>\n",
       "      <th>Var_39</th>\n",
       "      <th>default</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6689</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>1.767218e+09</td>\n",
       "      <td>1.948689e+10</td>\n",
       "      <td>2.329044e+08</td>\n",
       "      <td>0.734947</td>\n",
       "      <td>0.542522</td>\n",
       "      <td>1.714682e+09</td>\n",
       "      <td>0.384813</td>\n",
       "      <td>7.392133e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>8.425048e+09</td>\n",
       "      <td>2.074382</td>\n",
       "      <td>8.021040e+09</td>\n",
       "      <td>0.518512</td>\n",
       "      <td>8.425048e+09</td>\n",
       "      <td>3.433336</td>\n",
       "      <td>5.710332e+08</td>\n",
       "      <td>-6.373332e+08</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6689</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>3.232833e+09</td>\n",
       "      <td>2.160626e+10</td>\n",
       "      <td>1.899846e+09</td>\n",
       "      <td>1.105073</td>\n",
       "      <td>0.523663</td>\n",
       "      <td>1.638684e+09</td>\n",
       "      <td>0.372563</td>\n",
       "      <td>1.021945e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>8.252083e+09</td>\n",
       "      <td>1.937487</td>\n",
       "      <td>8.448764e+09</td>\n",
       "      <td>0.544391</td>\n",
       "      <td>8.252083e+09</td>\n",
       "      <td>3.101552</td>\n",
       "      <td>5.956984e+08</td>\n",
       "      <td>3.073853e+08</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>9353</td>\n",
       "      <td>2015-03-31</td>\n",
       "      <td>2.586283e+09</td>\n",
       "      <td>1.063976e+10</td>\n",
       "      <td>4.099697e+07</td>\n",
       "      <td>0.814620</td>\n",
       "      <td>0.547254</td>\n",
       "      <td>1.038821e+09</td>\n",
       "      <td>0.352002</td>\n",
       "      <td>2.454954e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>3.011540e+09</td>\n",
       "      <td>0.598954</td>\n",
       "      <td>2.348640e+09</td>\n",
       "      <td>0.640609</td>\n",
       "      <td>3.011540e+09</td>\n",
       "      <td>0.861973</td>\n",
       "      <td>9.297278e+08</td>\n",
       "      <td>-5.885497e+08</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>9353</td>\n",
       "      <td>2016-03-31</td>\n",
       "      <td>3.068856e+09</td>\n",
       "      <td>1.282924e+10</td>\n",
       "      <td>4.147103e+07</td>\n",
       "      <td>0.744966</td>\n",
       "      <td>0.785927</td>\n",
       "      <td>1.297698e+09</td>\n",
       "      <td>0.354587</td>\n",
       "      <td>3.170806e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>3.897413e+09</td>\n",
       "      <td>0.753560</td>\n",
       "      <td>2.729312e+09</td>\n",
       "      <td>0.666038</td>\n",
       "      <td>3.897413e+09</td>\n",
       "      <td>0.872196</td>\n",
       "      <td>1.389280e+09</td>\n",
       "      <td>-1.050599e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>9459</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>3.104298e+09</td>\n",
       "      <td>4.894807e+09</td>\n",
       "      <td>6.749521e+08</td>\n",
       "      <td>1.147094</td>\n",
       "      <td>-0.310680</td>\n",
       "      <td>1.532888e+08</td>\n",
       "      <td>0.062220</td>\n",
       "      <td>6.821751e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.605060e+08</td>\n",
       "      <td>-0.634492</td>\n",
       "      <td>1.012027e+09</td>\n",
       "      <td>0.651418</td>\n",
       "      <td>-6.605060e+08</td>\n",
       "      <td>-0.790586</td>\n",
       "      <td>1.162908e+09</td>\n",
       "      <td>3.980693e+08</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128338</th>\n",
       "      <td>99991694</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>6.192680e+05</td>\n",
       "      <td>2.533073e+06</td>\n",
       "      <td>1.471271e+05</td>\n",
       "      <td>1.708610</td>\n",
       "      <td>-0.026182</td>\n",
       "      <td>9.833619e+03</td>\n",
       "      <td>0.064670</td>\n",
       "      <td>1.122458e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000410e+05</td>\n",
       "      <td>-0.465534</td>\n",
       "      <td>2.761724e+05</td>\n",
       "      <td>0.143083</td>\n",
       "      <td>-1.000410e+05</td>\n",
       "      <td>-0.819475</td>\n",
       "      <td>1.655752e+05</td>\n",
       "      <td>2.568284e+05</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24213</th>\n",
       "      <td>99991954</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>4.354878e+07</td>\n",
       "      <td>5.113584e+07</td>\n",
       "      <td>8.003650e+05</td>\n",
       "      <td>2.846886</td>\n",
       "      <td>-2.546194</td>\n",
       "      <td>4.679316e+05</td>\n",
       "      <td>-6.612175</td>\n",
       "      <td>-1.673950e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>5.351337e+07</td>\n",
       "      <td>-2.767837</td>\n",
       "      <td>-1.768798e+07</td>\n",
       "      <td>1.345902</td>\n",
       "      <td>5.351337e+07</td>\n",
       "      <td>-3.288766</td>\n",
       "      <td>3.360523e+06</td>\n",
       "      <td>2.825179e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24215</th>\n",
       "      <td>99991954</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>1.798610e+07</td>\n",
       "      <td>2.201727e+07</td>\n",
       "      <td>8.853989e+03</td>\n",
       "      <td>0.397986</td>\n",
       "      <td>-0.526652</td>\n",
       "      <td>1.008313e+06</td>\n",
       "      <td>-0.629559</td>\n",
       "      <td>-1.082530e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>2.343495e+07</td>\n",
       "      <td>-1.243299</td>\n",
       "      <td>-2.317558e+07</td>\n",
       "      <td>2.052609</td>\n",
       "      <td>2.343495e+07</td>\n",
       "      <td>-2.387182</td>\n",
       "      <td>3.530658e+06</td>\n",
       "      <td>-2.720675e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24214</th>\n",
       "      <td>99991954</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>1.267978e+07</td>\n",
       "      <td>1.571147e+07</td>\n",
       "      <td>1.110184e+05</td>\n",
       "      <td>0.266830</td>\n",
       "      <td>-0.524300</td>\n",
       "      <td>1.040206e+06</td>\n",
       "      <td>-0.320276</td>\n",
       "      <td>-7.251975e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>3.169654e+07</td>\n",
       "      <td>-2.684783</td>\n",
       "      <td>-3.180862e+07</td>\n",
       "      <td>3.024547</td>\n",
       "      <td>3.169654e+07</td>\n",
       "      <td>-5.102660</td>\n",
       "      <td>5.735074e+04</td>\n",
       "      <td>-3.484031e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120832</th>\n",
       "      <td>99992121</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>4.541582e+05</td>\n",
       "      <td>7.378146e+05</td>\n",
       "      <td>2.625302e+04</td>\n",
       "      <td>1.867904</td>\n",
       "      <td>0.252903</td>\n",
       "      <td>2.529092e+04</td>\n",
       "      <td>0.111240</td>\n",
       "      <td>9.058881e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>9.183798e+04</td>\n",
       "      <td>0.644536</td>\n",
       "      <td>2.867858e+05</td>\n",
       "      <td>0.599730</td>\n",
       "      <td>9.183798e+04</td>\n",
       "      <td>0.792528</td>\n",
       "      <td>1.038426e+05</td>\n",
       "      <td>2.110204e+05</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125758 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID   obs_date        Var_01        Var_02        Var_03  \\\n",
       "15          6689 2015-12-31  1.767218e+09  1.948689e+10  2.329044e+08   \n",
       "16          6689 2016-12-31  3.232833e+09  2.160626e+10  1.899846e+09   \n",
       "22          9353 2015-03-31  2.586283e+09  1.063976e+10  4.099697e+07   \n",
       "23          9353 2016-03-31  3.068856e+09  1.282924e+10  4.147103e+07   \n",
       "26          9459 2015-12-31  3.104298e+09  4.894807e+09  6.749521e+08   \n",
       "...          ...        ...           ...           ...           ...   \n",
       "128338  99991694 2015-12-31  6.192680e+05  2.533073e+06  1.471271e+05   \n",
       "24213   99991954 2018-12-31  4.354878e+07  5.113584e+07  8.003650e+05   \n",
       "24215   99991954 2019-12-31  1.798610e+07  2.201727e+07  8.853989e+03   \n",
       "24214   99991954 2020-12-31  1.267978e+07  1.571147e+07  1.110184e+05   \n",
       "120832  99992121 2015-12-31  4.541582e+05  7.378146e+05  2.625302e+04   \n",
       "\n",
       "          Var_04    Var_05        Var_06    Var_07        Var_08  ...  \\\n",
       "15      0.734947  0.542522  1.714682e+09  0.384813  7.392133e+08  ...   \n",
       "16      1.105073  0.523663  1.638684e+09  0.372563  1.021945e+09  ...   \n",
       "22      0.814620  0.547254  1.038821e+09  0.352002  2.454954e+09  ...   \n",
       "23      0.744966  0.785927  1.297698e+09  0.354587  3.170806e+09  ...   \n",
       "26      1.147094 -0.310680  1.532888e+08  0.062220  6.821751e+08  ...   \n",
       "...          ...       ...           ...       ...           ...  ...   \n",
       "128338  1.708610 -0.026182  9.833619e+03  0.064670  1.122458e+05  ...   \n",
       "24213   2.846886 -2.546194  4.679316e+05 -6.612175 -1.673950e+07  ...   \n",
       "24215   0.397986 -0.526652  1.008313e+06 -0.629559 -1.082530e+07  ...   \n",
       "24214   0.266830 -0.524300  1.040206e+06 -0.320276 -7.251975e+06  ...   \n",
       "120832  1.867904  0.252903  2.529092e+04  0.111240  9.058881e+04  ...   \n",
       "\n",
       "              Var_32    Var_33        Var_34    Var_35        Var_36  \\\n",
       "15      8.425048e+09  2.074382  8.021040e+09  0.518512  8.425048e+09   \n",
       "16      8.252083e+09  1.937487  8.448764e+09  0.544391  8.252083e+09   \n",
       "22      3.011540e+09  0.598954  2.348640e+09  0.640609  3.011540e+09   \n",
       "23      3.897413e+09  0.753560  2.729312e+09  0.666038  3.897413e+09   \n",
       "26     -6.605060e+08 -0.634492  1.012027e+09  0.651418 -6.605060e+08   \n",
       "...              ...       ...           ...       ...           ...   \n",
       "128338 -1.000410e+05 -0.465534  2.761724e+05  0.143083 -1.000410e+05   \n",
       "24213   5.351337e+07 -2.767837 -1.768798e+07  1.345902  5.351337e+07   \n",
       "24215   2.343495e+07 -1.243299 -2.317558e+07  2.052609  2.343495e+07   \n",
       "24214   3.169654e+07 -2.684783 -3.180862e+07  3.024547  3.169654e+07   \n",
       "120832  9.183798e+04  0.644536  2.867858e+05  0.599730  9.183798e+04   \n",
       "\n",
       "          Var_37        Var_38        Var_39  default  year  \n",
       "15      3.433336  5.710332e+08 -6.373332e+08        0  2015  \n",
       "16      3.101552  5.956984e+08  3.073853e+08        1  2016  \n",
       "22      0.861973  9.297278e+08 -5.885497e+08        0  2015  \n",
       "23      0.872196  1.389280e+09 -1.050599e+09        1  2016  \n",
       "26     -0.790586  1.162908e+09  3.980693e+08        1  2015  \n",
       "...          ...           ...           ...      ...   ...  \n",
       "128338 -0.819475  1.655752e+05  2.568284e+05        0  2015  \n",
       "24213  -3.288766  3.360523e+06  2.825179e+07        0  2018  \n",
       "24215  -2.387182  3.530658e+06 -2.720675e+07        0  2019  \n",
       "24214  -5.102660  5.735074e+04 -3.484031e+07        0  2020  \n",
       "120832  0.792528  1.038426e+05  2.110204e+05        0  2015  \n",
       "\n",
       "[125758 rows x 43 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check default ratio in every year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year\n",
       "2015    0.155059\n",
       "2016    0.067632\n",
       "2017    0.050681\n",
       "2018    0.043920\n",
       "2019    0.044501\n",
       "2020    0.040701\n",
       "2021    0.048115\n",
       "Name: default, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.groupby('year')['default'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_columns = [f'Var_{i:02d}' for i in range(1, 40)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = preprocess_data(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>obs_date</th>\n",
       "      <th>Var_01</th>\n",
       "      <th>Var_02</th>\n",
       "      <th>Var_03</th>\n",
       "      <th>Var_04</th>\n",
       "      <th>Var_05</th>\n",
       "      <th>Var_06</th>\n",
       "      <th>Var_07</th>\n",
       "      <th>Var_08</th>\n",
       "      <th>...</th>\n",
       "      <th>Var_32</th>\n",
       "      <th>Var_33</th>\n",
       "      <th>Var_34</th>\n",
       "      <th>Var_35</th>\n",
       "      <th>Var_36</th>\n",
       "      <th>Var_37</th>\n",
       "      <th>Var_38</th>\n",
       "      <th>Var_39</th>\n",
       "      <th>default</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6689</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>1.767218e+09</td>\n",
       "      <td>1.948689e+10</td>\n",
       "      <td>2.329044e+08</td>\n",
       "      <td>0.734947</td>\n",
       "      <td>0.542522</td>\n",
       "      <td>1.714682e+09</td>\n",
       "      <td>0.384813</td>\n",
       "      <td>7.392133e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>8.425048e+09</td>\n",
       "      <td>2.074382</td>\n",
       "      <td>8.021040e+09</td>\n",
       "      <td>0.518512</td>\n",
       "      <td>8.425048e+09</td>\n",
       "      <td>3.433336</td>\n",
       "      <td>5.710332e+08</td>\n",
       "      <td>-6.373332e+08</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6689</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>3.232833e+09</td>\n",
       "      <td>2.160626e+10</td>\n",
       "      <td>1.899846e+09</td>\n",
       "      <td>1.105073</td>\n",
       "      <td>0.523663</td>\n",
       "      <td>1.638684e+09</td>\n",
       "      <td>0.372563</td>\n",
       "      <td>1.021945e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>8.252083e+09</td>\n",
       "      <td>1.937487</td>\n",
       "      <td>8.448764e+09</td>\n",
       "      <td>0.544391</td>\n",
       "      <td>8.252083e+09</td>\n",
       "      <td>3.101552</td>\n",
       "      <td>5.956984e+08</td>\n",
       "      <td>3.073853e+08</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>9353</td>\n",
       "      <td>2015-03-31</td>\n",
       "      <td>2.586283e+09</td>\n",
       "      <td>1.063976e+10</td>\n",
       "      <td>4.099697e+07</td>\n",
       "      <td>0.814620</td>\n",
       "      <td>0.547254</td>\n",
       "      <td>1.038821e+09</td>\n",
       "      <td>0.352002</td>\n",
       "      <td>2.454954e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>3.011540e+09</td>\n",
       "      <td>0.598954</td>\n",
       "      <td>2.348640e+09</td>\n",
       "      <td>0.640609</td>\n",
       "      <td>3.011540e+09</td>\n",
       "      <td>0.861973</td>\n",
       "      <td>9.297278e+08</td>\n",
       "      <td>-5.885497e+08</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>9353</td>\n",
       "      <td>2016-03-31</td>\n",
       "      <td>3.068856e+09</td>\n",
       "      <td>1.282924e+10</td>\n",
       "      <td>4.147103e+07</td>\n",
       "      <td>0.744966</td>\n",
       "      <td>0.785927</td>\n",
       "      <td>1.297698e+09</td>\n",
       "      <td>0.354587</td>\n",
       "      <td>3.170806e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>3.897413e+09</td>\n",
       "      <td>0.753560</td>\n",
       "      <td>2.729312e+09</td>\n",
       "      <td>0.666038</td>\n",
       "      <td>3.897413e+09</td>\n",
       "      <td>0.872196</td>\n",
       "      <td>1.389280e+09</td>\n",
       "      <td>-1.050599e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>9459</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>3.104298e+09</td>\n",
       "      <td>4.894807e+09</td>\n",
       "      <td>6.749521e+08</td>\n",
       "      <td>1.147094</td>\n",
       "      <td>-0.310680</td>\n",
       "      <td>1.532888e+08</td>\n",
       "      <td>0.062220</td>\n",
       "      <td>6.821751e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.605060e+08</td>\n",
       "      <td>-0.634492</td>\n",
       "      <td>1.012027e+09</td>\n",
       "      <td>0.651418</td>\n",
       "      <td>-6.605060e+08</td>\n",
       "      <td>-0.790586</td>\n",
       "      <td>1.162908e+09</td>\n",
       "      <td>3.980693e+08</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128338</th>\n",
       "      <td>99991694</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>6.192680e+05</td>\n",
       "      <td>2.533073e+06</td>\n",
       "      <td>1.471271e+05</td>\n",
       "      <td>1.708610</td>\n",
       "      <td>-0.026182</td>\n",
       "      <td>9.833619e+03</td>\n",
       "      <td>0.064670</td>\n",
       "      <td>1.122458e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000410e+05</td>\n",
       "      <td>-0.465534</td>\n",
       "      <td>2.761724e+05</td>\n",
       "      <td>0.143083</td>\n",
       "      <td>-1.000410e+05</td>\n",
       "      <td>-0.819475</td>\n",
       "      <td>1.655752e+05</td>\n",
       "      <td>2.568284e+05</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24213</th>\n",
       "      <td>99991954</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>4.354878e+07</td>\n",
       "      <td>5.113584e+07</td>\n",
       "      <td>8.003650e+05</td>\n",
       "      <td>2.846886</td>\n",
       "      <td>-2.546194</td>\n",
       "      <td>4.679316e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>5.351337e+07</td>\n",
       "      <td>-2.767837</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.345902</td>\n",
       "      <td>5.351337e+07</td>\n",
       "      <td>-3.288766</td>\n",
       "      <td>3.360523e+06</td>\n",
       "      <td>2.825179e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24215</th>\n",
       "      <td>99991954</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>1.798610e+07</td>\n",
       "      <td>2.201727e+07</td>\n",
       "      <td>8.853989e+03</td>\n",
       "      <td>0.397986</td>\n",
       "      <td>-0.526652</td>\n",
       "      <td>1.008313e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.343495e+07</td>\n",
       "      <td>-1.243299</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.052609</td>\n",
       "      <td>2.343495e+07</td>\n",
       "      <td>-2.387182</td>\n",
       "      <td>3.530658e+06</td>\n",
       "      <td>-2.720675e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24214</th>\n",
       "      <td>99991954</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>1.267978e+07</td>\n",
       "      <td>1.571147e+07</td>\n",
       "      <td>1.110184e+05</td>\n",
       "      <td>0.266830</td>\n",
       "      <td>-0.524300</td>\n",
       "      <td>1.040206e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>3.169654e+07</td>\n",
       "      <td>-2.684783</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.024547</td>\n",
       "      <td>3.169654e+07</td>\n",
       "      <td>-5.102660</td>\n",
       "      <td>5.735074e+04</td>\n",
       "      <td>-3.484031e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120832</th>\n",
       "      <td>99992121</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>4.541582e+05</td>\n",
       "      <td>7.378146e+05</td>\n",
       "      <td>2.625302e+04</td>\n",
       "      <td>1.867904</td>\n",
       "      <td>0.252903</td>\n",
       "      <td>2.529092e+04</td>\n",
       "      <td>0.111240</td>\n",
       "      <td>9.058881e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>9.183798e+04</td>\n",
       "      <td>0.644536</td>\n",
       "      <td>2.867858e+05</td>\n",
       "      <td>0.599730</td>\n",
       "      <td>9.183798e+04</td>\n",
       "      <td>0.792528</td>\n",
       "      <td>1.038426e+05</td>\n",
       "      <td>2.110204e+05</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125758 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID   obs_date        Var_01        Var_02        Var_03  \\\n",
       "15          6689 2015-12-31  1.767218e+09  1.948689e+10  2.329044e+08   \n",
       "16          6689 2016-12-31  3.232833e+09  2.160626e+10  1.899846e+09   \n",
       "22          9353 2015-03-31  2.586283e+09  1.063976e+10  4.099697e+07   \n",
       "23          9353 2016-03-31  3.068856e+09  1.282924e+10  4.147103e+07   \n",
       "26          9459 2015-12-31  3.104298e+09  4.894807e+09  6.749521e+08   \n",
       "...          ...        ...           ...           ...           ...   \n",
       "128338  99991694 2015-12-31  6.192680e+05  2.533073e+06  1.471271e+05   \n",
       "24213   99991954 2018-12-31  4.354878e+07  5.113584e+07  8.003650e+05   \n",
       "24215   99991954 2019-12-31  1.798610e+07  2.201727e+07  8.853989e+03   \n",
       "24214   99991954 2020-12-31  1.267978e+07  1.571147e+07  1.110184e+05   \n",
       "120832  99992121 2015-12-31  4.541582e+05  7.378146e+05  2.625302e+04   \n",
       "\n",
       "          Var_04    Var_05        Var_06    Var_07        Var_08  ...  \\\n",
       "15      0.734947  0.542522  1.714682e+09  0.384813  7.392133e+08  ...   \n",
       "16      1.105073  0.523663  1.638684e+09  0.372563  1.021945e+09  ...   \n",
       "22      0.814620  0.547254  1.038821e+09  0.352002  2.454954e+09  ...   \n",
       "23      0.744966  0.785927  1.297698e+09  0.354587  3.170806e+09  ...   \n",
       "26      1.147094 -0.310680  1.532888e+08  0.062220  6.821751e+08  ...   \n",
       "...          ...       ...           ...       ...           ...  ...   \n",
       "128338  1.708610 -0.026182  9.833619e+03  0.064670  1.122458e+05  ...   \n",
       "24213   2.846886 -2.546194  4.679316e+05  0.000000  0.000000e+00  ...   \n",
       "24215   0.397986 -0.526652  1.008313e+06  0.000000  0.000000e+00  ...   \n",
       "24214   0.266830 -0.524300  1.040206e+06  0.000000  0.000000e+00  ...   \n",
       "120832  1.867904  0.252903  2.529092e+04  0.111240  9.058881e+04  ...   \n",
       "\n",
       "              Var_32    Var_33        Var_34    Var_35        Var_36  \\\n",
       "15      8.425048e+09  2.074382  8.021040e+09  0.518512  8.425048e+09   \n",
       "16      8.252083e+09  1.937487  8.448764e+09  0.544391  8.252083e+09   \n",
       "22      3.011540e+09  0.598954  2.348640e+09  0.640609  3.011540e+09   \n",
       "23      3.897413e+09  0.753560  2.729312e+09  0.666038  3.897413e+09   \n",
       "26     -6.605060e+08 -0.634492  1.012027e+09  0.651418 -6.605060e+08   \n",
       "...              ...       ...           ...       ...           ...   \n",
       "128338 -1.000410e+05 -0.465534  2.761724e+05  0.143083 -1.000410e+05   \n",
       "24213   5.351337e+07 -2.767837  0.000000e+00  1.345902  5.351337e+07   \n",
       "24215   2.343495e+07 -1.243299  0.000000e+00  2.052609  2.343495e+07   \n",
       "24214   3.169654e+07 -2.684783  0.000000e+00  3.024547  3.169654e+07   \n",
       "120832  9.183798e+04  0.644536  2.867858e+05  0.599730  9.183798e+04   \n",
       "\n",
       "          Var_37        Var_38        Var_39  default  year  \n",
       "15      3.433336  5.710332e+08 -6.373332e+08        0  2015  \n",
       "16      3.101552  5.956984e+08  3.073853e+08        1  2016  \n",
       "22      0.861973  9.297278e+08 -5.885497e+08        0  2015  \n",
       "23      0.872196  1.389280e+09 -1.050599e+09        1  2016  \n",
       "26     -0.790586  1.162908e+09  3.980693e+08        1  2015  \n",
       "...          ...           ...           ...      ...   ...  \n",
       "128338 -0.819475  1.655752e+05  2.568284e+05        0  2015  \n",
       "24213  -3.288766  3.360523e+06  2.825179e+07        0  2018  \n",
       "24215  -2.387182  3.530658e+06 -2.720675e+07        0  2019  \n",
       "24214  -5.102660  5.735074e+04 -3.484031e+07        0  2020  \n",
       "120832  0.792528  1.038426e+05  2.110204e+05        0  2015  \n",
       "\n",
       "[125758 rows x 43 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill missing values with KNN imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = KNNImputer(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed = pd.DataFrame(imputer.fit_transform(df_1[var_columns]), columns=var_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_column = df_1['default'].reset_index(drop=True)\n",
    "\n",
    "# Assign the reset index column to df_imputed\n",
    "df_imputed['default'] = default_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight of evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binned = binning_with_decision_tree(df_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_woe = calculate_woe(df_binned, target='default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Var_01_woe</th>\n",
       "      <th>Var_02_woe</th>\n",
       "      <th>Var_03_woe</th>\n",
       "      <th>Var_04_woe</th>\n",
       "      <th>Var_05_woe</th>\n",
       "      <th>Var_06_woe</th>\n",
       "      <th>Var_07_woe</th>\n",
       "      <th>Var_08_woe</th>\n",
       "      <th>Var_09_woe</th>\n",
       "      <th>Var_10_woe</th>\n",
       "      <th>...</th>\n",
       "      <th>Var_31_woe</th>\n",
       "      <th>Var_32_woe</th>\n",
       "      <th>Var_33_woe</th>\n",
       "      <th>Var_34_woe</th>\n",
       "      <th>Var_35_woe</th>\n",
       "      <th>Var_36_woe</th>\n",
       "      <th>Var_37_woe</th>\n",
       "      <th>Var_38_woe</th>\n",
       "      <th>Var_39_woe</th>\n",
       "      <th>default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.602029</td>\n",
       "      <td>2.161178</td>\n",
       "      <td>1.601304</td>\n",
       "      <td>-0.242243</td>\n",
       "      <td>0.050486</td>\n",
       "      <td>2.558108</td>\n",
       "      <td>-0.096086</td>\n",
       "      <td>1.927932</td>\n",
       "      <td>2.507456</td>\n",
       "      <td>3.059200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663223</td>\n",
       "      <td>1.725419</td>\n",
       "      <td>-0.000282</td>\n",
       "      <td>3.066939</td>\n",
       "      <td>0.119413</td>\n",
       "      <td>1.722072</td>\n",
       "      <td>-0.090043</td>\n",
       "      <td>1.529446</td>\n",
       "      <td>0.435364</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.989977</td>\n",
       "      <td>2.161178</td>\n",
       "      <td>2.382690</td>\n",
       "      <td>-0.242243</td>\n",
       "      <td>0.050486</td>\n",
       "      <td>2.558108</td>\n",
       "      <td>-0.096086</td>\n",
       "      <td>2.747950</td>\n",
       "      <td>2.507456</td>\n",
       "      <td>3.059200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663223</td>\n",
       "      <td>1.725419</td>\n",
       "      <td>-0.000282</td>\n",
       "      <td>3.066939</td>\n",
       "      <td>0.119413</td>\n",
       "      <td>1.722072</td>\n",
       "      <td>-0.090043</td>\n",
       "      <td>1.529446</td>\n",
       "      <td>2.381269</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.989977</td>\n",
       "      <td>2.161178</td>\n",
       "      <td>1.172870</td>\n",
       "      <td>-0.242243</td>\n",
       "      <td>0.050486</td>\n",
       "      <td>2.558108</td>\n",
       "      <td>-0.096086</td>\n",
       "      <td>2.747950</td>\n",
       "      <td>2.507456</td>\n",
       "      <td>2.288942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178431</td>\n",
       "      <td>1.725419</td>\n",
       "      <td>-0.247608</td>\n",
       "      <td>2.305629</td>\n",
       "      <td>-0.065835</td>\n",
       "      <td>1.722072</td>\n",
       "      <td>-0.090043</td>\n",
       "      <td>1.822981</td>\n",
       "      <td>0.435364</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.989977</td>\n",
       "      <td>2.161178</td>\n",
       "      <td>1.172870</td>\n",
       "      <td>-0.242243</td>\n",
       "      <td>-0.128911</td>\n",
       "      <td>2.558108</td>\n",
       "      <td>-0.096086</td>\n",
       "      <td>2.747950</td>\n",
       "      <td>2.507456</td>\n",
       "      <td>2.288942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178431</td>\n",
       "      <td>1.725419</td>\n",
       "      <td>-0.000282</td>\n",
       "      <td>2.305629</td>\n",
       "      <td>-0.065835</td>\n",
       "      <td>1.722072</td>\n",
       "      <td>-0.090043</td>\n",
       "      <td>1.822981</td>\n",
       "      <td>0.435364</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.989977</td>\n",
       "      <td>1.785738</td>\n",
       "      <td>2.382690</td>\n",
       "      <td>-0.242243</td>\n",
       "      <td>-0.467310</td>\n",
       "      <td>1.559653</td>\n",
       "      <td>-0.096086</td>\n",
       "      <td>1.927932</td>\n",
       "      <td>1.907468</td>\n",
       "      <td>1.900164</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.787109</td>\n",
       "      <td>2.743854</td>\n",
       "      <td>-0.247608</td>\n",
       "      <td>1.906589</td>\n",
       "      <td>-0.065835</td>\n",
       "      <td>2.906220</td>\n",
       "      <td>-0.311843</td>\n",
       "      <td>1.822981</td>\n",
       "      <td>2.381269</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125753</th>\n",
       "      <td>1.602029</td>\n",
       "      <td>1.508321</td>\n",
       "      <td>1.601304</td>\n",
       "      <td>-0.242243</td>\n",
       "      <td>0.044977</td>\n",
       "      <td>-2.377479</td>\n",
       "      <td>-0.096086</td>\n",
       "      <td>1.927932</td>\n",
       "      <td>1.907468</td>\n",
       "      <td>1.900164</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.787109</td>\n",
       "      <td>1.849329</td>\n",
       "      <td>-0.247608</td>\n",
       "      <td>1.906589</td>\n",
       "      <td>-0.065835</td>\n",
       "      <td>1.859636</td>\n",
       "      <td>-0.311843</td>\n",
       "      <td>1.822981</td>\n",
       "      <td>2.381269</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125754</th>\n",
       "      <td>1.602029</td>\n",
       "      <td>1.508321</td>\n",
       "      <td>1.601304</td>\n",
       "      <td>-0.242243</td>\n",
       "      <td>0.218554</td>\n",
       "      <td>-0.704081</td>\n",
       "      <td>-0.096086</td>\n",
       "      <td>1.213114</td>\n",
       "      <td>1.293746</td>\n",
       "      <td>1.567715</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.787109</td>\n",
       "      <td>1.053915</td>\n",
       "      <td>-0.247608</td>\n",
       "      <td>1.573936</td>\n",
       "      <td>-0.231364</td>\n",
       "      <td>1.053113</td>\n",
       "      <td>-0.090043</td>\n",
       "      <td>1.822981</td>\n",
       "      <td>1.788023</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125755</th>\n",
       "      <td>-2.744596</td>\n",
       "      <td>-8.788252</td>\n",
       "      <td>-2.027166</td>\n",
       "      <td>0.001806</td>\n",
       "      <td>-0.467310</td>\n",
       "      <td>-2.377479</td>\n",
       "      <td>-0.096086</td>\n",
       "      <td>-2.905559</td>\n",
       "      <td>-3.122933</td>\n",
       "      <td>-3.475616</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.787109</td>\n",
       "      <td>-2.063711</td>\n",
       "      <td>-0.247608</td>\n",
       "      <td>-3.409683</td>\n",
       "      <td>-0.065835</td>\n",
       "      <td>-2.072218</td>\n",
       "      <td>0.091239</td>\n",
       "      <td>-2.049761</td>\n",
       "      <td>-2.721174</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125756</th>\n",
       "      <td>-2.744596</td>\n",
       "      <td>-8.788252</td>\n",
       "      <td>-2.027166</td>\n",
       "      <td>-0.242243</td>\n",
       "      <td>-0.467310</td>\n",
       "      <td>-2.377479</td>\n",
       "      <td>-0.096086</td>\n",
       "      <td>-2.905559</td>\n",
       "      <td>-3.122933</td>\n",
       "      <td>-3.475616</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.787109</td>\n",
       "      <td>-2.063711</td>\n",
       "      <td>-0.247608</td>\n",
       "      <td>-3.409683</td>\n",
       "      <td>-0.231364</td>\n",
       "      <td>-2.072218</td>\n",
       "      <td>-0.311843</td>\n",
       "      <td>-2.049761</td>\n",
       "      <td>-2.721174</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125757</th>\n",
       "      <td>-2.744596</td>\n",
       "      <td>-8.788252</td>\n",
       "      <td>-2.027166</td>\n",
       "      <td>0.413951</td>\n",
       "      <td>-0.467310</td>\n",
       "      <td>-2.377479</td>\n",
       "      <td>-0.096086</td>\n",
       "      <td>-2.905559</td>\n",
       "      <td>-3.122933</td>\n",
       "      <td>-3.475616</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.787109</td>\n",
       "      <td>-2.063711</td>\n",
       "      <td>0.187335</td>\n",
       "      <td>-3.409683</td>\n",
       "      <td>0.376593</td>\n",
       "      <td>-2.072218</td>\n",
       "      <td>0.091239</td>\n",
       "      <td>-2.049761</td>\n",
       "      <td>-2.721174</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125758 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Var_01_woe  Var_02_woe  Var_03_woe  Var_04_woe  Var_05_woe  \\\n",
       "0         1.602029    2.161178    1.601304   -0.242243    0.050486   \n",
       "1         1.989977    2.161178    2.382690   -0.242243    0.050486   \n",
       "2         1.989977    2.161178    1.172870   -0.242243    0.050486   \n",
       "3         1.989977    2.161178    1.172870   -0.242243   -0.128911   \n",
       "4         1.989977    1.785738    2.382690   -0.242243   -0.467310   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "125753    1.602029    1.508321    1.601304   -0.242243    0.044977   \n",
       "125754    1.602029    1.508321    1.601304   -0.242243    0.218554   \n",
       "125755   -2.744596   -8.788252   -2.027166    0.001806   -0.467310   \n",
       "125756   -2.744596   -8.788252   -2.027166   -0.242243   -0.467310   \n",
       "125757   -2.744596   -8.788252   -2.027166    0.413951   -0.467310   \n",
       "\n",
       "        Var_06_woe  Var_07_woe  Var_08_woe  Var_09_woe  Var_10_woe  ...  \\\n",
       "0         2.558108   -0.096086    1.927932    2.507456    3.059200  ...   \n",
       "1         2.558108   -0.096086    2.747950    2.507456    3.059200  ...   \n",
       "2         2.558108   -0.096086    2.747950    2.507456    2.288942  ...   \n",
       "3         2.558108   -0.096086    2.747950    2.507456    2.288942  ...   \n",
       "4         1.559653   -0.096086    1.927932    1.907468    1.900164  ...   \n",
       "...            ...         ...         ...         ...         ...  ...   \n",
       "125753   -2.377479   -0.096086    1.927932    1.907468    1.900164  ...   \n",
       "125754   -0.704081   -0.096086    1.213114    1.293746    1.567715  ...   \n",
       "125755   -2.377479   -0.096086   -2.905559   -3.122933   -3.475616  ...   \n",
       "125756   -2.377479   -0.096086   -2.905559   -3.122933   -3.475616  ...   \n",
       "125757   -2.377479   -0.096086   -2.905559   -3.122933   -3.475616  ...   \n",
       "\n",
       "        Var_31_woe  Var_32_woe  Var_33_woe  Var_34_woe  Var_35_woe  \\\n",
       "0         0.663223    1.725419   -0.000282    3.066939    0.119413   \n",
       "1         0.663223    1.725419   -0.000282    3.066939    0.119413   \n",
       "2         0.178431    1.725419   -0.247608    2.305629   -0.065835   \n",
       "3         0.178431    1.725419   -0.000282    2.305629   -0.065835   \n",
       "4        -0.787109    2.743854   -0.247608    1.906589   -0.065835   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "125753   -0.787109    1.849329   -0.247608    1.906589   -0.065835   \n",
       "125754   -0.787109    1.053915   -0.247608    1.573936   -0.231364   \n",
       "125755   -0.787109   -2.063711   -0.247608   -3.409683   -0.065835   \n",
       "125756   -0.787109   -2.063711   -0.247608   -3.409683   -0.231364   \n",
       "125757   -0.787109   -2.063711    0.187335   -3.409683    0.376593   \n",
       "\n",
       "        Var_36_woe  Var_37_woe  Var_38_woe  Var_39_woe  default  \n",
       "0         1.722072   -0.090043    1.529446    0.435364        0  \n",
       "1         1.722072   -0.090043    1.529446    2.381269        1  \n",
       "2         1.722072   -0.090043    1.822981    0.435364        0  \n",
       "3         1.722072   -0.090043    1.822981    0.435364        1  \n",
       "4         2.906220   -0.311843    1.822981    2.381269        1  \n",
       "...            ...         ...         ...         ...      ...  \n",
       "125753    1.859636   -0.311843    1.822981    2.381269        0  \n",
       "125754    1.053113   -0.090043    1.822981    1.788023        0  \n",
       "125755   -2.072218    0.091239   -2.049761   -2.721174        0  \n",
       "125756   -2.072218   -0.311843   -2.049761   -2.721174        0  \n",
       "125757   -2.072218    0.091239   -2.049761   -2.721174        0  \n",
       "\n",
       "[125758 rows x 40 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_woe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_woe.to_csv('../data/processed/df_woe.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_woe = pd.read_csv('../data/processed/df_woe.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Population stability index (PSI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation for PSI calculation in order to check the colinearity. \n",
    "df_woe_time = df_woe\n",
    "df_woe_time['year'] = df_1['year'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "psi_results = calculate_psi(df_woe_time, time_column='year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PSI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Var_01_woe</th>\n",
       "      <td>0.036774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var_02_woe</th>\n",
       "      <td>0.035857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var_03_woe</th>\n",
       "      <td>0.029598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var_04_woe</th>\n",
       "      <td>0.007033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var_05_woe</th>\n",
       "      <td>0.003115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 PSI\n",
       "Var_01_woe  0.036774\n",
       "Var_02_woe  0.035857\n",
       "Var_03_woe  0.029598\n",
       "Var_04_woe  0.007033\n",
       "Var_05_woe  0.003115"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psi_results.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check colinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/stats/outliers_influence.py:198: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/stats/outliers_influence.py:198: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/stats/outliers_influence.py:198: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_VIF_woe \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_collinearity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_woe\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[68], line 9\u001b[0m, in \u001b[0;36mcheck_collinearity\u001b[0;34m(df, threshold)\u001b[0m\n\u001b[1;32m      7\u001b[0m vif_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m      8\u001b[0m vif_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariable\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m----> 9\u001b[0m vif_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVIF\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mvariance_inflation_factor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Find variables with high collinearity\u001b[39;00m\n\u001b[1;32m     12\u001b[0m high_collinearity_vars \u001b[38;5;241m=\u001b[39m vif_data[vif_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVIF\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m threshold]\n",
      "Cell \u001b[0;32mIn[68], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m vif_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m      8\u001b[0m vif_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariable\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m----> 9\u001b[0m vif_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVIF\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[43mvariance_inflation_factor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Find variables with high collinearity\u001b[39;00m\n\u001b[1;32m     12\u001b[0m high_collinearity_vars \u001b[38;5;241m=\u001b[39m vif_data[vif_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVIF\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m threshold]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/stats/outliers_influence.py:197\u001b[0m, in \u001b[0;36mvariance_inflation_factor\u001b[0;34m(exog, exog_idx)\u001b[0m\n\u001b[1;32m    195\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(k_vars) \u001b[38;5;241m!=\u001b[39m exog_idx\n\u001b[1;32m    196\u001b[0m x_noti \u001b[38;5;241m=\u001b[39m exog[:, mask]\n\u001b[0;32m--> 197\u001b[0m r_squared_i \u001b[38;5;241m=\u001b[39m \u001b[43mOLS\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_noti\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfit()\u001b[38;5;241m.\u001b[39mrsquared\n\u001b[1;32m    198\u001b[0m vif \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m-\u001b[39m r_squared_i)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vif\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/regression/linear_model.py:922\u001b[0m, in \u001b[0;36mOLS.__init__\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    919\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights are not supported in OLS and will be ignored\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    920\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn exception will be raised in the next version.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    921\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg, ValueWarning)\n\u001b[0;32m--> 922\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mOLS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_keys:\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_keys\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/regression/linear_model.py:748\u001b[0m, in \u001b[0;36mWLS.__init__\u001b[0;34m(self, endog, exog, weights, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    747\u001b[0m     weights \u001b[38;5;241m=\u001b[39m weights\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m--> 748\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mWLS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    750\u001b[0m nobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    751\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/regression/linear_model.py:202\u001b[0m, in \u001b[0;36mRegressionModel.__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mRegressionModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpinv_wexog: Float64Array \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_attr\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpinv_wexog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwendog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwexog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/base/model.py:270\u001b[0m, in \u001b[0;36mLikelihoodModel.__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/base/model.py:95\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m missing \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     94\u001b[0m hasconst \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhasconst\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_constant \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mk_constant\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mexog\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/base/model.py:135\u001b[0m, in \u001b[0;36mModel._handle_data\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_handle_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, missing, hasconst, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 135\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mhandle_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# kwargs arrays could have changed, easier to just attach here\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/base/data.py:675\u001b[0m, in \u001b[0;36mhandle_data\u001b[0;34m(endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    672\u001b[0m     exog \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(exog)\n\u001b[1;32m    674\u001b[0m klass \u001b[38;5;241m=\u001b[39m handle_data_class_factory(endog, exog)\n\u001b[0;32m--> 675\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mklass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m             \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/base/data.py:88\u001b[0m, in \u001b[0;36mModelData.__init__\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconst_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_constant \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_constant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity()\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/statsmodels/base/data.py:178\u001b[0m, in \u001b[0;36mModelData._handle_constant\u001b[0;34m(self, hasconst)\u001b[0m\n\u001b[1;32m    175\u001b[0m augmented_exog \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcolumn_stack(\n\u001b[1;32m    176\u001b[0m             (np\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog))\n\u001b[1;32m    177\u001b[0m rank_augm \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mmatrix_rank(augmented_exog)\n\u001b[0;32m--> 178\u001b[0m rank_orig \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatrix_rank\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexog\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_constant \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(rank_orig \u001b[38;5;241m==\u001b[39m rank_augm)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconst_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mmatrix_rank\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/linalg/linalg.py:1883\u001b[0m, in \u001b[0;36mmatrix_rank\u001b[0;34m(A, tol, hermitian)\u001b[0m\n\u001b[1;32m   1881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1882\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(A\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m-> 1883\u001b[0m S \u001b[38;5;241m=\u001b[39m \u001b[43msvd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_uv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhermitian\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhermitian\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1885\u001b[0m     tol \u001b[38;5;241m=\u001b[39m S\u001b[38;5;241m.\u001b[39mmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mmax\u001b[39m(A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]) \u001b[38;5;241m*\u001b[39m finfo(S\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39meps\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36msvd\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/linalg/linalg.py:1654\u001b[0m, in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, hermitian)\u001b[0m\n\u001b[1;32m   1651\u001b[0m     gufunc \u001b[38;5;241m=\u001b[39m _umath_linalg\u001b[38;5;241m.\u001b[39msvd_n\n\u001b[1;32m   1653\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->d\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1654\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[43mgufunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1655\u001b[0m s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mastype(_realType(result_t), copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m s\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_VIF_woe = check_collinearity(df_woe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "        'penalty': ['l1', 'l2'],  # Regularization penalty\n",
    "        'C': [0.001, 0.01, 0.1, 1],  # Inverse of regularization strength\n",
    "        'solver': ['liblinear', 'saga'],  # Algorithm to use in the optimization problem\n",
    "        'max_iter': [100, 500, 1000],  # Maximum number of iterations\n",
    "        'class_weight': [None, 'balanced']  # Weights associated with classes\n",
    "    }\n",
    "\n",
    "model = LogisticRegression(random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to drop after VIW calculation\n",
    "cols_to_drop = ['Var_02_woe', 'Var_01_woe', 'Var_11_woe', 'Var_18_woe', 'Var_29_woe', 'Var_36_woe', 'Var_34_woe', 'Var_22_woe', 'Var_12_woe', 'Var_16_woe', 'Var_19_woe', 'Var_30_woe', 'Var_09_woe', 'Var_21_woe', 'Var_17_woe', 'Var_14_woe', 'Var_26_woe', 'Var_32_woe', 'Var_39_woe', 'Var_24_woe', 'Var_06_woe', 'Var_38_woe', 'Var_10_woe', 'Var_20_woe']\n",
    "df_VIF_woe = df_woe.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_VIF_woe.to_csv('../data/processed/df_VIF_woe.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate Logistic regresison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "[CV 1/5] END C=0.001, class_weight=None, max_iter=100, penalty=l1, solver=liblinear;, score=0.917 total time=   0.1s\n",
      "[CV 2/5] END C=0.001, class_weight=None, max_iter=100, penalty=l1, solver=liblinear;, score=0.912 total time=   0.2s\n",
      "[CV 3/5] END C=0.001, class_weight=None, max_iter=100, penalty=l1, solver=liblinear;, score=0.916 total time=   0.1s\n",
      "[CV 4/5] END C=0.001, class_weight=None, max_iter=100, penalty=l1, solver=liblinear;, score=0.910 total time=   0.2s\n",
      "[CV 5/5] END C=0.001, class_weight=None, max_iter=100, penalty=l1, solver=liblinear;, score=0.910 total time=   0.2s\n",
      "[CV 1/5] END C=0.001, class_weight=None, max_iter=100, penalty=l1, solver=saga;, score=0.912 total time=   0.4s\n",
      "[CV 2/5] END C=0.001, class_weight=None, max_iter=100, penalty=l1, solver=saga;, score=0.907 total time=   0.3s\n",
      "[CV 3/5] END C=0.001, class_weight=None, max_iter=100, penalty=l1, solver=saga;, score=0.911 total time=   0.3s\n",
      "[CV 4/5] END C=0.001, class_weight=None, max_iter=100, penalty=l1, solver=saga;, score=0.905 total time=   0.4s\n",
      "[CV 5/5] END C=0.001, class_weight=None, max_iter=100, penalty=l1, solver=saga;, score=0.904 total time=   0.4s\n",
      "[CV 1/5] END C=0.001, class_weight=None, max_iter=100, penalty=l2, solver=liblinear;, score=0.939 total time=   0.1s\n",
      "[CV 2/5] END C=0.001, class_weight=None, max_iter=100, penalty=l2, solver=liblinear;, score=0.934 total time=   0.1s\n",
      "[CV 3/5] END C=0.001, class_weight=None, max_iter=100, penalty=l2, solver=liblinear;, score=0.939 total time=   0.1s\n",
      "[CV 4/5] END C=0.001, class_weight=None, max_iter=100, penalty=l2, solver=liblinear;, score=0.931 total time=   0.2s\n",
      "[CV 5/5] END C=0.001, class_weight=None, max_iter=100, penalty=l2, solver=liblinear;, score=0.933 total time=   0.2s\n",
      "[CV 1/5] END C=0.001, class_weight=None, max_iter=100, penalty=l2, solver=saga;, score=0.937 total time=   0.3s\n",
      "[CV 2/5] END C=0.001, class_weight=None, max_iter=100, penalty=l2, solver=saga;, score=0.932 total time=   0.3s\n",
      "[CV 3/5] END C=0.001, class_weight=None, max_iter=100, penalty=l2, solver=saga;, score=0.937 total time=   0.3s\n",
      "[CV 4/5] END C=0.001, class_weight=None, max_iter=100, penalty=l2, solver=saga;, score=0.929 total time=   0.3s\n",
      "[CV 5/5] END C=0.001, class_weight=None, max_iter=100, penalty=l2, solver=saga;, score=0.931 total time=   0.3s\n",
      "[CV 1/5] END C=0.001, class_weight=None, max_iter=500, penalty=l1, solver=liblinear;, score=0.917 total time=   0.1s\n",
      "[CV 2/5] END C=0.001, class_weight=None, max_iter=500, penalty=l1, solver=liblinear;, score=0.912 total time=   0.1s\n",
      "[CV 3/5] END C=0.001, class_weight=None, max_iter=500, penalty=l1, solver=liblinear;, score=0.916 total time=   0.1s\n",
      "[CV 4/5] END C=0.001, class_weight=None, max_iter=500, penalty=l1, solver=liblinear;, score=0.910 total time=   0.1s\n",
      "[CV 5/5] END C=0.001, class_weight=None, max_iter=500, penalty=l1, solver=liblinear;, score=0.910 total time=   0.1s\n",
      "[CV 1/5] END C=0.001, class_weight=None, max_iter=500, penalty=l1, solver=saga;, score=0.912 total time=   0.4s\n",
      "[CV 2/5] END C=0.001, class_weight=None, max_iter=500, penalty=l1, solver=saga;, score=0.907 total time=   0.4s\n",
      "[CV 3/5] END C=0.001, class_weight=None, max_iter=500, penalty=l1, solver=saga;, score=0.911 total time=   0.4s\n",
      "[CV 4/5] END C=0.001, class_weight=None, max_iter=500, penalty=l1, solver=saga;, score=0.905 total time=   0.4s\n",
      "[CV 5/5] END C=0.001, class_weight=None, max_iter=500, penalty=l1, solver=saga;, score=0.904 total time=   0.4s\n",
      "[CV 1/5] END C=0.001, class_weight=None, max_iter=500, penalty=l2, solver=liblinear;, score=0.939 total time=   0.2s\n",
      "[CV 2/5] END C=0.001, class_weight=None, max_iter=500, penalty=l2, solver=liblinear;, score=0.934 total time=   0.1s\n",
      "[CV 3/5] END C=0.001, class_weight=None, max_iter=500, penalty=l2, solver=liblinear;, score=0.939 total time=   0.1s\n",
      "[CV 4/5] END C=0.001, class_weight=None, max_iter=500, penalty=l2, solver=liblinear;, score=0.931 total time=   0.1s\n",
      "[CV 5/5] END C=0.001, class_weight=None, max_iter=500, penalty=l2, solver=liblinear;, score=0.933 total time=   0.1s\n",
      "[CV 1/5] END C=0.001, class_weight=None, max_iter=500, penalty=l2, solver=saga;, score=0.937 total time=   0.3s\n",
      "[CV 2/5] END C=0.001, class_weight=None, max_iter=500, penalty=l2, solver=saga;, score=0.932 total time=   0.2s\n",
      "[CV 3/5] END C=0.001, class_weight=None, max_iter=500, penalty=l2, solver=saga;, score=0.937 total time=   0.3s\n",
      "[CV 4/5] END C=0.001, class_weight=None, max_iter=500, penalty=l2, solver=saga;, score=0.929 total time=   0.4s\n",
      "[CV 5/5] END C=0.001, class_weight=None, max_iter=500, penalty=l2, solver=saga;, score=0.931 total time=   0.3s\n",
      "[CV 1/5] END C=0.001, class_weight=None, max_iter=1000, penalty=l1, solver=liblinear;, score=0.917 total time=   0.1s\n",
      "[CV 2/5] END C=0.001, class_weight=None, max_iter=1000, penalty=l1, solver=liblinear;, score=0.912 total time=   0.1s\n",
      "[CV 3/5] END C=0.001, class_weight=None, max_iter=1000, penalty=l1, solver=liblinear;, score=0.916 total time=   0.1s\n",
      "[CV 4/5] END C=0.001, class_weight=None, max_iter=1000, penalty=l1, solver=liblinear;, score=0.910 total time=   0.1s\n",
      "[CV 5/5] END C=0.001, class_weight=None, max_iter=1000, penalty=l1, solver=liblinear;, score=0.910 total time=   0.1s\n",
      "[CV 1/5] END C=0.001, class_weight=None, max_iter=1000, penalty=l1, solver=saga;, score=0.912 total time=   0.4s\n",
      "[CV 2/5] END C=0.001, class_weight=None, max_iter=1000, penalty=l1, solver=saga;, score=0.907 total time=   0.3s\n",
      "[CV 3/5] END C=0.001, class_weight=None, max_iter=1000, penalty=l1, solver=saga;, score=0.911 total time=   0.4s\n",
      "[CV 4/5] END C=0.001, class_weight=None, max_iter=1000, penalty=l1, solver=saga;, score=0.905 total time=   0.4s\n",
      "[CV 5/5] END C=0.001, class_weight=None, max_iter=1000, penalty=l1, solver=saga;, score=0.904 total time=   0.4s\n",
      "[CV 1/5] END C=0.001, class_weight=None, max_iter=1000, penalty=l2, solver=liblinear;, score=0.939 total time=   0.1s\n",
      "[CV 2/5] END C=0.001, class_weight=None, max_iter=1000, penalty=l2, solver=liblinear;, score=0.934 total time=   0.1s\n",
      "[CV 3/5] END C=0.001, class_weight=None, max_iter=1000, penalty=l2, solver=liblinear;, score=0.939 total time=   0.1s\n",
      "[CV 4/5] END C=0.001, class_weight=None, max_iter=1000, penalty=l2, solver=liblinear;, score=0.931 total time=   0.3s\n",
      "[CV 5/5] END C=0.001, class_weight=None, max_iter=1000, penalty=l2, solver=liblinear;, score=0.933 total time=   0.2s\n",
      "[CV 1/5] END C=0.001, class_weight=None, max_iter=1000, penalty=l2, solver=saga;, score=0.937 total time=   0.3s\n",
      "[CV 2/5] END C=0.001, class_weight=None, max_iter=1000, penalty=l2, solver=saga;, score=0.932 total time=   0.3s\n",
      "[CV 3/5] END C=0.001, class_weight=None, max_iter=1000, penalty=l2, solver=saga;, score=0.937 total time=   0.3s\n",
      "[CV 4/5] END C=0.001, class_weight=None, max_iter=1000, penalty=l2, solver=saga;, score=0.929 total time=   0.3s\n",
      "[CV 5/5] END C=0.001, class_weight=None, max_iter=1000, penalty=l2, solver=saga;, score=0.931 total time=   0.3s\n",
      "[CV 1/5] END C=0.001, class_weight=balanced, max_iter=100, penalty=l1, solver=liblinear;, score=0.931 total time=   0.1s\n",
      "[CV 2/5] END C=0.001, class_weight=balanced, max_iter=100, penalty=l1, solver=liblinear;, score=0.927 total time=   0.1s\n",
      "[CV 3/5] END C=0.001, class_weight=balanced, max_iter=100, penalty=l1, solver=liblinear;, score=0.931 total time=   0.1s\n",
      "[CV 4/5] END C=0.001, class_weight=balanced, max_iter=100, penalty=l1, solver=liblinear;, score=0.923 total time=   0.1s\n",
      "[CV 5/5] END C=0.001, class_weight=balanced, max_iter=100, penalty=l1, solver=liblinear;, score=0.924 total time=   0.1s\n",
      "[CV 1/5] END C=0.001, class_weight=balanced, max_iter=100, penalty=l1, solver=saga;, score=0.931 total time=   0.4s\n",
      "[CV 2/5] END C=0.001, class_weight=balanced, max_iter=100, penalty=l1, solver=saga;, score=0.927 total time=   0.4s\n",
      "[CV 3/5] END C=0.001, class_weight=balanced, max_iter=100, penalty=l1, solver=saga;, score=0.931 total time=   0.4s\n",
      "[CV 4/5] END C=0.001, class_weight=balanced, max_iter=100, penalty=l1, solver=saga;, score=0.923 total time=   0.4s\n",
      "[CV 5/5] END C=0.001, class_weight=balanced, max_iter=100, penalty=l1, solver=saga;, score=0.924 total time=   0.4s\n",
      "[CV 1/5] END C=0.001, class_weight=balanced, max_iter=100, penalty=l2, solver=liblinear;, score=0.939 total time=   0.2s\n",
      "[CV 2/5] END C=0.001, class_weight=balanced, max_iter=100, penalty=l2, solver=liblinear;, score=0.935 total time=   0.3s\n",
      "[CV 3/5] END C=0.001, class_weight=balanced, max_iter=100, penalty=l2, solver=liblinear;, score=0.939 total time=   0.2s\n",
      "[CV 4/5] END C=0.001, class_weight=balanced, max_iter=100, penalty=l2, solver=liblinear;, score=0.932 total time=   0.2s\n",
      "[CV 5/5] END C=0.001, class_weight=balanced, max_iter=100, penalty=l2, solver=liblinear;, score=0.934 total time=   0.2s\n",
      "[CV 1/5] END C=0.001, class_weight=balanced, max_iter=100, penalty=l2, solver=saga;, score=0.939 total time=   0.4s\n",
      "[CV 2/5] END C=0.001, class_weight=balanced, max_iter=100, penalty=l2, solver=saga;, score=0.935 total time=   0.5s\n",
      "[CV 3/5] END C=0.001, class_weight=balanced, max_iter=100, penalty=l2, solver=saga;, score=0.939 total time=   0.4s\n",
      "[CV 4/5] END C=0.001, class_weight=balanced, max_iter=100, penalty=l2, solver=saga;, score=0.932 total time=   0.3s\n",
      "[CV 5/5] END C=0.001, class_weight=balanced, max_iter=100, penalty=l2, solver=saga;, score=0.934 total time=   0.4s\n",
      "[CV 1/5] END C=0.001, class_weight=balanced, max_iter=500, penalty=l1, solver=liblinear;, score=0.931 total time=   0.1s\n",
      "[CV 2/5] END C=0.001, class_weight=balanced, max_iter=500, penalty=l1, solver=liblinear;, score=0.927 total time=   0.1s\n",
      "[CV 3/5] END C=0.001, class_weight=balanced, max_iter=500, penalty=l1, solver=liblinear;, score=0.931 total time=   0.1s\n",
      "[CV 4/5] END C=0.001, class_weight=balanced, max_iter=500, penalty=l1, solver=liblinear;, score=0.923 total time=   0.1s\n",
      "[CV 5/5] END C=0.001, class_weight=balanced, max_iter=500, penalty=l1, solver=liblinear;, score=0.924 total time=   0.1s\n",
      "[CV 1/5] END C=0.001, class_weight=balanced, max_iter=500, penalty=l1, solver=saga;, score=0.931 total time=   0.4s\n",
      "[CV 2/5] END C=0.001, class_weight=balanced, max_iter=500, penalty=l1, solver=saga;, score=0.927 total time=   0.4s\n",
      "[CV 3/5] END C=0.001, class_weight=balanced, max_iter=500, penalty=l1, solver=saga;, score=0.931 total time=   0.4s\n",
      "[CV 4/5] END C=0.001, class_weight=balanced, max_iter=500, penalty=l1, solver=saga;, score=0.923 total time=   0.4s\n",
      "[CV 5/5] END C=0.001, class_weight=balanced, max_iter=500, penalty=l1, solver=saga;, score=0.924 total time=   0.4s\n",
      "[CV 1/5] END C=0.001, class_weight=balanced, max_iter=500, penalty=l2, solver=liblinear;, score=0.939 total time=   0.1s\n",
      "[CV 2/5] END C=0.001, class_weight=balanced, max_iter=500, penalty=l2, solver=liblinear;, score=0.935 total time=   0.1s\n",
      "[CV 3/5] END C=0.001, class_weight=balanced, max_iter=500, penalty=l2, solver=liblinear;, score=0.939 total time=   0.2s\n",
      "[CV 4/5] END C=0.001, class_weight=balanced, max_iter=500, penalty=l2, solver=liblinear;, score=0.932 total time=   0.2s\n",
      "[CV 5/5] END C=0.001, class_weight=balanced, max_iter=500, penalty=l2, solver=liblinear;, score=0.934 total time=   0.2s\n",
      "[CV 1/5] END C=0.001, class_weight=balanced, max_iter=500, penalty=l2, solver=saga;, score=0.939 total time=   0.4s\n",
      "[CV 2/5] END C=0.001, class_weight=balanced, max_iter=500, penalty=l2, solver=saga;, score=0.935 total time=   0.4s\n",
      "[CV 3/5] END C=0.001, class_weight=balanced, max_iter=500, penalty=l2, solver=saga;, score=0.939 total time=   0.4s\n",
      "[CV 4/5] END C=0.001, class_weight=balanced, max_iter=500, penalty=l2, solver=saga;, score=0.932 total time=   0.3s\n",
      "[CV 5/5] END C=0.001, class_weight=balanced, max_iter=500, penalty=l2, solver=saga;, score=0.934 total time=   0.3s\n",
      "[CV 1/5] END C=0.001, class_weight=balanced, max_iter=1000, penalty=l1, solver=liblinear;, score=0.931 total time=   0.1s\n",
      "[CV 2/5] END C=0.001, class_weight=balanced, max_iter=1000, penalty=l1, solver=liblinear;, score=0.927 total time=   0.1s\n",
      "[CV 3/5] END C=0.001, class_weight=balanced, max_iter=1000, penalty=l1, solver=liblinear;, score=0.931 total time=   0.1s\n",
      "[CV 4/5] END C=0.001, class_weight=balanced, max_iter=1000, penalty=l1, solver=liblinear;, score=0.923 total time=   0.1s\n",
      "[CV 5/5] END C=0.001, class_weight=balanced, max_iter=1000, penalty=l1, solver=liblinear;, score=0.924 total time=   0.1s\n",
      "[CV 1/5] END C=0.001, class_weight=balanced, max_iter=1000, penalty=l1, solver=saga;, score=0.931 total time=   0.4s\n",
      "[CV 2/5] END C=0.001, class_weight=balanced, max_iter=1000, penalty=l1, solver=saga;, score=0.927 total time=   0.4s\n",
      "[CV 3/5] END C=0.001, class_weight=balanced, max_iter=1000, penalty=l1, solver=saga;, score=0.931 total time=   0.5s\n",
      "[CV 4/5] END C=0.001, class_weight=balanced, max_iter=1000, penalty=l1, solver=saga;, score=0.923 total time=   0.3s\n",
      "[CV 5/5] END C=0.001, class_weight=balanced, max_iter=1000, penalty=l1, solver=saga;, score=0.924 total time=   0.3s\n",
      "[CV 1/5] END C=0.001, class_weight=balanced, max_iter=1000, penalty=l2, solver=liblinear;, score=0.939 total time=   0.1s\n",
      "[CV 2/5] END C=0.001, class_weight=balanced, max_iter=1000, penalty=l2, solver=liblinear;, score=0.935 total time=   0.1s\n",
      "[CV 3/5] END C=0.001, class_weight=balanced, max_iter=1000, penalty=l2, solver=liblinear;, score=0.939 total time=   0.3s\n",
      "[CV 4/5] END C=0.001, class_weight=balanced, max_iter=1000, penalty=l2, solver=liblinear;, score=0.932 total time=   0.1s\n",
      "[CV 5/5] END C=0.001, class_weight=balanced, max_iter=1000, penalty=l2, solver=liblinear;, score=0.934 total time=   0.2s\n",
      "[CV 1/5] END C=0.001, class_weight=balanced, max_iter=1000, penalty=l2, solver=saga;, score=0.939 total time=   0.4s\n",
      "[CV 2/5] END C=0.001, class_weight=balanced, max_iter=1000, penalty=l2, solver=saga;, score=0.935 total time=   0.3s\n",
      "[CV 3/5] END C=0.001, class_weight=balanced, max_iter=1000, penalty=l2, solver=saga;, score=0.939 total time=   0.4s\n",
      "[CV 4/5] END C=0.001, class_weight=balanced, max_iter=1000, penalty=l2, solver=saga;, score=0.932 total time=   0.5s\n",
      "[CV 5/5] END C=0.001, class_weight=balanced, max_iter=1000, penalty=l2, solver=saga;, score=0.934 total time=   0.3s\n",
      "[CV 1/5] END C=0.01, class_weight=None, max_iter=100, penalty=l1, solver=liblinear;, score=0.942 total time=   0.1s\n",
      "[CV 2/5] END C=0.01, class_weight=None, max_iter=100, penalty=l1, solver=liblinear;, score=0.937 total time=   0.2s\n",
      "[CV 3/5] END C=0.01, class_weight=None, max_iter=100, penalty=l1, solver=liblinear;, score=0.941 total time=   0.2s\n",
      "[CV 4/5] END C=0.01, class_weight=None, max_iter=100, penalty=l1, solver=liblinear;, score=0.934 total time=   0.3s\n",
      "[CV 5/5] END C=0.01, class_weight=None, max_iter=100, penalty=l1, solver=liblinear;, score=0.935 total time=   0.2s\n",
      "[CV 1/5] END C=0.01, class_weight=None, max_iter=100, penalty=l1, solver=saga;, score=0.942 total time=   0.3s\n",
      "[CV 2/5] END C=0.01, class_weight=None, max_iter=100, penalty=l1, solver=saga;, score=0.937 total time=   0.3s\n",
      "[CV 3/5] END C=0.01, class_weight=None, max_iter=100, penalty=l1, solver=saga;, score=0.942 total time=   0.3s\n",
      "[CV 4/5] END C=0.01, class_weight=None, max_iter=100, penalty=l1, solver=saga;, score=0.934 total time=   0.3s\n",
      "[CV 5/5] END C=0.01, class_weight=None, max_iter=100, penalty=l1, solver=saga;, score=0.935 total time=   0.3s\n",
      "[CV 1/5] END C=0.01, class_weight=None, max_iter=100, penalty=l2, solver=liblinear;, score=0.943 total time=   0.2s\n",
      "[CV 2/5] END C=0.01, class_weight=None, max_iter=100, penalty=l2, solver=liblinear;, score=0.938 total time=   0.2s\n",
      "[CV 3/5] END C=0.01, class_weight=None, max_iter=100, penalty=l2, solver=liblinear;, score=0.943 total time=   0.2s\n",
      "[CV 4/5] END C=0.01, class_weight=None, max_iter=100, penalty=l2, solver=liblinear;, score=0.936 total time=   0.2s\n",
      "[CV 5/5] END C=0.01, class_weight=None, max_iter=100, penalty=l2, solver=liblinear;, score=0.937 total time=   0.3s\n",
      "[CV 1/5] END C=0.01, class_weight=None, max_iter=100, penalty=l2, solver=saga;, score=0.943 total time=   0.3s\n",
      "[CV 2/5] END C=0.01, class_weight=None, max_iter=100, penalty=l2, solver=saga;, score=0.938 total time=   0.3s\n",
      "[CV 3/5] END C=0.01, class_weight=None, max_iter=100, penalty=l2, solver=saga;, score=0.943 total time=   0.2s\n",
      "[CV 4/5] END C=0.01, class_weight=None, max_iter=100, penalty=l2, solver=saga;, score=0.936 total time=   0.2s\n",
      "[CV 5/5] END C=0.01, class_weight=None, max_iter=100, penalty=l2, solver=saga;, score=0.937 total time=   0.3s\n",
      "[CV 1/5] END C=0.01, class_weight=None, max_iter=500, penalty=l1, solver=liblinear;, score=0.942 total time=   0.2s\n",
      "[CV 2/5] END C=0.01, class_weight=None, max_iter=500, penalty=l1, solver=liblinear;, score=0.937 total time=   0.2s\n",
      "[CV 3/5] END C=0.01, class_weight=None, max_iter=500, penalty=l1, solver=liblinear;, score=0.941 total time=   0.2s\n",
      "[CV 4/5] END C=0.01, class_weight=None, max_iter=500, penalty=l1, solver=liblinear;, score=0.934 total time=   0.2s\n",
      "[CV 5/5] END C=0.01, class_weight=None, max_iter=500, penalty=l1, solver=liblinear;, score=0.935 total time=   0.1s\n",
      "[CV 1/5] END C=0.01, class_weight=None, max_iter=500, penalty=l1, solver=saga;, score=0.942 total time=   0.3s\n",
      "[CV 2/5] END C=0.01, class_weight=None, max_iter=500, penalty=l1, solver=saga;, score=0.937 total time=   0.3s\n",
      "[CV 3/5] END C=0.01, class_weight=None, max_iter=500, penalty=l1, solver=saga;, score=0.942 total time=   0.3s\n",
      "[CV 4/5] END C=0.01, class_weight=None, max_iter=500, penalty=l1, solver=saga;, score=0.934 total time=   0.3s\n",
      "[CV 5/5] END C=0.01, class_weight=None, max_iter=500, penalty=l1, solver=saga;, score=0.935 total time=   0.3s\n",
      "[CV 1/5] END C=0.01, class_weight=None, max_iter=500, penalty=l2, solver=liblinear;, score=0.943 total time=   0.2s\n",
      "[CV 2/5] END C=0.01, class_weight=None, max_iter=500, penalty=l2, solver=liblinear;, score=0.938 total time=   0.2s\n",
      "[CV 3/5] END C=0.01, class_weight=None, max_iter=500, penalty=l2, solver=liblinear;, score=0.943 total time=   0.2s\n",
      "[CV 4/5] END C=0.01, class_weight=None, max_iter=500, penalty=l2, solver=liblinear;, score=0.936 total time=   0.2s\n",
      "[CV 5/5] END C=0.01, class_weight=None, max_iter=500, penalty=l2, solver=liblinear;, score=0.937 total time=   0.2s\n",
      "[CV 1/5] END C=0.01, class_weight=None, max_iter=500, penalty=l2, solver=saga;, score=0.943 total time=   0.4s\n",
      "[CV 2/5] END C=0.01, class_weight=None, max_iter=500, penalty=l2, solver=saga;, score=0.938 total time=   0.4s\n",
      "[CV 3/5] END C=0.01, class_weight=None, max_iter=500, penalty=l2, solver=saga;, score=0.943 total time=   0.4s\n",
      "[CV 4/5] END C=0.01, class_weight=None, max_iter=500, penalty=l2, solver=saga;, score=0.936 total time=   0.3s\n",
      "[CV 5/5] END C=0.01, class_weight=None, max_iter=500, penalty=l2, solver=saga;, score=0.937 total time=   0.4s\n",
      "[CV 1/5] END C=0.01, class_weight=None, max_iter=1000, penalty=l1, solver=liblinear;, score=0.942 total time=   0.1s\n",
      "[CV 2/5] END C=0.01, class_weight=None, max_iter=1000, penalty=l1, solver=liblinear;, score=0.937 total time=   0.3s\n",
      "[CV 3/5] END C=0.01, class_weight=None, max_iter=1000, penalty=l1, solver=liblinear;, score=0.941 total time=   0.2s\n",
      "[CV 4/5] END C=0.01, class_weight=None, max_iter=1000, penalty=l1, solver=liblinear;, score=0.934 total time=   0.2s\n",
      "[CV 5/5] END C=0.01, class_weight=None, max_iter=1000, penalty=l1, solver=liblinear;, score=0.935 total time=   0.2s\n",
      "[CV 1/5] END C=0.01, class_weight=None, max_iter=1000, penalty=l1, solver=saga;, score=0.942 total time=   0.3s\n",
      "[CV 2/5] END C=0.01, class_weight=None, max_iter=1000, penalty=l1, solver=saga;, score=0.937 total time=   0.3s\n",
      "[CV 3/5] END C=0.01, class_weight=None, max_iter=1000, penalty=l1, solver=saga;, score=0.942 total time=   0.4s\n",
      "[CV 4/5] END C=0.01, class_weight=None, max_iter=1000, penalty=l1, solver=saga;, score=0.934 total time=   0.3s\n",
      "[CV 5/5] END C=0.01, class_weight=None, max_iter=1000, penalty=l1, solver=saga;, score=0.935 total time=   0.4s\n",
      "[CV 1/5] END C=0.01, class_weight=None, max_iter=1000, penalty=l2, solver=liblinear;, score=0.943 total time=   0.3s\n",
      "[CV 2/5] END C=0.01, class_weight=None, max_iter=1000, penalty=l2, solver=liblinear;, score=0.938 total time=   0.2s\n",
      "[CV 3/5] END C=0.01, class_weight=None, max_iter=1000, penalty=l2, solver=liblinear;, score=0.943 total time=   0.2s\n",
      "[CV 4/5] END C=0.01, class_weight=None, max_iter=1000, penalty=l2, solver=liblinear;, score=0.936 total time=   0.3s\n",
      "[CV 5/5] END C=0.01, class_weight=None, max_iter=1000, penalty=l2, solver=liblinear;, score=0.937 total time=   0.3s\n",
      "[CV 1/5] END C=0.01, class_weight=None, max_iter=1000, penalty=l2, solver=saga;, score=0.943 total time=   0.3s\n",
      "[CV 2/5] END C=0.01, class_weight=None, max_iter=1000, penalty=l2, solver=saga;, score=0.938 total time=   0.2s\n",
      "[CV 3/5] END C=0.01, class_weight=None, max_iter=1000, penalty=l2, solver=saga;, score=0.943 total time=   0.4s\n",
      "[CV 4/5] END C=0.01, class_weight=None, max_iter=1000, penalty=l2, solver=saga;, score=0.936 total time=   0.5s\n",
      "[CV 5/5] END C=0.01, class_weight=None, max_iter=1000, penalty=l2, solver=saga;, score=0.937 total time=   0.3s\n",
      "[CV 1/5] END C=0.01, class_weight=balanced, max_iter=100, penalty=l1, solver=liblinear;, score=0.942 total time=   0.2s\n",
      "[CV 2/5] END C=0.01, class_weight=balanced, max_iter=100, penalty=l1, solver=liblinear;, score=0.939 total time=   0.3s\n",
      "[CV 3/5] END C=0.01, class_weight=balanced, max_iter=100, penalty=l1, solver=liblinear;, score=0.943 total time=   0.3s\n",
      "[CV 4/5] END C=0.01, class_weight=balanced, max_iter=100, penalty=l1, solver=liblinear;, score=0.935 total time=   0.3s\n",
      "[CV 5/5] END C=0.01, class_weight=balanced, max_iter=100, penalty=l1, solver=liblinear;, score=0.938 total time=   0.2s\n",
      "[CV 1/5] END C=0.01, class_weight=balanced, max_iter=100, penalty=l1, solver=saga;, score=0.942 total time=   0.5s\n",
      "[CV 2/5] END C=0.01, class_weight=balanced, max_iter=100, penalty=l1, solver=saga;, score=0.939 total time=   0.5s\n",
      "[CV 3/5] END C=0.01, class_weight=balanced, max_iter=100, penalty=l1, solver=saga;, score=0.943 total time=   0.5s\n",
      "[CV 4/5] END C=0.01, class_weight=balanced, max_iter=100, penalty=l1, solver=saga;, score=0.935 total time=   0.5s\n",
      "[CV 5/5] END C=0.01, class_weight=balanced, max_iter=100, penalty=l1, solver=saga;, score=0.938 total time=   0.5s\n",
      "[CV 1/5] END C=0.01, class_weight=balanced, max_iter=100, penalty=l2, solver=liblinear;, score=0.942 total time=   0.3s\n",
      "[CV 2/5] END C=0.01, class_weight=balanced, max_iter=100, penalty=l2, solver=liblinear;, score=0.939 total time=   0.3s\n",
      "[CV 3/5] END C=0.01, class_weight=balanced, max_iter=100, penalty=l2, solver=liblinear;, score=0.943 total time=   0.2s\n",
      "[CV 4/5] END C=0.01, class_weight=balanced, max_iter=100, penalty=l2, solver=liblinear;, score=0.936 total time=   0.2s\n",
      "[CV 5/5] END C=0.01, class_weight=balanced, max_iter=100, penalty=l2, solver=liblinear;, score=0.938 total time=   0.2s\n",
      "[CV 1/5] END C=0.01, class_weight=balanced, max_iter=100, penalty=l2, solver=saga;, score=0.942 total time=   0.4s\n",
      "[CV 2/5] END C=0.01, class_weight=balanced, max_iter=100, penalty=l2, solver=saga;, score=0.939 total time=   0.3s\n",
      "[CV 3/5] END C=0.01, class_weight=balanced, max_iter=100, penalty=l2, solver=saga;, score=0.943 total time=   0.4s\n",
      "[CV 4/5] END C=0.01, class_weight=balanced, max_iter=100, penalty=l2, solver=saga;, score=0.936 total time=   0.3s\n",
      "[CV 5/5] END C=0.01, class_weight=balanced, max_iter=100, penalty=l2, solver=saga;, score=0.938 total time=   0.4s\n",
      "[CV 1/5] END C=0.01, class_weight=balanced, max_iter=500, penalty=l1, solver=liblinear;, score=0.942 total time=   0.2s\n",
      "[CV 2/5] END C=0.01, class_weight=balanced, max_iter=500, penalty=l1, solver=liblinear;, score=0.939 total time=   0.2s\n",
      "[CV 3/5] END C=0.01, class_weight=balanced, max_iter=500, penalty=l1, solver=liblinear;, score=0.943 total time=   0.2s\n",
      "[CV 4/5] END C=0.01, class_weight=balanced, max_iter=500, penalty=l1, solver=liblinear;, score=0.935 total time=   0.2s\n",
      "[CV 5/5] END C=0.01, class_weight=balanced, max_iter=500, penalty=l1, solver=liblinear;, score=0.938 total time=   0.3s\n",
      "[CV 1/5] END C=0.01, class_weight=balanced, max_iter=500, penalty=l1, solver=saga;, score=0.942 total time=   0.4s\n",
      "[CV 2/5] END C=0.01, class_weight=balanced, max_iter=500, penalty=l1, solver=saga;, score=0.939 total time=   0.3s\n",
      "[CV 3/5] END C=0.01, class_weight=balanced, max_iter=500, penalty=l1, solver=saga;, score=0.943 total time=   0.4s\n",
      "[CV 4/5] END C=0.01, class_weight=balanced, max_iter=500, penalty=l1, solver=saga;, score=0.935 total time=   0.3s\n",
      "[CV 5/5] END C=0.01, class_weight=balanced, max_iter=500, penalty=l1, solver=saga;, score=0.938 total time=   0.4s\n",
      "[CV 1/5] END C=0.01, class_weight=balanced, max_iter=500, penalty=l2, solver=liblinear;, score=0.942 total time=   0.1s\n",
      "[CV 2/5] END C=0.01, class_weight=balanced, max_iter=500, penalty=l2, solver=liblinear;, score=0.939 total time=   0.2s\n",
      "[CV 3/5] END C=0.01, class_weight=balanced, max_iter=500, penalty=l2, solver=liblinear;, score=0.943 total time=   0.3s\n",
      "[CV 4/5] END C=0.01, class_weight=balanced, max_iter=500, penalty=l2, solver=liblinear;, score=0.936 total time=   0.2s\n",
      "[CV 5/5] END C=0.01, class_weight=balanced, max_iter=500, penalty=l2, solver=liblinear;, score=0.938 total time=   0.3s\n",
      "[CV 1/5] END C=0.01, class_weight=balanced, max_iter=500, penalty=l2, solver=saga;, score=0.942 total time=   0.3s\n",
      "[CV 2/5] END C=0.01, class_weight=balanced, max_iter=500, penalty=l2, solver=saga;, score=0.939 total time=   0.3s\n",
      "[CV 3/5] END C=0.01, class_weight=balanced, max_iter=500, penalty=l2, solver=saga;, score=0.943 total time=   0.3s\n",
      "[CV 4/5] END C=0.01, class_weight=balanced, max_iter=500, penalty=l2, solver=saga;, score=0.936 total time=   0.3s\n",
      "[CV 5/5] END C=0.01, class_weight=balanced, max_iter=500, penalty=l2, solver=saga;, score=0.938 total time=   0.3s\n",
      "[CV 1/5] END C=0.01, class_weight=balanced, max_iter=1000, penalty=l1, solver=liblinear;, score=0.942 total time=   0.2s\n",
      "[CV 2/5] END C=0.01, class_weight=balanced, max_iter=1000, penalty=l1, solver=liblinear;, score=0.939 total time=   0.2s\n",
      "[CV 3/5] END C=0.01, class_weight=balanced, max_iter=1000, penalty=l1, solver=liblinear;, score=0.943 total time=   0.2s\n",
      "[CV 4/5] END C=0.01, class_weight=balanced, max_iter=1000, penalty=l1, solver=liblinear;, score=0.935 total time=   0.2s\n",
      "[CV 5/5] END C=0.01, class_weight=balanced, max_iter=1000, penalty=l1, solver=liblinear;, score=0.938 total time=   0.2s\n",
      "[CV 1/5] END C=0.01, class_weight=balanced, max_iter=1000, penalty=l1, solver=saga;, score=0.942 total time=   0.4s\n",
      "[CV 2/5] END C=0.01, class_weight=balanced, max_iter=1000, penalty=l1, solver=saga;, score=0.939 total time=   0.3s\n",
      "[CV 3/5] END C=0.01, class_weight=balanced, max_iter=1000, penalty=l1, solver=saga;, score=0.943 total time=   0.4s\n",
      "[CV 4/5] END C=0.01, class_weight=balanced, max_iter=1000, penalty=l1, solver=saga;, score=0.935 total time=   0.4s\n",
      "[CV 5/5] END C=0.01, class_weight=balanced, max_iter=1000, penalty=l1, solver=saga;, score=0.938 total time=   0.3s\n",
      "[CV 1/5] END C=0.01, class_weight=balanced, max_iter=1000, penalty=l2, solver=liblinear;, score=0.942 total time=   0.2s\n",
      "[CV 2/5] END C=0.01, class_weight=balanced, max_iter=1000, penalty=l2, solver=liblinear;, score=0.939 total time=   0.2s\n",
      "[CV 3/5] END C=0.01, class_weight=balanced, max_iter=1000, penalty=l2, solver=liblinear;, score=0.943 total time=   0.2s\n",
      "[CV 4/5] END C=0.01, class_weight=balanced, max_iter=1000, penalty=l2, solver=liblinear;, score=0.936 total time=   0.2s\n",
      "[CV 5/5] END C=0.01, class_weight=balanced, max_iter=1000, penalty=l2, solver=liblinear;, score=0.938 total time=   0.2s\n",
      "[CV 1/5] END C=0.01, class_weight=balanced, max_iter=1000, penalty=l2, solver=saga;, score=0.942 total time=   0.3s\n",
      "[CV 2/5] END C=0.01, class_weight=balanced, max_iter=1000, penalty=l2, solver=saga;, score=0.939 total time=   0.3s\n",
      "[CV 3/5] END C=0.01, class_weight=balanced, max_iter=1000, penalty=l2, solver=saga;, score=0.943 total time=   0.3s\n",
      "[CV 4/5] END C=0.01, class_weight=balanced, max_iter=1000, penalty=l2, solver=saga;, score=0.936 total time=   0.3s\n",
      "[CV 5/5] END C=0.01, class_weight=balanced, max_iter=1000, penalty=l2, solver=saga;, score=0.938 total time=   0.3s\n",
      "[CV 1/5] END C=0.1, class_weight=None, max_iter=100, penalty=l1, solver=liblinear;, score=0.943 total time=   0.2s\n",
      "[CV 2/5] END C=0.1, class_weight=None, max_iter=100, penalty=l1, solver=liblinear;, score=0.938 total time=   0.2s\n",
      "[CV 3/5] END C=0.1, class_weight=None, max_iter=100, penalty=l1, solver=liblinear;, score=0.943 total time=   0.2s\n",
      "[CV 4/5] END C=0.1, class_weight=None, max_iter=100, penalty=l1, solver=liblinear;, score=0.936 total time=   0.2s\n",
      "[CV 5/5] END C=0.1, class_weight=None, max_iter=100, penalty=l1, solver=liblinear;, score=0.937 total time=   0.2s\n",
      "[CV 1/5] END C=0.1, class_weight=None, max_iter=100, penalty=l1, solver=saga;, score=0.943 total time=   0.3s\n",
      "[CV 2/5] END C=0.1, class_weight=None, max_iter=100, penalty=l1, solver=saga;, score=0.938 total time=   0.3s\n",
      "[CV 3/5] END C=0.1, class_weight=None, max_iter=100, penalty=l1, solver=saga;, score=0.943 total time=   0.3s\n",
      "[CV 4/5] END C=0.1, class_weight=None, max_iter=100, penalty=l1, solver=saga;, score=0.936 total time=   0.4s\n",
      "[CV 5/5] END C=0.1, class_weight=None, max_iter=100, penalty=l1, solver=saga;, score=0.937 total time=   0.4s\n",
      "[CV 1/5] END C=0.1, class_weight=None, max_iter=100, penalty=l2, solver=liblinear;, score=0.943 total time=   0.3s\n",
      "[CV 2/5] END C=0.1, class_weight=None, max_iter=100, penalty=l2, solver=liblinear;, score=0.938 total time=   0.4s\n",
      "[CV 3/5] END C=0.1, class_weight=None, max_iter=100, penalty=l2, solver=liblinear;, score=0.943 total time=   0.5s\n",
      "[CV 4/5] END C=0.1, class_weight=None, max_iter=100, penalty=l2, solver=liblinear;, score=0.936 total time=   0.4s\n",
      "[CV 5/5] END C=0.1, class_weight=None, max_iter=100, penalty=l2, solver=liblinear;, score=0.937 total time=   0.4s\n",
      "[CV 1/5] END C=0.1, class_weight=None, max_iter=100, penalty=l2, solver=saga;, score=0.943 total time=   0.5s\n",
      "[CV 2/5] END C=0.1, class_weight=None, max_iter=100, penalty=l2, solver=saga;, score=0.938 total time=   0.4s\n",
      "[CV 3/5] END C=0.1, class_weight=None, max_iter=100, penalty=l2, solver=saga;, score=0.943 total time=   0.4s\n",
      "[CV 4/5] END C=0.1, class_weight=None, max_iter=100, penalty=l2, solver=saga;, score=0.936 total time=   0.4s\n",
      "[CV 5/5] END C=0.1, class_weight=None, max_iter=100, penalty=l2, solver=saga;, score=0.937 total time=   0.3s\n",
      "[CV 1/5] END C=0.1, class_weight=None, max_iter=500, penalty=l1, solver=liblinear;, score=0.943 total time=   0.3s\n",
      "[CV 2/5] END C=0.1, class_weight=None, max_iter=500, penalty=l1, solver=liblinear;, score=0.938 total time=   0.3s\n",
      "[CV 3/5] END C=0.1, class_weight=None, max_iter=500, penalty=l1, solver=liblinear;, score=0.943 total time=   0.3s\n",
      "[CV 4/5] END C=0.1, class_weight=None, max_iter=500, penalty=l1, solver=liblinear;, score=0.936 total time=   0.3s\n",
      "[CV 5/5] END C=0.1, class_weight=None, max_iter=500, penalty=l1, solver=liblinear;, score=0.937 total time=   0.3s\n",
      "[CV 1/5] END C=0.1, class_weight=None, max_iter=500, penalty=l1, solver=saga;, score=0.943 total time=   0.3s\n",
      "[CV 2/5] END C=0.1, class_weight=None, max_iter=500, penalty=l1, solver=saga;, score=0.938 total time=   0.6s\n",
      "[CV 3/5] END C=0.1, class_weight=None, max_iter=500, penalty=l1, solver=saga;, score=0.943 total time=   0.5s\n",
      "[CV 4/5] END C=0.1, class_weight=None, max_iter=500, penalty=l1, solver=saga;, score=0.936 total time=   0.5s\n",
      "[CV 5/5] END C=0.1, class_weight=None, max_iter=500, penalty=l1, solver=saga;, score=0.937 total time=   0.3s\n",
      "[CV 1/5] END C=0.1, class_weight=None, max_iter=500, penalty=l2, solver=liblinear;, score=0.943 total time=   0.2s\n",
      "[CV 2/5] END C=0.1, class_weight=None, max_iter=500, penalty=l2, solver=liblinear;, score=0.938 total time=   0.4s\n",
      "[CV 3/5] END C=0.1, class_weight=None, max_iter=500, penalty=l2, solver=liblinear;, score=0.943 total time=   0.3s\n",
      "[CV 4/5] END C=0.1, class_weight=None, max_iter=500, penalty=l2, solver=liblinear;, score=0.936 total time=   0.2s\n",
      "[CV 5/5] END C=0.1, class_weight=None, max_iter=500, penalty=l2, solver=liblinear;, score=0.937 total time=   0.3s\n",
      "[CV 1/5] END C=0.1, class_weight=None, max_iter=500, penalty=l2, solver=saga;, score=0.943 total time=   0.3s\n",
      "[CV 2/5] END C=0.1, class_weight=None, max_iter=500, penalty=l2, solver=saga;, score=0.938 total time=   0.2s\n",
      "[CV 3/5] END C=0.1, class_weight=None, max_iter=500, penalty=l2, solver=saga;, score=0.943 total time=   0.3s\n",
      "[CV 4/5] END C=0.1, class_weight=None, max_iter=500, penalty=l2, solver=saga;, score=0.936 total time=   0.3s\n",
      "[CV 5/5] END C=0.1, class_weight=None, max_iter=500, penalty=l2, solver=saga;, score=0.937 total time=   0.3s\n",
      "[CV 1/5] END C=0.1, class_weight=None, max_iter=1000, penalty=l1, solver=liblinear;, score=0.943 total time=   0.2s\n",
      "[CV 2/5] END C=0.1, class_weight=None, max_iter=1000, penalty=l1, solver=liblinear;, score=0.938 total time=   0.2s\n",
      "[CV 3/5] END C=0.1, class_weight=None, max_iter=1000, penalty=l1, solver=liblinear;, score=0.943 total time=   0.2s\n",
      "[CV 4/5] END C=0.1, class_weight=None, max_iter=1000, penalty=l1, solver=liblinear;, score=0.936 total time=   0.2s\n",
      "[CV 5/5] END C=0.1, class_weight=None, max_iter=1000, penalty=l1, solver=liblinear;, score=0.937 total time=   0.3s\n",
      "[CV 1/5] END C=0.1, class_weight=None, max_iter=1000, penalty=l1, solver=saga;, score=0.943 total time=   0.3s\n",
      "[CV 2/5] END C=0.1, class_weight=None, max_iter=1000, penalty=l1, solver=saga;, score=0.938 total time=   0.3s\n",
      "[CV 3/5] END C=0.1, class_weight=None, max_iter=1000, penalty=l1, solver=saga;, score=0.943 total time=   0.3s\n",
      "[CV 4/5] END C=0.1, class_weight=None, max_iter=1000, penalty=l1, solver=saga;, score=0.936 total time=   0.3s\n",
      "[CV 5/5] END C=0.1, class_weight=None, max_iter=1000, penalty=l1, solver=saga;, score=0.937 total time=   0.3s\n",
      "[CV 1/5] END C=0.1, class_weight=None, max_iter=1000, penalty=l2, solver=liblinear;, score=0.943 total time=   0.2s\n",
      "[CV 2/5] END C=0.1, class_weight=None, max_iter=1000, penalty=l2, solver=liblinear;, score=0.938 total time=   0.2s\n",
      "[CV 3/5] END C=0.1, class_weight=None, max_iter=1000, penalty=l2, solver=liblinear;, score=0.943 total time=   0.4s\n",
      "[CV 4/5] END C=0.1, class_weight=None, max_iter=1000, penalty=l2, solver=liblinear;, score=0.936 total time=   0.3s\n",
      "[CV 5/5] END C=0.1, class_weight=None, max_iter=1000, penalty=l2, solver=liblinear;, score=0.937 total time=   0.3s\n",
      "[CV 1/5] END C=0.1, class_weight=None, max_iter=1000, penalty=l2, solver=saga;, score=0.943 total time=   0.4s\n",
      "[CV 2/5] END C=0.1, class_weight=None, max_iter=1000, penalty=l2, solver=saga;, score=0.938 total time=   0.5s\n",
      "[CV 3/5] END C=0.1, class_weight=None, max_iter=1000, penalty=l2, solver=saga;, score=0.943 total time=   0.4s\n",
      "[CV 4/5] END C=0.1, class_weight=None, max_iter=1000, penalty=l2, solver=saga;, score=0.936 total time=   0.4s\n",
      "[CV 5/5] END C=0.1, class_weight=None, max_iter=1000, penalty=l2, solver=saga;, score=0.937 total time=   0.4s\n",
      "[CV 1/5] END C=0.1, class_weight=balanced, max_iter=100, penalty=l1, solver=liblinear;, score=0.942 total time=   0.3s\n",
      "[CV 2/5] END C=0.1, class_weight=balanced, max_iter=100, penalty=l1, solver=liblinear;, score=0.940 total time=   0.3s\n",
      "[CV 3/5] END C=0.1, class_weight=balanced, max_iter=100, penalty=l1, solver=liblinear;, score=0.944 total time=   0.3s\n",
      "[CV 4/5] END C=0.1, class_weight=balanced, max_iter=100, penalty=l1, solver=liblinear;, score=0.936 total time=   0.3s\n",
      "[CV 5/5] END C=0.1, class_weight=balanced, max_iter=100, penalty=l1, solver=liblinear;, score=0.938 total time=   0.3s\n",
      "[CV 1/5] END C=0.1, class_weight=balanced, max_iter=100, penalty=l1, solver=saga;, score=0.942 total time=   0.4s\n",
      "[CV 2/5] END C=0.1, class_weight=balanced, max_iter=100, penalty=l1, solver=saga;, score=0.940 total time=   0.5s\n",
      "[CV 3/5] END C=0.1, class_weight=balanced, max_iter=100, penalty=l1, solver=saga;, score=0.944 total time=   0.6s\n",
      "[CV 4/5] END C=0.1, class_weight=balanced, max_iter=100, penalty=l1, solver=saga;, score=0.936 total time=   0.6s\n",
      "[CV 5/5] END C=0.1, class_weight=balanced, max_iter=100, penalty=l1, solver=saga;, score=0.938 total time=   0.5s\n",
      "[CV 1/5] END C=0.1, class_weight=balanced, max_iter=100, penalty=l2, solver=liblinear;, score=0.942 total time=   0.4s\n",
      "[CV 2/5] END C=0.1, class_weight=balanced, max_iter=100, penalty=l2, solver=liblinear;, score=0.939 total time=   0.4s\n",
      "[CV 3/5] END C=0.1, class_weight=balanced, max_iter=100, penalty=l2, solver=liblinear;, score=0.944 total time=   0.3s\n",
      "[CV 4/5] END C=0.1, class_weight=balanced, max_iter=100, penalty=l2, solver=liblinear;, score=0.936 total time=   0.3s\n",
      "[CV 5/5] END C=0.1, class_weight=balanced, max_iter=100, penalty=l2, solver=liblinear;, score=0.938 total time=   0.3s\n",
      "[CV 1/5] END C=0.1, class_weight=balanced, max_iter=100, penalty=l2, solver=saga;, score=0.942 total time=   0.4s\n",
      "[CV 2/5] END C=0.1, class_weight=balanced, max_iter=100, penalty=l2, solver=saga;, score=0.939 total time=   0.4s\n",
      "[CV 3/5] END C=0.1, class_weight=balanced, max_iter=100, penalty=l2, solver=saga;, score=0.944 total time=   0.4s\n",
      "[CV 4/5] END C=0.1, class_weight=balanced, max_iter=100, penalty=l2, solver=saga;, score=0.936 total time=   0.3s\n",
      "[CV 5/5] END C=0.1, class_weight=balanced, max_iter=100, penalty=l2, solver=saga;, score=0.938 total time=   0.4s\n",
      "[CV 1/5] END C=0.1, class_weight=balanced, max_iter=500, penalty=l1, solver=liblinear;, score=0.942 total time=   0.2s\n",
      "[CV 2/5] END C=0.1, class_weight=balanced, max_iter=500, penalty=l1, solver=liblinear;, score=0.940 total time=   0.2s\n",
      "[CV 3/5] END C=0.1, class_weight=balanced, max_iter=500, penalty=l1, solver=liblinear;, score=0.944 total time=   0.3s\n",
      "[CV 4/5] END C=0.1, class_weight=balanced, max_iter=500, penalty=l1, solver=liblinear;, score=0.936 total time=   0.2s\n",
      "[CV 5/5] END C=0.1, class_weight=balanced, max_iter=500, penalty=l1, solver=liblinear;, score=0.938 total time=   0.2s\n",
      "[CV 1/5] END C=0.1, class_weight=balanced, max_iter=500, penalty=l1, solver=saga;, score=0.942 total time=   0.4s\n",
      "[CV 2/5] END C=0.1, class_weight=balanced, max_iter=500, penalty=l1, solver=saga;, score=0.940 total time=   0.4s\n",
      "[CV 3/5] END C=0.1, class_weight=balanced, max_iter=500, penalty=l1, solver=saga;, score=0.944 total time=   0.5s\n",
      "[CV 4/5] END C=0.1, class_weight=balanced, max_iter=500, penalty=l1, solver=saga;, score=0.936 total time=   0.5s\n",
      "[CV 5/5] END C=0.1, class_weight=balanced, max_iter=500, penalty=l1, solver=saga;, score=0.938 total time=   0.5s\n",
      "[CV 1/5] END C=0.1, class_weight=balanced, max_iter=500, penalty=l2, solver=liblinear;, score=0.942 total time=   0.2s\n",
      "[CV 2/5] END C=0.1, class_weight=balanced, max_iter=500, penalty=l2, solver=liblinear;, score=0.939 total time=   0.3s\n",
      "[CV 3/5] END C=0.1, class_weight=balanced, max_iter=500, penalty=l2, solver=liblinear;, score=0.944 total time=   0.2s\n",
      "[CV 4/5] END C=0.1, class_weight=balanced, max_iter=500, penalty=l2, solver=liblinear;, score=0.936 total time=   0.3s\n",
      "[CV 5/5] END C=0.1, class_weight=balanced, max_iter=500, penalty=l2, solver=liblinear;, score=0.938 total time=   0.3s\n",
      "[CV 1/5] END C=0.1, class_weight=balanced, max_iter=500, penalty=l2, solver=saga;, score=0.942 total time=   0.4s\n",
      "[CV 2/5] END C=0.1, class_weight=balanced, max_iter=500, penalty=l2, solver=saga;, score=0.939 total time=   0.3s\n",
      "[CV 3/5] END C=0.1, class_weight=balanced, max_iter=500, penalty=l2, solver=saga;, score=0.944 total time=   0.4s\n",
      "[CV 4/5] END C=0.1, class_weight=balanced, max_iter=500, penalty=l2, solver=saga;, score=0.936 total time=   0.3s\n",
      "[CV 5/5] END C=0.1, class_weight=balanced, max_iter=500, penalty=l2, solver=saga;, score=0.938 total time=   0.4s\n",
      "[CV 1/5] END C=0.1, class_weight=balanced, max_iter=1000, penalty=l1, solver=liblinear;, score=0.942 total time=   0.2s\n",
      "[CV 2/5] END C=0.1, class_weight=balanced, max_iter=1000, penalty=l1, solver=liblinear;, score=0.940 total time=   0.2s\n",
      "[CV 3/5] END C=0.1, class_weight=balanced, max_iter=1000, penalty=l1, solver=liblinear;, score=0.944 total time=   0.3s\n",
      "[CV 4/5] END C=0.1, class_weight=balanced, max_iter=1000, penalty=l1, solver=liblinear;, score=0.936 total time=   0.3s\n",
      "[CV 5/5] END C=0.1, class_weight=balanced, max_iter=1000, penalty=l1, solver=liblinear;, score=0.938 total time=   0.2s\n",
      "[CV 1/5] END C=0.1, class_weight=balanced, max_iter=1000, penalty=l1, solver=saga;, score=0.942 total time=   0.6s\n",
      "[CV 2/5] END C=0.1, class_weight=balanced, max_iter=1000, penalty=l1, solver=saga;, score=0.940 total time=   0.5s\n",
      "[CV 3/5] END C=0.1, class_weight=balanced, max_iter=1000, penalty=l1, solver=saga;, score=0.944 total time=   0.4s\n",
      "[CV 4/5] END C=0.1, class_weight=balanced, max_iter=1000, penalty=l1, solver=saga;, score=0.936 total time=   0.5s\n",
      "[CV 5/5] END C=0.1, class_weight=balanced, max_iter=1000, penalty=l1, solver=saga;, score=0.938 total time=   0.5s\n",
      "[CV 1/5] END C=0.1, class_weight=balanced, max_iter=1000, penalty=l2, solver=liblinear;, score=0.942 total time=   0.3s\n",
      "[CV 2/5] END C=0.1, class_weight=balanced, max_iter=1000, penalty=l2, solver=liblinear;, score=0.939 total time=   0.2s\n",
      "[CV 3/5] END C=0.1, class_weight=balanced, max_iter=1000, penalty=l2, solver=liblinear;, score=0.944 total time=   0.2s\n",
      "[CV 4/5] END C=0.1, class_weight=balanced, max_iter=1000, penalty=l2, solver=liblinear;, score=0.936 total time=   0.2s\n",
      "[CV 5/5] END C=0.1, class_weight=balanced, max_iter=1000, penalty=l2, solver=liblinear;, score=0.938 total time=   0.2s\n",
      "[CV 1/5] END C=0.1, class_weight=balanced, max_iter=1000, penalty=l2, solver=saga;, score=0.942 total time=   0.3s\n",
      "[CV 2/5] END C=0.1, class_weight=balanced, max_iter=1000, penalty=l2, solver=saga;, score=0.939 total time=   0.3s\n",
      "[CV 3/5] END C=0.1, class_weight=balanced, max_iter=1000, penalty=l2, solver=saga;, score=0.944 total time=   0.3s\n",
      "[CV 4/5] END C=0.1, class_weight=balanced, max_iter=1000, penalty=l2, solver=saga;, score=0.936 total time=   0.3s\n",
      "[CV 5/5] END C=0.1, class_weight=balanced, max_iter=1000, penalty=l2, solver=saga;, score=0.938 total time=   0.3s\n",
      "[CV 1/5] END C=1, class_weight=None, max_iter=100, penalty=l1, solver=liblinear;, score=0.942 total time=   0.2s\n",
      "[CV 2/5] END C=1, class_weight=None, max_iter=100, penalty=l1, solver=liblinear;, score=0.938 total time=   0.2s\n",
      "[CV 3/5] END C=1, class_weight=None, max_iter=100, penalty=l1, solver=liblinear;, score=0.943 total time=   0.4s\n",
      "[CV 4/5] END C=1, class_weight=None, max_iter=100, penalty=l1, solver=liblinear;, score=0.936 total time=   0.3s\n",
      "[CV 5/5] END C=1, class_weight=None, max_iter=100, penalty=l1, solver=liblinear;, score=0.937 total time=   0.3s\n",
      "[CV 1/5] END C=1, class_weight=None, max_iter=100, penalty=l1, solver=saga;, score=0.943 total time=   0.5s\n",
      "[CV 2/5] END C=1, class_weight=None, max_iter=100, penalty=l1, solver=saga;, score=0.938 total time=   0.4s\n",
      "[CV 3/5] END C=1, class_weight=None, max_iter=100, penalty=l1, solver=saga;, score=0.943 total time=   0.3s\n",
      "[CV 4/5] END C=1, class_weight=None, max_iter=100, penalty=l1, solver=saga;, score=0.936 total time=   0.4s\n",
      "[CV 5/5] END C=1, class_weight=None, max_iter=100, penalty=l1, solver=saga;, score=0.937 total time=   0.3s\n",
      "[CV 1/5] END C=1, class_weight=None, max_iter=100, penalty=l2, solver=liblinear;, score=0.943 total time=   0.2s\n",
      "[CV 2/5] END C=1, class_weight=None, max_iter=100, penalty=l2, solver=liblinear;, score=0.938 total time=   0.4s\n",
      "[CV 3/5] END C=1, class_weight=None, max_iter=100, penalty=l2, solver=liblinear;, score=0.943 total time=   0.3s\n",
      "[CV 4/5] END C=1, class_weight=None, max_iter=100, penalty=l2, solver=liblinear;, score=0.936 total time=   0.4s\n",
      "[CV 5/5] END C=1, class_weight=None, max_iter=100, penalty=l2, solver=liblinear;, score=0.937 total time=   0.3s\n",
      "[CV 1/5] END C=1, class_weight=None, max_iter=100, penalty=l2, solver=saga;, score=0.943 total time=   0.4s\n",
      "[CV 2/5] END C=1, class_weight=None, max_iter=100, penalty=l2, solver=saga;, score=0.938 total time=   0.4s\n",
      "[CV 3/5] END C=1, class_weight=None, max_iter=100, penalty=l2, solver=saga;, score=0.943 total time=   0.6s\n",
      "[CV 4/5] END C=1, class_weight=None, max_iter=100, penalty=l2, solver=saga;, score=0.936 total time=   1.1s\n",
      "[CV 5/5] END C=1, class_weight=None, max_iter=100, penalty=l2, solver=saga;, score=0.937 total time=   0.7s\n",
      "[CV 1/5] END C=1, class_weight=None, max_iter=500, penalty=l1, solver=liblinear;, score=0.942 total time=   0.4s\n",
      "[CV 2/5] END C=1, class_weight=None, max_iter=500, penalty=l1, solver=liblinear;, score=0.938 total time=   0.4s\n",
      "[CV 3/5] END C=1, class_weight=None, max_iter=500, penalty=l1, solver=liblinear;, score=0.943 total time=   0.4s\n",
      "[CV 4/5] END C=1, class_weight=None, max_iter=500, penalty=l1, solver=liblinear;, score=0.936 total time=   0.3s\n",
      "[CV 5/5] END C=1, class_weight=None, max_iter=500, penalty=l1, solver=liblinear;, score=0.937 total time=   0.4s\n",
      "[CV 1/5] END C=1, class_weight=None, max_iter=500, penalty=l1, solver=saga;, score=0.943 total time=   0.5s\n",
      "[CV 2/5] END C=1, class_weight=None, max_iter=500, penalty=l1, solver=saga;, score=0.938 total time=   0.6s\n",
      "[CV 3/5] END C=1, class_weight=None, max_iter=500, penalty=l1, solver=saga;, score=0.943 total time=   0.4s\n",
      "[CV 4/5] END C=1, class_weight=None, max_iter=500, penalty=l1, solver=saga;, score=0.936 total time=   0.4s\n",
      "[CV 5/5] END C=1, class_weight=None, max_iter=500, penalty=l1, solver=saga;, score=0.937 total time=   0.3s\n",
      "[CV 1/5] END C=1, class_weight=None, max_iter=500, penalty=l2, solver=liblinear;, score=0.943 total time=   0.2s\n",
      "[CV 2/5] END C=1, class_weight=None, max_iter=500, penalty=l2, solver=liblinear;, score=0.938 total time=   0.3s\n",
      "[CV 3/5] END C=1, class_weight=None, max_iter=500, penalty=l2, solver=liblinear;, score=0.943 total time=   0.3s\n",
      "[CV 4/5] END C=1, class_weight=None, max_iter=500, penalty=l2, solver=liblinear;, score=0.936 total time=   0.3s\n",
      "[CV 5/5] END C=1, class_weight=None, max_iter=500, penalty=l2, solver=liblinear;, score=0.937 total time=   0.2s\n",
      "[CV 1/5] END C=1, class_weight=None, max_iter=500, penalty=l2, solver=saga;, score=0.943 total time=   0.3s\n",
      "[CV 2/5] END C=1, class_weight=None, max_iter=500, penalty=l2, solver=saga;, score=0.938 total time=   0.4s\n",
      "[CV 3/5] END C=1, class_weight=None, max_iter=500, penalty=l2, solver=saga;, score=0.943 total time=   0.3s\n",
      "[CV 4/5] END C=1, class_weight=None, max_iter=500, penalty=l2, solver=saga;, score=0.936 total time=   0.5s\n",
      "[CV 5/5] END C=1, class_weight=None, max_iter=500, penalty=l2, solver=saga;, score=0.937 total time=   0.4s\n",
      "[CV 1/5] END C=1, class_weight=None, max_iter=1000, penalty=l1, solver=liblinear;, score=0.942 total time=   0.3s\n",
      "[CV 2/5] END C=1, class_weight=None, max_iter=1000, penalty=l1, solver=liblinear;, score=0.938 total time=   0.3s\n",
      "[CV 3/5] END C=1, class_weight=None, max_iter=1000, penalty=l1, solver=liblinear;, score=0.943 total time=   0.3s\n",
      "[CV 4/5] END C=1, class_weight=None, max_iter=1000, penalty=l1, solver=liblinear;, score=0.936 total time=   0.2s\n",
      "[CV 5/5] END C=1, class_weight=None, max_iter=1000, penalty=l1, solver=liblinear;, score=0.937 total time=   0.3s\n",
      "[CV 1/5] END C=1, class_weight=None, max_iter=1000, penalty=l1, solver=saga;, score=0.943 total time=   0.4s\n",
      "[CV 2/5] END C=1, class_weight=None, max_iter=1000, penalty=l1, solver=saga;, score=0.938 total time=   0.4s\n",
      "[CV 3/5] END C=1, class_weight=None, max_iter=1000, penalty=l1, solver=saga;, score=0.943 total time=   0.3s\n",
      "[CV 4/5] END C=1, class_weight=None, max_iter=1000, penalty=l1, solver=saga;, score=0.936 total time=   0.4s\n",
      "[CV 5/5] END C=1, class_weight=None, max_iter=1000, penalty=l1, solver=saga;, score=0.937 total time=   0.3s\n",
      "[CV 1/5] END C=1, class_weight=None, max_iter=1000, penalty=l2, solver=liblinear;, score=0.943 total time=   0.2s\n",
      "[CV 2/5] END C=1, class_weight=None, max_iter=1000, penalty=l2, solver=liblinear;, score=0.938 total time=   0.4s\n",
      "[CV 3/5] END C=1, class_weight=None, max_iter=1000, penalty=l2, solver=liblinear;, score=0.943 total time=   0.3s\n",
      "[CV 4/5] END C=1, class_weight=None, max_iter=1000, penalty=l2, solver=liblinear;, score=0.936 total time=   0.4s\n",
      "[CV 5/5] END C=1, class_weight=None, max_iter=1000, penalty=l2, solver=liblinear;, score=0.937 total time=   0.3s\n",
      "[CV 1/5] END C=1, class_weight=None, max_iter=1000, penalty=l2, solver=saga;, score=0.943 total time=   0.5s\n",
      "[CV 2/5] END C=1, class_weight=None, max_iter=1000, penalty=l2, solver=saga;, score=0.938 total time=   0.5s\n",
      "[CV 3/5] END C=1, class_weight=None, max_iter=1000, penalty=l2, solver=saga;, score=0.943 total time=   0.5s\n",
      "[CV 4/5] END C=1, class_weight=None, max_iter=1000, penalty=l2, solver=saga;, score=0.936 total time=   0.5s\n",
      "[CV 5/5] END C=1, class_weight=None, max_iter=1000, penalty=l2, solver=saga;, score=0.937 total time=   0.5s\n",
      "[CV 1/5] END C=1, class_weight=balanced, max_iter=100, penalty=l1, solver=liblinear;, score=0.942 total time=   0.4s\n",
      "[CV 2/5] END C=1, class_weight=balanced, max_iter=100, penalty=l1, solver=liblinear;, score=0.939 total time=   0.4s\n",
      "[CV 3/5] END C=1, class_weight=balanced, max_iter=100, penalty=l1, solver=liblinear;, score=0.944 total time=   0.4s\n",
      "[CV 4/5] END C=1, class_weight=balanced, max_iter=100, penalty=l1, solver=liblinear;, score=0.936 total time=   0.3s\n",
      "[CV 5/5] END C=1, class_weight=balanced, max_iter=100, penalty=l1, solver=liblinear;, score=0.938 total time=   0.3s\n",
      "[CV 1/5] END C=1, class_weight=balanced, max_iter=100, penalty=l1, solver=saga;, score=0.942 total time=   0.5s\n",
      "[CV 2/5] END C=1, class_weight=balanced, max_iter=100, penalty=l1, solver=saga;, score=0.939 total time=   0.6s\n",
      "[CV 3/5] END C=1, class_weight=balanced, max_iter=100, penalty=l1, solver=saga;, score=0.944 total time=   0.5s\n",
      "[CV 4/5] END C=1, class_weight=balanced, max_iter=100, penalty=l1, solver=saga;, score=0.936 total time=   0.7s\n",
      "[CV 5/5] END C=1, class_weight=balanced, max_iter=100, penalty=l1, solver=saga;, score=0.938 total time=   0.6s\n",
      "[CV 1/5] END C=1, class_weight=balanced, max_iter=100, penalty=l2, solver=liblinear;, score=0.942 total time=   0.4s\n",
      "[CV 2/5] END C=1, class_weight=balanced, max_iter=100, penalty=l2, solver=liblinear;, score=0.939 total time=   0.3s\n",
      "[CV 3/5] END C=1, class_weight=balanced, max_iter=100, penalty=l2, solver=liblinear;, score=0.944 total time=   0.4s\n",
      "[CV 4/5] END C=1, class_weight=balanced, max_iter=100, penalty=l2, solver=liblinear;, score=0.936 total time=   0.3s\n",
      "[CV 5/5] END C=1, class_weight=balanced, max_iter=100, penalty=l2, solver=liblinear;, score=0.938 total time=   0.2s\n",
      "[CV 1/5] END C=1, class_weight=balanced, max_iter=100, penalty=l2, solver=saga;, score=0.942 total time=   0.4s\n",
      "[CV 2/5] END C=1, class_weight=balanced, max_iter=100, penalty=l2, solver=saga;, score=0.939 total time=   0.4s\n",
      "[CV 3/5] END C=1, class_weight=balanced, max_iter=100, penalty=l2, solver=saga;, score=0.944 total time=   0.5s\n",
      "[CV 4/5] END C=1, class_weight=balanced, max_iter=100, penalty=l2, solver=saga;, score=0.936 total time=   0.4s\n",
      "[CV 5/5] END C=1, class_weight=balanced, max_iter=100, penalty=l2, solver=saga;, score=0.938 total time=   0.4s\n",
      "[CV 1/5] END C=1, class_weight=balanced, max_iter=500, penalty=l1, solver=liblinear;, score=0.942 total time=   0.2s\n",
      "[CV 2/5] END C=1, class_weight=balanced, max_iter=500, penalty=l1, solver=liblinear;, score=0.939 total time=   0.3s\n",
      "[CV 3/5] END C=1, class_weight=balanced, max_iter=500, penalty=l1, solver=liblinear;, score=0.944 total time=   0.3s\n",
      "[CV 4/5] END C=1, class_weight=balanced, max_iter=500, penalty=l1, solver=liblinear;, score=0.936 total time=   0.3s\n",
      "[CV 5/5] END C=1, class_weight=balanced, max_iter=500, penalty=l1, solver=liblinear;, score=0.938 total time=   0.4s\n",
      "[CV 1/5] END C=1, class_weight=balanced, max_iter=500, penalty=l1, solver=saga;, score=0.942 total time=   0.5s\n",
      "[CV 2/5] END C=1, class_weight=balanced, max_iter=500, penalty=l1, solver=saga;, score=0.939 total time=   0.4s\n",
      "[CV 3/5] END C=1, class_weight=balanced, max_iter=500, penalty=l1, solver=saga;, score=0.944 total time=   0.4s\n",
      "[CV 4/5] END C=1, class_weight=balanced, max_iter=500, penalty=l1, solver=saga;, score=0.936 total time=   0.4s\n",
      "[CV 5/5] END C=1, class_weight=balanced, max_iter=500, penalty=l1, solver=saga;, score=0.938 total time=   0.4s\n",
      "[CV 1/5] END C=1, class_weight=balanced, max_iter=500, penalty=l2, solver=liblinear;, score=0.942 total time=   0.2s\n",
      "[CV 2/5] END C=1, class_weight=balanced, max_iter=500, penalty=l2, solver=liblinear;, score=0.939 total time=   0.4s\n",
      "[CV 3/5] END C=1, class_weight=balanced, max_iter=500, penalty=l2, solver=liblinear;, score=0.944 total time=   0.3s\n",
      "[CV 4/5] END C=1, class_weight=balanced, max_iter=500, penalty=l2, solver=liblinear;, score=0.936 total time=   0.3s\n",
      "[CV 5/5] END C=1, class_weight=balanced, max_iter=500, penalty=l2, solver=liblinear;, score=0.938 total time=   0.3s\n",
      "[CV 1/5] END C=1, class_weight=balanced, max_iter=500, penalty=l2, solver=saga;, score=0.942 total time=   0.4s\n",
      "[CV 2/5] END C=1, class_weight=balanced, max_iter=500, penalty=l2, solver=saga;, score=0.939 total time=   0.3s\n",
      "[CV 3/5] END C=1, class_weight=balanced, max_iter=500, penalty=l2, solver=saga;, score=0.944 total time=   0.4s\n",
      "[CV 4/5] END C=1, class_weight=balanced, max_iter=500, penalty=l2, solver=saga;, score=0.936 total time=   0.3s\n",
      "[CV 5/5] END C=1, class_weight=balanced, max_iter=500, penalty=l2, solver=saga;, score=0.938 total time=   0.4s\n",
      "[CV 1/5] END C=1, class_weight=balanced, max_iter=1000, penalty=l1, solver=liblinear;, score=0.942 total time=   0.2s\n",
      "[CV 2/5] END C=1, class_weight=balanced, max_iter=1000, penalty=l1, solver=liblinear;, score=0.939 total time=   0.4s\n",
      "[CV 3/5] END C=1, class_weight=balanced, max_iter=1000, penalty=l1, solver=liblinear;, score=0.944 total time=   0.4s\n",
      "[CV 4/5] END C=1, class_weight=balanced, max_iter=1000, penalty=l1, solver=liblinear;, score=0.936 total time=   0.4s\n",
      "[CV 5/5] END C=1, class_weight=balanced, max_iter=1000, penalty=l1, solver=liblinear;, score=0.938 total time=   0.4s\n",
      "[CV 1/5] END C=1, class_weight=balanced, max_iter=1000, penalty=l1, solver=saga;, score=0.942 total time=   0.6s\n",
      "[CV 2/5] END C=1, class_weight=balanced, max_iter=1000, penalty=l1, solver=saga;, score=0.939 total time=   0.6s\n",
      "[CV 3/5] END C=1, class_weight=balanced, max_iter=1000, penalty=l1, solver=saga;, score=0.944 total time=   0.6s\n",
      "[CV 4/5] END C=1, class_weight=balanced, max_iter=1000, penalty=l1, solver=saga;, score=0.936 total time=   0.5s\n",
      "[CV 5/5] END C=1, class_weight=balanced, max_iter=1000, penalty=l1, solver=saga;, score=0.938 total time=   0.5s\n",
      "[CV 1/5] END C=1, class_weight=balanced, max_iter=1000, penalty=l2, solver=liblinear;, score=0.942 total time=   0.4s\n",
      "[CV 2/5] END C=1, class_weight=balanced, max_iter=1000, penalty=l2, solver=liblinear;, score=0.939 total time=   0.3s\n",
      "[CV 3/5] END C=1, class_weight=balanced, max_iter=1000, penalty=l2, solver=liblinear;, score=0.944 total time=   0.3s\n",
      "[CV 4/5] END C=1, class_weight=balanced, max_iter=1000, penalty=l2, solver=liblinear;, score=0.936 total time=   0.3s\n",
      "[CV 5/5] END C=1, class_weight=balanced, max_iter=1000, penalty=l2, solver=liblinear;, score=0.938 total time=   0.3s\n",
      "[CV 1/5] END C=1, class_weight=balanced, max_iter=1000, penalty=l2, solver=saga;, score=0.942 total time=   0.3s\n",
      "[CV 2/5] END C=1, class_weight=balanced, max_iter=1000, penalty=l2, solver=saga;, score=0.939 total time=   0.3s\n",
      "[CV 3/5] END C=1, class_weight=balanced, max_iter=1000, penalty=l2, solver=saga;, score=0.944 total time=   0.3s\n",
      "[CV 4/5] END C=1, class_weight=balanced, max_iter=1000, penalty=l2, solver=saga;, score=0.936 total time=   0.3s\n",
      "[CV 5/5] END C=1, class_weight=balanced, max_iter=1000, penalty=l2, solver=saga;, score=0.938 total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rh/w6cl6rd95vj2g4s03scz_4fc0000gn/T/ipykernel_1984/975800168.py:37: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append({'Model_Specs': best_model,\n",
      "/var/folders/rh/w6cl6rd95vj2g4s03scz_4fc0000gn/T/ipykernel_1984/975800168.py:57: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append({'Model_Specs': model_params,\n",
      "/var/folders/rh/w6cl6rd95vj2g4s03scz_4fc0000gn/T/ipykernel_1984/975800168.py:57: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append({'Model_Specs': model_params,\n",
      "/var/folders/rh/w6cl6rd95vj2g4s03scz_4fc0000gn/T/ipykernel_1984/975800168.py:57: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append({'Model_Specs': model_params,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-Time Train Evaluation Results:\n",
      "Train Gini: 0.8423\n",
      "Train Confusion Matrix:\n",
      "[[56008 11515]\n",
      " [  618  4860]]\n",
      "\n",
      "In-Time Test Evaluation Results:\n",
      "Test Gini: 0.8383\n",
      "Test Confusion Matrix:\n",
      "[[23811  5128]\n",
      " [  266  2082]]\n",
      "\n",
      "Out-of-Time Test Evaluation Results:\n",
      "Test Gini: 0.7915\n",
      "Test Confusion Matrix:\n",
      "[[20505    42]\n",
      " [  846    77]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rh/w6cl6rd95vj2g4s03scz_4fc0000gn/T/ipykernel_1984/975800168.py:57: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append({'Model_Specs': model_params,\n"
     ]
    }
   ],
   "source": [
    "results_vif_LR = train_and_evaluate_pipeline(df_VIF_woe, time = 2020, model = model, param_grid = param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Define XGBoost model\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "param_grid_xgboost = {\n",
    "    'learning_rate': [0.1, 0.2],\n",
    "    'n_estimators': [200],\n",
    "    #'max_depth': [3, 4, 5],\n",
    "    'min_child_weight': [2, 3],\n",
    "    'subsample': [0.7, 0.9],\n",
    "    'colsample_bytree': [0.7],\n",
    "    'gamma': [0.1, 0.2],\n",
    "    'reg_alpha': [0.1, 0.2],\n",
    "    #'reg_lambda': [0, 0.1, 0.2]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "[CV 1/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.957 total time=   0.8s\n",
      "[CV 2/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.960 total time=   0.7s\n",
      "[CV 3/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.959 total time=   0.7s\n",
      "[CV 4/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.953 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.957 total time=   0.7s\n",
      "[CV 1/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.957 total time=   0.5s\n",
      "[CV 2/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.959 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.959 total time=   0.5s\n",
      "[CV 4/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.954 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.957 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.957 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.960 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.959 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.954 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.957 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.958 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.960 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.959 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.954 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.958 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.957 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.960 total time=   0.5s\n",
      "[CV 3/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.959 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.954 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.956 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.958 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.959 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.959 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.954 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.957 total time=   0.8s\n",
      "[CV 1/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.957 total time=   0.8s\n",
      "[CV 2/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.960 total time=   0.9s\n",
      "[CV 3/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.959 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.953 total time=   0.5s\n",
      "[CV 5/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.957 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.957 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.960 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.959 total time=   0.5s\n",
      "[CV 4/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.954 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.957 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.955 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.957 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.956 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.950 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.953 total time=   0.7s\n",
      "[CV 1/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.954 total time=   0.7s\n",
      "[CV 2/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.957 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.956 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.952 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.954 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.954 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.956 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.956 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.951 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.952 total time=   0.7s\n",
      "[CV 1/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.955 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.957 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.956 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.952 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.954 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.954 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.958 total time=   0.5s\n",
      "[CV 3/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.955 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.950 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.953 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.955 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.957 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.956 total time=   0.7s\n",
      "[CV 4/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.952 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.955 total time=   0.7s\n",
      "[CV 1/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.954 total time=   0.7s\n",
      "[CV 2/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.957 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.956 total time=   0.7s\n",
      "[CV 4/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.951 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.953 total time=   0.5s\n",
      "[CV 1/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.956 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.956 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.957 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.952 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.954 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.957 total time=   0.5s\n",
      "[CV 2/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.960 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.959 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.954 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.957 total time=   0.5s\n",
      "[CV 1/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.957 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.959 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.959 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.954 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.958 total time=   0.8s\n",
      "[CV 1/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.957 total time=   0.9s\n",
      "[CV 2/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.961 total time=   0.8s\n",
      "[CV 3/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.959 total time=   0.8s\n",
      "[CV 4/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.954 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.956 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.958 total time=   0.7s\n",
      "[CV 2/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.960 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.959 total time=   0.8s\n",
      "[CV 4/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.953 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.957 total time=   0.5s\n",
      "[CV 1/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.958 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.960 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.959 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.953 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.957 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.958 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.959 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.959 total time=   0.5s\n",
      "[CV 4/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.954 total time=   0.5s\n",
      "[CV 5/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.957 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.957 total time=   0.5s\n",
      "[CV 2/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.960 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.959 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.953 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.957 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.957 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.960 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.959 total time=   0.5s\n",
      "[CV 4/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.954 total time=   0.5s\n",
      "[CV 5/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.957 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.954 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.957 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.956 total time=   0.5s\n",
      "[CV 4/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.950 total time=   0.5s\n",
      "[CV 5/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.952 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.955 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.957 total time=   0.8s\n",
      "[CV 3/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.956 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.952 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.954 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.954 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.958 total time=   0.5s\n",
      "[CV 3/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.955 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.951 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.952 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.955 total time=   0.6s\n",
      "[CV 2/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.956 total time=   0.5s\n",
      "[CV 3/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.956 total time=   0.7s\n",
      "[CV 4/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.952 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=2, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.954 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.954 total time=   0.7s\n",
      "[CV 2/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.957 total time=   0.8s\n",
      "[CV 3/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.956 total time=   0.9s\n",
      "[CV 4/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.951 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.7;, score=0.953 total time=   0.7s\n",
      "[CV 1/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.955 total time=   0.7s\n",
      "[CV 2/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.958 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.957 total time=   0.7s\n",
      "[CV 4/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.951 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.1, subsample=0.9;, score=0.954 total time=   1.0s\n",
      "[CV 1/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.953 total time=   0.9s\n",
      "[CV 2/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.958 total time=   1.1s\n",
      "[CV 3/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.956 total time=   0.7s\n",
      "[CV 4/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.950 total time=   0.5s\n",
      "[CV 5/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.7;, score=0.953 total time=   0.6s\n",
      "[CV 1/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.955 total time=   0.5s\n",
      "[CV 2/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.957 total time=   0.6s\n",
      "[CV 3/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.956 total time=   0.6s\n",
      "[CV 4/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.952 total time=   0.6s\n",
      "[CV 5/5] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, min_child_weight=3, n_estimators=200, reg_alpha=0.2, subsample=0.9;, score=0.954 total time=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rh/w6cl6rd95vj2g4s03scz_4fc0000gn/T/ipykernel_1984/975800168.py:37: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append({'Model_Specs': best_model,\n",
      "/var/folders/rh/w6cl6rd95vj2g4s03scz_4fc0000gn/T/ipykernel_1984/975800168.py:57: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append({'Model_Specs': model_params,\n",
      "/var/folders/rh/w6cl6rd95vj2g4s03scz_4fc0000gn/T/ipykernel_1984/975800168.py:57: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append({'Model_Specs': model_params,\n",
      "/var/folders/rh/w6cl6rd95vj2g4s03scz_4fc0000gn/T/ipykernel_1984/975800168.py:57: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append({'Model_Specs': model_params,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-Time Train Evaluation Results:\n",
      "Train Gini: 0.9469\n",
      "Train Confusion Matrix:\n",
      "[[66986   537]\n",
      " [ 2135  3343]]\n",
      "\n",
      "In-Time Test Evaluation Results:\n",
      "Test Gini: 0.9142\n",
      "Test Confusion Matrix:\n",
      "[[28623   316]\n",
      " [ 1004  1344]]\n",
      "\n",
      "Out-of-Time Test Evaluation Results:\n",
      "Test Gini: 0.8850\n",
      "Test Confusion Matrix:\n",
      "[[20388   159]\n",
      " [  553   370]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rh/w6cl6rd95vj2g4s03scz_4fc0000gn/T/ipykernel_1984/975800168.py:57: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append({'Model_Specs': model_params,\n"
     ]
    }
   ],
   "source": [
    "results_vif_xgb = train_and_evaluate_pipeline(df_VIF_woe, target='default', time=2020, model=xgb_model, param_grid=param_grid_xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_vif_xgb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresults_vif_xgb\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results_vif_xgb' is not defined"
     ]
    }
   ],
   "source": [
    "results_vif_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model_Specs</th>\n",
       "      <th>Model_Name</th>\n",
       "      <th>In_Time_Train_Gini</th>\n",
       "      <th>In_Time_Test_Gini</th>\n",
       "      <th>Out_of_Time_Test_Gini</th>\n",
       "      <th>In_Time_Train_Confusion_Matrix</th>\n",
       "      <th>In_Time_Test_Confusion_Matrix</th>\n",
       "      <th>Out_of_Time_Test_Confusion_Matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(C=1, class_weight='balanced...</td>\n",
       "      <td>Lo_1</td>\n",
       "      <td>0.880477</td>\n",
       "      <td>0.876069</td>\n",
       "      <td>0.832646</td>\n",
       "      <td>[[56008, 11515], [618, 4860]]</td>\n",
       "      <td>[[23811, 5128], [266, 2082]]</td>\n",
       "      <td>[[17345, 3202], [179, 744]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'C': 0.001, 'class_weight': None, 'max_iter':...</td>\n",
       "      <td>Lo_2</td>\n",
       "      <td>0.834341</td>\n",
       "      <td>0.830179</td>\n",
       "      <td>0.785227</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[[20510, 37], [871, 52]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'C': 0.001, 'class_weight': None, 'max_iter':...</td>\n",
       "      <td>Lo_3</td>\n",
       "      <td>0.872601</td>\n",
       "      <td>0.869718</td>\n",
       "      <td>0.823082</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[[20475, 72], [759, 164]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'C': 0.001, 'class_weight': None, 'max_iter':...</td>\n",
       "      <td>Lo_4</td>\n",
       "      <td>0.869891</td>\n",
       "      <td>0.866591</td>\n",
       "      <td>0.821787</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[[20507, 40], [814, 109]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'C': 0.001, 'class_weight': None, 'max_iter':...</td>\n",
       "      <td>Lo_5</td>\n",
       "      <td>0.842314</td>\n",
       "      <td>0.838324</td>\n",
       "      <td>0.791508</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[[20505, 42], [846, 77]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Model_Specs Model_Name  \\\n",
       "0  LogisticRegression(C=1, class_weight='balanced...       Lo_1   \n",
       "1  {'C': 0.001, 'class_weight': None, 'max_iter':...       Lo_2   \n",
       "2  {'C': 0.001, 'class_weight': None, 'max_iter':...       Lo_3   \n",
       "3  {'C': 0.001, 'class_weight': None, 'max_iter':...       Lo_4   \n",
       "4  {'C': 0.001, 'class_weight': None, 'max_iter':...       Lo_5   \n",
       "\n",
       "   In_Time_Train_Gini  In_Time_Test_Gini  Out_of_Time_Test_Gini  \\\n",
       "0            0.880477           0.876069               0.832646   \n",
       "1            0.834341           0.830179               0.785227   \n",
       "2            0.872601           0.869718               0.823082   \n",
       "3            0.869891           0.866591               0.821787   \n",
       "4            0.842314           0.838324               0.791508   \n",
       "\n",
       "  In_Time_Train_Confusion_Matrix In_Time_Test_Confusion_Matrix  \\\n",
       "0  [[56008, 11515], [618, 4860]]  [[23811, 5128], [266, 2082]]   \n",
       "1                           None                          None   \n",
       "2                           None                          None   \n",
       "3                           None                          None   \n",
       "4                           None                          None   \n",
       "\n",
       "  Out_of_Time_Test_Confusion_Matrix  \n",
       "0       [[17345, 3202], [179, 744]]  \n",
       "1          [[20510, 37], [871, 52]]  \n",
       "2         [[20475, 72], [759, 164]]  \n",
       "3         [[20507, 40], [814, 109]]  \n",
       "4          [[20505, 42], [846, 77]]  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_vif_LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write results of LR and XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_vif_LR.to_csv('../data/results/results_vif_LR.csv', index=False)\n",
    "results_vif_xgb.to_csv('../data/results/results_vif_xgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model_Specs</th>\n",
       "      <th>Model_Name</th>\n",
       "      <th>In_Time_Train_Gini</th>\n",
       "      <th>In_Time_Test_Gini</th>\n",
       "      <th>Out_of_Time_Test_Gini</th>\n",
       "      <th>In_Time_Train_Confusion_Matrix</th>\n",
       "      <th>In_Time_Test_Confusion_Matrix</th>\n",
       "      <th>Out_of_Time_Test_Confusion_Matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n",
       "      <td>XG_1</td>\n",
       "      <td>0.946175</td>\n",
       "      <td>0.91603</td>\n",
       "      <td>0.886228</td>\n",
       "      <td>[[66986, 537], [2135, 3343]]</td>\n",
       "      <td>[[28623, 316], [1004, 1344]]</td>\n",
       "      <td>[[20395, 152], [552, 371]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Model_Specs Model_Name  \\\n",
       "0  XGBClassifier(base_score=None, booster=None, c...       XG_1   \n",
       "\n",
       "  In_Time_Train_Gini In_Time_Test_Gini Out_of_Time_Test_Gini  \\\n",
       "0           0.946175           0.91603              0.886228   \n",
       "\n",
       "  In_Time_Train_Confusion_Matrix In_Time_Test_Confusion_Matrix  \\\n",
       "0   [[66986, 537], [2135, 3343]]  [[28623, 316], [1004, 1344]]   \n",
       "\n",
       "  Out_of_Time_Test_Confusion_Matrix  \n",
       "0        [[20395, 152], [552, 371]]  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select just first row from results_vif_LR as dataframe\n",
    "one_xgb = results_vif_xgb.iloc[0]\n",
    "one_xgb = pd.DataFrame(one_xgb).T\n",
    "one_xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_final = pd.concat([results_vif_LR, one_xgb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA94AAAJOCAYAAABBfN/cAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB070lEQVR4nO3deVxWZf7/8fe9cLOoISLuC+JWmnujmZKYmma5ZlpqpmWWy3c0WkbLLWvSNkdmSq1G09I0M8amMtOYKCrTEq0sU9zGJVRExdRkue/z+4Px/nkLKN5wbm7g9Xw8eIznnOu+zufgNdibc53rWAzDMAQAAAAAAExhLekCAAAAAAAoywjeAAAAAACYiOANAAAAAICJCN4AAAAAAJiI4A0AAAAAgIkI3gAAAAAAmIjgDQAAAACAiQjeAAAAAACYiOANAAAAAICJCN4AAPxPYmKiLBaL+2v//v1F7jMmJsbd38iRI4vcnz/YtWuXBg0apGrVqslms7mvb9u2be428+bNU8uWLRUcHOw+3r9/f0lSZGSke9/MmTO9rmPkyJHufmJiYop0TQAAmMle0gUAAGCG9PR0LVq0SJ999pm2b9+uEydOyDAMValSRY0bN1aHDh3Ut29fde7cWRaLpaTLLTXOnTun3r17a8+ePQW2ef311/XII4/4sKqStWTJEo0aNcq9bRhGCVYDAPBHBG8AQJnz+uuvKzY2VmfPns1z7MiRIzpy5IiSkpL00ksvKTU1VTVq1JAkNWzYUC+++KK7bZUqVYpcy9ixY3XHHXdIkq6//voi91fSvvvuO4/Qfe+996pFixayWCyqXbu2JGnFihXu4/Xq1dODDz6ooKAgNW7cWJL01FNPKSMjQ5J00003eV3L3Xff7f6e1q1b1+t+AAAwm8Xg17IAgDLkxRdf1BNPPOHetlgs6tq1q2688UZVrFhRJ06c0LZt2/TVV1/p/PnzHsEbV/bWW2/pvvvuc2/n5OTIZrN5tImKitK+ffskSVOnTtUzzzzj0xp9jTveAIAr4RlvAECZsWPHDk2ZMsW9HR4erq+++koJCQn661//qilTpujFF1/Uhg0blJaWpvnz5ys4ONjd/nLPeF/6PHFqaqrGjBmjmjVrKjAwUNddd53eeOONPDUV5Rnv7777TqNGjVKjRo0UEhKiihUrqkmTJho1alSeqd5//PGH/va3v6lTp04KCwuTw+FQ9erV1bt3b61atarAc/zwww+6//771bBhQwUHB6tixYpq06aNnnvuOY8ZA/v375fFYvEI3ZJkt9tlsVgUGRnp/h5dCN2S9Oyzz7qvf8mSJZKu/Iz3r7/+qvHjx6tZs2aqWLGiQkJCFBUVpbvvvlvff/+9u92VnvE+evSonnzySbVu3VqVKlVSUFCQGjVqpPHjx+vAgQN52l/t3/GF78nFoVuSxxgqyjPsAICyg6nmAIAy4+9//7ucTqd7e+HChQVOZa5YsaLGjh3r1XkOHjyodu3aKTU11b3v119/1ZgxY2Sz2XT//fd71e/FZs2apZkzZ+a5e5qSkqKUlBT169dPDRs2lJQ7fb579+76+eefPdoeO3ZMn3zyiT755BOtWrVKK1eulN3+///pX7Bggf785z8rJyfH43Pbtm3Ttm3btHz5ciUkJPh0RsCiRYs0btw4ZWVleezft2+f9u3bpxtvvFE33HDDFfvZuHGj+vbtq+PHj3vs37Nnj+bPn6/ly5frww8/VHR0dL6f98XfMQCg/CB4AwDKjISEBPefw8LCNHDgQFPOs3fvXgUFBWns2LEKDg7WggUL9Mcff0iSXnjhhSKHsvfee08zZsxwb4eEhOjuu+9W/fr1tW/fPn344Yce7YcNG+YRugcNGqRmzZppw4YN2rhxoyTp/fff13PPPafp06dLkr755htNmDBBLpdLknTjjTeqV69e+v3337V06VIdP35cv/zyi0aMGKH169erSpUqevHFF/X999/r3XffdZ/rwjPxoaGhqlu3rq6//no999xzOnnypCSpR48euvXWWyVJf/rTny573d9++63GjBnjrslut+uuu+7Stddeq0OHDmndunWF+v6dPn1a/fv3d4fu+vXra8iQIQoODtbq1av1888/KyMjQ3feeadSUlIUGhqap4/C/B1f6XsiFe0ZdgBAGWIAAFBGhISEGJIMSUb79u09ju3YscN97OKv++67z93m888/9zi2b98+97H77rvP49iaNWvcx+bNm+dx7PTp0+5jXbp0yfdcl9O2bVv3ZypUqGDs3LnT4/iZM2eMo0ePGoZhGFu3bvU49xNPPOFul5OTY3Ts2NF9rEqVKobT6TQMwzAGDBjg3h8TE+PebxiGsXnzZo8+f/jhB/exN9980+NYfurXr+8+PmPGjEIfHzhwoHu/1Wo1vvzyS4/PZWZmGgcPHnRvX/x30qVLF/f+uLg49/6wsDAjPT3d43sXERHhPh4XF5dvf1fzd1yY7wkAoHzjGW8AQJlk5ivCatWqpX79+rm3mzZt6nH8wt1eb5w7d05bt251b48YMUJNmjTxaFOhQgVVq1ZNktx3tC+4+Blsm82m4cOHu7dPnDihnTt3SpK+/vpr9/7ExESP93G3b9/eo89vvvnG6+u5Gl999ZX7zz179swzDdzhcKhOnTpX7Ofiazt58qTCw8Pd11axYkWlpaW5jxd0bWb+HQMAyh+CNwCgzLjwOisp91lo46Lno6tVq6YXX3xRL774okJCQop0nsjISI/twMBAj+0LU6W9cfLkSY+6GzRocNn2J06c8NiuXr36ZbcvBMZLP3c5FwdVM11c05Wuu7D9XElB12bm3zEAoPzhGW8AQJnRrVs3paSkSMoNX//+97/ddy2rVKmixx57TJI0Z84cnTt3zuvzBAQEeGwX5931sLAwWSwWd/i+eIXw/Fz6rvGjR48qPDzcY/vS/i987tixY5Kkzp07e9zdvZSvnlO+uKYrXfeV+rmgZs2aio2NLbBtQe//NvPvGABQ/hC8AQBlxoQJE/TGG2+4VzZ/+OGHVb9+fbVu3bpkC7sKISEhatOmjZKTkyVJb7/9tmJjY9WoUSN3mz/++EO///67qlWrlicUL126VM8//7wkyel0atmyZe5jVapUcU+Zvummm7RmzRpJuauijxkzRtdcc41HX3/88Yfee+89nwXvzp07Kz4+XpK0fv16ff311+rUqZP7eE5Ojo4ePeoxsyE/N910k/sVamlpabr11lvVsmVLjzaGYSghIcG9MnxRXBrSz507V+RZFQCAsoXgDQAoM5o3b65nnnlGTz75pKTcQHnDDTfotttuU7t27RQQEKB9+/bp9OnTJVzp5U2ePFmDBw+WJJ05c0atW7d2r2p+8OBBffTRR5o/f7769++vVq1aqVu3bu4V3V944QXt3btXzZs31/r16z2eAZ84caKs1tynzB599FF98MEHMgxDu3fv1vXXX6+BAweqevXqysjI0E8//aQvvvhCZ8+e1YgRI3xy3Y8//rjWrFkjl8slp9Oprl27avDgwWratKmOHDmiTz/9VBMmTNCkSZMu28/IkSP17LPP6vjx48rJyVGnTp101113qVGjRsrMzNTOnTuVmJioo0eP6vPPPy/StHZJeX4RMHToUN10002yWq26995780z3BwCUPwRvAECZMmXKFFWoUEFPPPGEMjMz5XQ69dFHH+mjjz7Kt/3F07L9xV133aWZM2fq6aeflmEYOnv2rBYtWlRg+2XLlqlbt2765ZdfJEmrV6/W6tWrPdrceeed7l9ISLl3l1955RVNnDhROTk5OnjwoOLi4sy5oEK68cYb9frrr7vf452dna3ly5dfdT+hoaH64IMP1K9fPx0/flxnzpzRm2++aULFuTp27KiaNWu63/n9wQcf6IMPPpAkxcTEELwBACyuBgAoe/785z9r3759mjlzpjp37qyIiAjZ7XYFBwerXr166tGjh2bOnKnk5GS9/PLLJV1uvmbMmKFvv/1W9913n6KiohQUFKSQkBBFRUXp3nvv1fXXX+9uW6NGDX333Xd6+eWX1bFjR4WGhsputysiIkK9evXSypUrtXr1atntnr9vHzdunLZu3aoxY8aoSZMmCgkJkd1uV/Xq1dWlSxdNmzZNP/zwg0+v+4EHHtC2bds0duxYXXvttQoJCVFgYKDq1q2rQYMGqXPnzoXq56abbtLPP/+sadOmqV27drrmmmtks9lUuXJltWvXThMmTNCGDRt08803F7nmwMBArV27Vrfeemue6foAAEiSxbh46VQAAAAAAFCsuOMNAAAAAICJCN4AAAAAAJiI4A0AAAAAgIn8Lni/+uqrioyMVFBQkDp06KDNmzcX2DY7O1uzZs1Sw4YNFRQUpFatWmndunUebWbOnCmLxeLxde2115p9GQAAAAAASPKz4P3uu+8qNjZWM2bMUHJyslq1aqWePXvq2LFj+bafOnWqXnvtNf3jH//QL7/8oocfflgDBgzQ1q1bPdo1b95cqamp7q+vvvrKF5cDAAAAAIB/rWreoUMH/elPf9Irr7wiSXK5XKpbt67+7//+T5MnT87TvlatWnrqqac0fvx4974777xTwcHBWrZsmaTcO95r1qzRtm3bfHINAAAAAABczH7lJr6RlZWlLVu2aMqUKe59VqtV3bt318aNG/P9TGZmpoKCgjz2BQcH57mjnZKSolq1aikoKEgdO3bU7NmzVa9evQJryczMVGZmpnvb5XLpxIkTCg8Pl8Vi8ebyAAAAAAB+xjAM/f7776pVq5asVvMmhPtN8D5+/LicTqeqV6/usb969er69ddf8/1Mz549NXfuXN18881q2LChEhISFB8fL6fT6W7ToUMHLVmyRE2bNlVqaqqefvppRUdHa/v27apUqVK+/c6ePVtPP/108V0cAAAAAMBvHTx4UHXq1DGtf7+Zav7bb7+pdu3a+uabb9SxY0f3/ieeeEJffPGFNm3alOczaWlpevDBB/Xhhx/KYrGoYcOG6t69uxYvXqw//vgj3/OcOnVK9evX19y5c/XAAw/k2+bSO94ZGRmqV6+edu3apSpVqhTxSoGSkZ2drc8//1xdu3ZVQEBASZcDeIVxjLKAcYyygrGMsuDEiRNq0qSJTp06pdDQUNPO4zd3vKtWrSqbzaajR4967D969Khq1KiR72ciIiK0Zs0anT9/Xunp6apVq5YmT56sqKioAs9TuXJlNWnSRLt37y6wTWBgoAIDA/Psr1KlisLDwwt5RYB/yc7OVkhIiMLDw/nHEaUW4xhlAeMYZQVjGWWJ2Y8U+82q5g6HQ+3atVNCQoJ7n8vlUkJCgscd8PwEBQWpdu3aysnJ0fvvv69+/foV2PbMmTPas2ePatasWWy1AwAAAABQEL8J3pIUGxurN954Q0uXLtWOHTs0duxYnT17VqNGjZIkjRgxwmPxtU2bNik+Pl579+5VUlKSevXqJZfLpSeeeMLd5rHHHtMXX3yh/fv365tvvtGAAQNks9l0zz33+Pz6AAAAAADlj99MNZekIUOGKC0tTdOnT9eRI0fUunVrrVu3zr3g2oEDBzxWmjt//rymTp2qvXv3qmLFiurdu7fefvttVa5c2d3m0KFDuueee5Senq6IiAh17txZ3377rSIiInx9eQAAAACAcsivgrckTZgwQRMmTMj3WGJiosd2ly5d9Msvv1y2v5UrVxZXaQAAAAB8yOl0Kjs7u6TLQCkWEBAgm81W0mX4X/AGAAAAUL4ZhqEjR47o1KlTJV0KyoDKlSurRo0api+gdjkEbwAAAAB+5ULorlatmkJCQko0MKH0MgxD586d07FjxySpRBfYJngDAAAA8BtOp9MdunmVL4oqODhYknTs2DFVq1atxKad+9Wq5gAAAADKtwvPdIeEhJRwJSgrLoylklwvgOANAAAAwO8wvRzFxR/GEsEbAAAAAAATEbwBAAAAoAw7cuSIevTooQoVKqhy5colXc5ViYmJ0aRJk0q6jCIjeAMAAABAMTl48KDuv/9+1apVSw6HQ/Xr19fEiROVnp5e6D72798vi8Wibdu2FUtNf/vb35Samqpt27Zp165dxdInrg7BGwAAAECZ5HS6lJi4UytWbFZi4k45nS5Tz7d3717dcMMNSklJ0YoVK7R7924tXLhQCQkJ6tixo06cOGHq+QuyZ88etWvXTo0bN1a1atVKpIbyjuANAAAAoMyJj09WZOST6tp1roYOXaSuXecqMvJJxccnm3bO8ePHy+FwaP369erSpYvq1aun2267TZ999pkOHz6sp556SlLuYl9r1qzx+GzlypW1ZMkSSVKDBg0kSW3atJHFYlFMTMxlz7tgwQI1bNhQDodDTZs21dtvv+0+FhkZqffff19vvfWWLBaLRo4cmW8f8+fPV+PGjRUUFKTq1atr0KBB7mPr1q1T586dVblyZYWHh+uOO+7Qnj173Mcv3KFftWqVoqOjFRwcrD/96U/atWuXvvvuO91www2qWLGibrvtNqWlpbk/N3LkSPXv319PP/20IiIidM011+jhhx9WVlZWgdeamZmpxx57TLVr11aFChXUoUMHJSYmXvb74w8I3gAAAADKlPj4ZA0a9JoOHTrpsf/w4ZMaNOg1U8L3iRMn9Omnn2rcuHHud0dfUKNGDQ0bNkzvvvuuDMO4Yl+bN2+WJH322WdKTU1VfHx8gW3/9a9/aeLEiXr00Ue1fft2PfTQQxo1apQ+//xzSdJ3332nXr16afDgwUpNTVVcXFyePr7//nv9+c9/1qxZs7Rz506tW7dON998s/v42bNnFRsbq++//14JCQmyWq0aMGCAXC7PGQQzZszQ1KlTlZycLLvdrqFDh+qJJ55QXFyckpKStHv3bk2fPt3jMwkJCdqxY4cSExO1YsUKxcfH6+mnny7weidMmKCNGzdq5cqV+vHHH3XXXXepV69eSklJueL3tSTZS7oAAAAAACguTqdLEyeuUn751jAki0WaNGmV+vVrLZut+O5DpqSkyDAMXXfddfkev+6663Ty5EmPO74FiYiIkCSFh4erRo0al2370ksvaeTIkRo3bpwkKTY2Vt9++61eeuklde3aVREREQoMDFRwcHCBfR04cEAVKlTQHXfcoUqVKql+/fpq06aN+/idd97p0X7x4sWKiIjQL7/8ouuvv969/7HHHlPPnj0lSRMnTtQ999yjhIQEderUSZL0wAMPuO/qX+BwOLR48WKFhISoefPmmjVrlh5//HE988wzslo9/34OHDigN998UwcOHFCtWrXc51y3bp3efPNNPffcc5f9XpUk7ngDAAAAKDOSklLy3Om+mGFIBw+eVFKSOXdIC3NH2xtJSUmqWLGi+2v58uWSpB07driD7QWdOnXSjh078u1n+fLlHv0kJSWpR48eql+/vqKionTvvfdq+fLlOnfunPszKSkpuueeexQVFaVrrrlGkZGRknKD8MVatmzp/nP16tUlSS1atPDYd+zYMY/PtGrVSiEhIe7tjh076syZMzp48GCe2n/66Sc5nU41adLE4xq++OILj6nv/og73gAAAADKjNTUjGJtV1iNGjWSxWLRjh07NGDAgDzHd+zYobCwMEVERMhiseQJ6NnZ2Zft/4YbbvBY5fxCsL1affv2VYcOHdzbtWvXVnBwsJKTk5WYmKj169dr+vTpmjlzpr777jtVrlxZffr0Uf369fXGG2+oVq1acrlcuv766/M8ix0QEOD+s8ViyXffpdPTr8aZM2dks9m0ZcsW2Ww2j2MVK1b0ul9fIHgDAAAAKDNq1gwt1naFFR4erh49emj+/Pl65JFHPJ7zPnLkiJYvX64RI0bIYrEoIiJCqamp7uMpKSked5gdDockyel0uvcFBwerUaNGec573XXX6euvv9Z9993n3vf111+rWbNm+dZZqVIlVapUKc9+u92u7t27q3v37poxY4YqV66s//znP+rSpYt27typN954Q9HR0ZKkr776qrDfliv64Ycf9Mcff7i/X99++60qVqyounXr5mnbpk0bOZ1OHTt2zF1LaUHwBgAAAFBmREc3Vp06YTp8+GS+z3lbLFKdOmGKjm5c7Od+5ZVXdNNNN6lnz5569tln1aBBA/388896/PHHVbt2bf31r3+VJN1yyy165ZVX1LFjRzmdTv3lL3/xuDNcrVo1BQcHa926dapTp46CgoIUGpr/Lwoef/xxDR48WG3atFH37t314YcfKj4+Xp999lmh6/7oo4+0d+9e3XzzzQoLC9PatWvlcrnUtGlThYWFKTw8XK+//rpq1qypAwcOaPLkyUX7Rl0kKytLDzzwgKZOnar9+/drxowZmjBhQp7nuyWpSZMmGjZsmEaMGKGXX35Zbdq0UVpamhISEtSyZUvdfvvtxVZXceMZbwAAAABlhs1mVVzcYEm5IftiF7bnzRtcrAurXdC4cWN9//33ioqK0uDBg9WwYUONGTNGXbt21caNG1WlShVJ0ssvv6y6desqOjpaQ4cO1WOPPebxnLPdbtff//53vfbaa6pVq5b69etX4Dn79++vuLg4vfTSS2revLlee+01vfnmm1d8BdnFKleurPj4eN1yyy267rrrtHDhQq1YsULNmzeX1WrVypUrtWXLFl1//fV65JFH9OKLL3r9PbpUt27d1LhxY918880aMmSI+vbtq5kzZxbY/s0339SIESP06KOPqmnTpurfv7++++471atXr9hqMoPFMOvp/zLk9OnTCg0N1fHjxxUeHl7S5QBeyc7O1tq1a9W7d2+P36gCpQnjGGUB4xhlhVlj+fz589q3b58aNGigoKAgr/uJj0/WxImrPBZaq1s3TPPmDdbAgW2Lo1QU0ciRI3Xq1Kk87zQvbpcbU+np6apataoyMjJ0zTXXmFYDU80BAAAAlDkDB7ZVv36tlZSUotTUDNWsGaro6Mam3OkGroTgDQAAAKBMstmsiolpWtJlAARvAAAAAIDvLVmypKRL8BnmWQAAAAAAYCKCNwAAAAAAJiJ4AwAAAABgIoI3AAAAAAAmIngDAAAAAGAigjcAAAAAACYieAMAAABAGXbkyBH16NFDFSpUUOXKlQv9uSVLllxVexSM4A0AAAAAxeTgwYO6//77VatWLTkcDtWvX18TJ05Uenp6ofvYv3+/LBaLtm3bViw1/e1vf1Nqaqq2bdumXbt25dsmMjJS8+bN89g3ZMiQAtvj6hC8AQAAAJRNLqf0a6K0aUXu/7qcpp5u7969uuGGG5SSkqIVK1Zo9+7dWrhwoRISEtSxY0edOHHC1PMXZM+ePWrXrp0aN26satWqFfpzwcHBV9UeBSN4AwAAACh7tsRLT0RKL3aVXh+a+79PRObuN8n48ePlcDi0fv16denSRfXq1dNtt92mzz77TIcPH9ZTTz0lSbJYLFqzZo3HZytXrqwlS5ZIkho0aCBJatOmjSwWi2JiYi573gULFqhhw4ZyOBxq2rSp3n77bfexyMhIvf/++3rrrbdksVg0cuTIPJ+PiYnRf//7Xz3yyCOyWCyyWCyS8k41nzlzplq3bq3FixerXr16qlixosaNGyen06kXXnhBNWrUULVq1fTXv/7Vo/9Tp05p9OjRioiI0DXXXKNbbrlFP/zwQyG+o2UHwRsAAABA2bIlXpo/SDp5yHP/ycO5+00I3ydOnNCnn36qcePGKTg42ONYjRo1NGzYML377rsyDOOKfW3evFmS9Nlnnyk1NVXx8QXX+69//UsTJ07Uo48+qu3bt+uhhx7SqFGj9Pnnn0uSvvvuO/Xq1UuDBw9Wamqq4uLi8vQRHx+vOnXqaNasWUpNTVVqamqB59uzZ48++eQTrVu3TitWrNCiRYt0++2369ChQ/riiy/0/PPPa+rUqdq0aZP7M3fddZeOHTumTz75RFu2bFHbtm3VrVu3EpsBUBLsJV0AAAAAABQbl1NaMVFSfgHXkGSRVk6S2vSTrLZiO21KSooMw9B1112X7/HrrrtOJ0+eVFpa2hX7ioiIkCSFh4erRo0al2370ksvaeTIkRo3bpwkKTY2Vt9++61eeuklde3aVREREQoMDFRwcHCBfVWpUkU2m02VKlW64vlcLpcWL16sSpUqqVmzZuratat27typtWvXymq1qmnTpnr++ef1+eefq0OHDvrqq6+0efNmHTt2TIGBge6a16xZo9WrV2vMmDFX/H6UBdzxBgAAAFB27ErKe6fbgyGdOJjbzgSFuaPtjaSkJFWsWNH9tXz5cknSjh071KlTJ4+2nTp10o4dO/LtZ/ny5R79JCVd3fchMjJSlSpVcm9Xr15dzZo1k9Vq9dh37NgxSdIPP/ygM2fOKDw83OO8+/bt0549e67q3KUZd7wBAAAAlB0ZBU+T9qpdITVq1EgWi0U7duzQgAED8hzfsWOHwsLCFBERIYvFkiegZ2dnX7b/G264wWOV8+rVq3tVZ9++fdWhQwf3du3ata/q8wEBAR7bFosl330ul0uSdObMGdWsWVOJiYl5+ipPryojeAMAAAAoO0JrFm+7QgoPD1ePHj00f/58PfLIIx7PeR85ckTLly/XiBEjZLFYFBER4fEcdUpKis6dO+fedjgckiSn8/+vwh4cHKxGjRrlOe91112nr7/+Wvfdd59739dff61mzZrlW2elSpU87lhffM6Lz1dc2rZtqyNHjshutysyMrLY+y8tmGoOAAAAoOxoEi2F1ZFkKaCBRapSN7ddMXvllVeUmZmpnj176ssvv9TBgwe1bt069ejRQ7Vr13av9n3LLbfolVde0datW/X999/r4Ycf9rhrXK1aNQUHB2vdunU6evSoMjIyCjzn448/riVLlmjBggVKSUnR3LlzFR8fr8cee+yqao+MjNSXX36pw4cP6/jx4959A/LRvXt3dezYUf3799f69eu1f/9+ffPNN3rqqaf0/fffF9t5/B3BGwAAAEDZYbVJ91xYufvS8P2/7bvnFevCahc0btxY33//vaKiojR48GA1bNhQY8aMUdeuXbVx40ZVqVJFkvTyyy+rbt26io6O1tChQ/XYY48pJCTE3Y/dbtff//53vfbaa6pVq5b69etX4Dn79++vuLg4vfTSS2revLlee+01vfnmm1d8BdmlZs2apf3796thw4buxd2Kg8Vi0dq1a3XzzTdr1KhRatKkie6++27997//9Xq6fGlkMcx6+r8MOX36tEJDQ3X8+HGFh4eXdDmAV7Kzs7V27Vr17t07z3M4QGnBOEZZwDhGWWHWWD5//rz27dunBg0aKCgoyPuOtsTnrm5+8UJrVermhu52A4tcJ0qPy42p9PR0Va1aVRkZGbrmmmtMq4FnvAEAAACUPe0G5r4ybFdS7kJqoTVzp5ebcKcbuBKCNwAAAICyyWqTro0p6SoAnvEGAAAAAMBMBG8AAAAAAExE8AYAAAAAwEQEbwAAAAB+x+VylXQJKCP8YSyxuBoAAAAAv+FwOGS1WvXbb78pIiJCDodDFsul7+MGrswwDGVlZSktLU1Wq1UOh6PEaiF4AwAAAPAbVqtVDRo0UGpqqn777beSLgdlQEhIiOrVqyerteQmfBO8AQAAAPgVh8OhevXqKScnR06ns6TLQQFOnz6t/v376+WXX1abNm0kScuWLdP69eu1dOlSHThwQHPnztX27dt1zTXXyOVyqVOnTpo0adJl7z7/+c9/1rZt23Ts2DFt3rxZ11xzjdc12mw22e32Ep81QfAGAAAA4HcsFosCAgIUEBBQ0qWgAEFBQZo+fbpGjhypbdu26dChQ3rqqaf07bff6tSpU+rSpYv++te/6rXXXpMknT17VnPnzlVWVtZlw/TgwYP17LPPqnr16nI4HAoKCvLVJZmG4A0AAAAA8EqvXr3UpUsXPfbYY9q6datmzZqlBg0aaOrUqYqJidEDDzzgbluhQgVNmzbtin12797dzJJLBMEbAAAAAOC1l19+WVFRUWrRooUeeughSVJycrJ69OhRwpX5D14nBgAAAADwWlJSkgIDA7V3716dPn063zZ/+9vf1Lp1a9WrV0/r1q3zcYUlj+ANAAAAAPDKiRMn9PDDDys+Pl633367Hn30UUlSmzZttHnzZne7Rx55RNu2bVNUVJTOnz9fUuWWGKaaAwAAAAC8Mn78eA0fPlzt27dX8+bN1bJlS61fv17jx49X69attWTJEo0cOVKS5HK5ymXolrjjDQAAAADwwurVq7V9+3bNnDlTUu7iaYsXL9aDDz6oChUqKCkpSR9++KEiIyPVrl07de7cWV26dFF0dPRl+7399ttVp04dSVLz5s0VExNj8pWYjzveAAAAAICrNmjQIA0aNMhjX5cuXfTf//5XkhQaGqr333//qvv9+OOPi6U+f0LwBgAAAACUWk6nS0lJKUpNzVDNmqGKjm4sm82/JncTvAEAAAAAPtW3b18dOHDAY19YWJg+//zzq+onPj5ZEyeu0qFDJ9376tQJU1zcYA0c2LZYai0OBG8AAAAAgE/9+9//LnIf8fHJGjToNRmG5/7Dh09q0KDXtHr1Q34Tvv3r/jsAAAAAAFfgdLo0ceKqPKFbknvfpEmr5HS6fFtYAQjeAAAAAIBSJSkpxWN6+aUMQzp48KSSklJ8WFXBCN4AAAAAgFIlNTXDY9tmk2JiKslmu3y7kkLwBgAAAACUKjVrhnps2+0WxcRUkt1uuWy7kkLwBgAAAACUKtHRjVWnTpgslvyPWyxS3bphio5u7NvCCkDwBgAAAACUKjabVXFxgyUpT/i+sD1v3mC/eZ+3f1QBAAAAAMBVGDiwrVavfki1a4fJ6TSUnHxWTqehOnXC/OpVYhLv8QYAAAAAlFIDB7ZVv36tlZSUotTUDD3ySKiioxv7zZ3uC/yrGgAAAAAAroLNZlWnTlGqUOGIOnWK8rvQLRG8AQAAAAClnMvl0tatW+VyuUq6lHwRvAEAAAAAMBHBGwAAAAAAExG8AQAAAAClms1mU5cuXWSz2Uq6lHyxqjkAAAAAoFSz2+2KiYkp6TIKxB1vAAAAAECplpWVpWXLlikrK6ukS8kXwRsAAAAAUKoZhqE9e/bIMIySLiVfBG8AAAAAAExE8AYAAAAAwEQEbwAAAABAqWa329WnTx/Z7f65frh/VgUAAAAAQCHZbDa1bdu2pMsoEHe8AQAAAAClWlZWlubPn8+q5gAAAAAAmMEwDKWlpbGqOQAAAAAA5RHBGwAAAAAAExG8AQAAAAClWkBAgIYNG6aAgICSLiVfrGoOAAAAACjVrFarGjVqVNJlFIg73gAAAACAUi0zM1OzZ89WZmZmSZeSL4I3AAAAAKDU89dXiUkEbwAAAAAATEXwBgAAAADARH4XvF999VVFRkYqKChIHTp00ObNmwtsm52drVmzZqlhw4YKCgpSq1attG7duiL1CQAAAAAoXQICAjR27Fi/XdXcr4L3u+++q9jYWM2YMUPJyclq1aqVevbsqWPHjuXbfurUqXrttdf0j3/8Q7/88osefvhhDRgwQFu3bvW6TwAAAABA6WKxWBQaGiqLxVLSpeTLr4L33Llz9eCDD2rUqFFq1qyZFi5cqJCQEC1evDjf9m+//baefPJJ9e7dW1FRURo7dqx69+6tl19+2es+AQAAAAClS1ZWlubMmeO3C6z5zXu8s7KytGXLFk2ZMsW9z2q1qnv37tq4cWO+n8nMzFRQUJDHvuDgYH311Vde93mh34uXoT99+rSk3Knt2dnZV39xgB+4MHYZwyjNGMcoCxjHKCsYy/AnF49Hq7Xw95d9NX79JngfP35cTqdT1atX99hfvXp1/frrr/l+pmfPnpo7d65uvvlmNWzYUAkJCYqPj5fT6fS6T0maPXu2nn766Tz7P//8c4WEhFztpQF+ZcOGDSVdAlBkjGOUBYxjlBWMZfiDCxlw/fr1stlshf7cuXPnzCrJg98Eb2/ExcXpwQcf1LXXXiuLxaKGDRtq1KhRRZ5GPmXKFMXGxrq3T58+rbp166pr164KDw8vatlAicjOztaGDRvUo0cPv110ArgSxjHKAsYxygrGMvxJZmamfvrpJ916660KDAws9OfS09NNrOr/85vgXbVqVdlsNh09etRj/9GjR1WjRo18PxMREaE1a9bo/PnzSk9PV61atTR58mRFRUV53ackBQYG5vuXFRAQwA8VlHqMY5QFjGOUBYxjlBWMZfgDu92uyZMny+FwXNUCa74au36zuJrD4VC7du2UkJDg3udyuZSQkKCOHTte9rNBQUGqXbu2cnJy9P7776tfv35F7hMAAAAAUDoYhqGMjAwZhlHSpeTLb4K3JMXGxuqNN97Q0qVLtWPHDo0dO1Znz57VqFGjJEkjRozwWCht06ZNio+P1969e5WUlKRevXrJ5XLpiSeeKHSfAAAAAIDSLTs7WwsWLPDbxf78Zqq5JA0ZMkRpaWmaPn26jhw5otatW2vdunXuxdEOHDjgsULd+fPnNXXqVO3du1cVK1ZU79699fbbb6ty5cqF7hMAAAAAADP5VfCWpAkTJmjChAn5HktMTPTY7tKli3755Zci9QkAAAAAgJn8aqo5AAAAAADecDgcJV1CgfzujjcAAAAAAFcjMDDQYz0wf8MdbwAAAABAqeZyubR79265XK6SLiVfBG8AAAAAQKmWnZ2t5cuX++2q5gRvAAAAAABMRPAGAAAAAMBEBG8AAAAAQKlmsVgUEREhi8VS0qXki1XNAQAAAAClmsPh0Lhx40q6jAJxxxsAAAAAUKo5nU4lJyfL6XSWdCn5IngDAAAAAEq1nJwcffjhh8rJySnpUvJF8AYAAAAAwEQEbwAAAAAATETwBgAAAACUahaLRQ0bNmRVcwAAAAAAzOBwODR8+PCSLqNA3PEGAAAAAJRqOTk5SkxMZHE1AAAAAADM4HQ69cUXX/A6MQAAAAAAip3LKe1Kyv3zrqTcbT9D8AYAAAAAlE5b4qUnIqW4O3K34+7I3d4SX5JV5UHwBgAAAACUPlvipfmDpJOHZJVTbc4kyyqndPJw7n4/Ct8EbwAAAABA6eJySismSjIkSQFGjvqe+LcCjBz3Pq2c5DfTzgneAAAAAIDSZVeSdPKQezPbYte/q/RVtuXCG7MN6cTB///sdwkjeAMAAAAASpeMVI9Ni2HoGudpWQzjsu1Kiv3KTQAAAAAA8COhNT027XIqJiPxiu1KCne8AQAAAAClS5NoKayOJEsBDSxSlbq57fwAwRsAAAAAULpYbdI9cf/buDR8/2/77nm57fwAwRsAAAAAUPq0GyiNWy2F1fbcX6VO7v52A0umrnzwjDcAAAAAoHRqN1Bq0y939fKM1NxnuptE+82d7gu44w0AAIBSITIyUtu2bStSHx9//LHatWunwMBATZo0qVjqAlDCrDbp2hipwz25/+tnoVvijjcAAADKkcaNG2vx4sV67733dObMmZIuB0A5wR1vAAAAlFqffvqp2rZtq5YtW6pLly765ZdfLtu+SZMmatWqlex27j8B8B2CNwAAAEqlY8eOaejQoVq6dKl+/PFHjRkzRoMGDZJhGCVdGgB4IHgDAACgVNq0aZNatGihFi1aSJKGDRum3377TYcPHy7hygDAE8EbAAAAAAATEbwBAABQKt1444366aeftH37dknSypUrVbt2bdWuXfsKnwQA32JVCQAAAJQaPXv2VEBAgHs7Li5OI0aMUE5OjsLCwvTee+/JYrEU+PmEhATdd999On36tAzD0OrVqzV//nz17dvXF+UDKKcI3gAAACgV9u/fn+/+4cOHF7qPbt266dChQ8VUEQAUDsEbAAAAKIDT6VJSUopSUzNUs2aooqMby2bjaU0AV4fgDQAAgDJl7dq1evLJJ/PsnzJlioYMGVLofuLjkzVx4iodOnTSva9OnTDFxQ3WwIFti6VWAOUDwRsAAABlSu/evdW7d+8i9REfn6xBg17Tpa8EP3z4pAYNek2rVz9E+AZQaMyTAQAAAC7idLo0ceKqPKFbknvfpEmr5HS6fFsYgFKL4A0AAACvREZGatu2bUXq49VXX1WLFi3UunVrXX/99fr73/9ePMUVQVJSisf08ksZhnTw4EklJaX4sCoApRnBGwAAACVm+PDh+umnn7Rt2zZ98803eumll7R169YSrSk1NcNj22aTYmIqyWa7fDsAKAjBGwAAAMXm008/Vdu2bdWyZUt16dJFv/zyy2Xbh4aGuv989uxZZWdnm13iFdWsGeqxbbdbFBNTSXa75bLtAKAgBG8AAAAUi2PHjmno0KFaunSpfvzxR40ZM0aDBg2Skd/D0hdZvXq1mjdvrsjISD322GNq06aNjyrOX3R0Y9WpEyaLJf/jFotUt26YoqMb+7YwAKUWwRsAAADFYtOmTWrRooVatGghSRo2bJh+++03HT58+LKfGzRokH7++Wft3LlTy5Yt086dO31RboFsNqvi4gZLUp7wfWF73rzBvM8bQKHx0wIAAAB+ITIyUh06dNBHH31U0qVo4MC2Wr36IdWuHSan01By8lk5nYbq1AnjVWIArhrBGwAAAB68Xa38xhtv1E8//aTt27dLklauXKnq1aurXbt26t+/f76fufgZ8LS0NP3nP/9Ry5YtvSm72A0c2Fb79z+nDRtidffdg7RhQ6z27XuO0A3gqtlLugAAAACUXj179lRAQIB7Oy4uTiNGjFBOTo7CwsJUt25dde7cWenp6fl+Pi4uTklJSXI4HDIMQ5MmTVKPHj18Vf4V2WxWdeoUpU8++USdOrVhejkArxC8AQAAcEWffvqppkyZ4g7UCxYs0P79+/NtO3z4cEnSokWL9PPPP6tly5Zas2ZNvm1fe+01kyouPi6XS1u3blXPnj1LuhQApRTBGwAAAJd1YbXyxMREtWjRQsuXL3cviGYpYOnvffv2aeHChfryyy/17rvvFlstTqdLSUkpSk3NUM2aoYqObsxdaAB+j+ANAACAy8pvtfLx48fr8OHDqlOnTp72hmHo/vvv1yuvvKLg4GD98MMP+vzzz9W6dWuPdlOmTNGQIUMKXUd8fLImTlylQ4dOuvfVqROmuLjBPHcNwK8RvAEAAFCsTp8+rR9//NEdqs+cOaPs7GyFh4crISHBqz7j45M1aNBruvSV4IcPn9SgQa+ZutK4zWZTly5dZLPZTOkfQNnHvBwAAABcVn6rldeuXVu1a9fOt31oaKjS09O1f/9+7d+/Xy+99JJuvfVWr0O30+nSxImr8oRuSe59kyatktPp8qr/K7Hb7YqJiZHdzj0rAN7hpwcAAADyuNJq5e+9916Bz3cXt6SkFI/p5ZcyDOngwZNKSkpRTEzTYj9/VlaWVq1apcGDB8vhcBR7/wDKPoI3AAAAPFxptfKrNXLkSI0cOdLrelJTMzy2bTYpOrqSkpJ+l9NZcLviYhiG9uzZIyO/W+4AUAhMNQcAAECJczpdSkzcqRUrNisxcafHtPGaNUM92trtFsXEVJLd7nnH/dJ2AOAvuOMNAAAAr6xdu1ZPPvlknv3FvVp5dHRj1akTpsOHT+b7nLfFkts+OrqxV9cBAGYjeAMAAMArvXv3Vu/evYvUR2FXK4+LG6xBg17TpY+VX9ieN2+wae/zttvt6tOnD4urAfAaU80BAABQIq5mtfKBA9tq9eqHVLt2mJxOQ8nJZ+V0GqpTJ8zUV4lJua8Ta9u2La8TA+A1fm0HAACAEnG1q5UPHNhW/fq1VlJSilJTM/TII6GKjm5s2p3uC7KysvTPf/5To0ePZlVzAF7hjjcAAABKRH6rlcfEVNKlN5YvbmezWdWpU5QqVDiiTp2iTA/dUu6q5mlpaaxqDsBrBG8AAACUCG9XK3e5XNq6datcLpcAoDQgeAMAAKBEXFit/NIF0y6wWKS6dVmtHEDpR/AGAABAibDZrIqLGyxJJbJaeWEFBARo2LBhCggIKNE6AJReBG8AAACUGG9WK7fZbOrSpYvPVhm3Wq1q1KiRrFb+0xmAd1jVHAAAACXqalcrt9vtiomJ8Vl9mZmZmjt3rmJjYxUYGOiz8wIoO/i1HQAAAErc1axWnpWVpWXLlikrK8tn9fnyXADKHoI3AAAA/EJhVys3DEN79uzh9V4ASg2CNwAAAAAAJiJ4AwAAAJcREBCgsWPHsqo5AK8RvAEAAOAXCrtaud1uV58+fWS3+2adYIvFotDQUFkKeuE4AFwBwRsAAAB+4cJq5VcK1DabTW3btvXZ68SysrI0Z84cFlgD4DWCNwAAAPxCYVcrz8rK0vz58wnCAEoNgjcAAAD8QmFXKzcMQ2lpaaxqDqDUIHgDAAAAAGAigjcAAABwGQ6HQ5MnT5bD4SjpUgCUUgRvAAAA+IXCrlYeEBCgYcOG+ez1XoZhKCMjg6ntALxG8AYAAIBfKOxq5VarVY0aNZLV6pv/lM3OztaCBQuUnZ3tk/MBKHsI3gAAAPALhV2tPDMzU7Nnz1ZmZqaPKgOAoiF4AwAAwC9czWrlvEoMQGlC8AYAAACugIXVABTF5VeuAAAAAMq5wMBATZkypaTLAFCKcccbAAAAfqGwq5UHBARo7NixPlvV3OVyaffu3XK5XD45H4Cyh+ANAAAAv1DY1cotFotCQ0NlsVh8Uld2draWL1/OquYAvEbwBgAAgF8o7GrlWVlZmjNnDgusASg1CN4AAADwG4RpAGURwRsAAAC4DIvFooiICJ9NbQdQ9rCqOQAAAHAZDodD48aNK+kyAJRi3PEGAACAXyjsauUOh0OTJ0/22bu1nU6nkpOT5XQ6fXI+AGWP3wXvV199VZGRkQoKClKHDh20efPmy7afN2+emjZtquDgYNWtW1ePPPKIzp8/7z4+c+ZMWSwWj69rr73W7MsAAADAVSrsauWGYSgjI0OGYfikrpycHH344YfKycnxyfkAlD1+FbzfffddxcbGasaMGUpOTlarVq3Us2dPHTt2LN/277zzjiZPnqwZM2Zox44dWrRokd599109+eSTHu2aN2+u1NRU99dXX33li8sBAADAVSjsauXZ2dlasGABr/cCUGr4VfCeO3euHnzwQY0aNUrNmjXTwoULFRISosWLF+fb/ptvvlGnTp00dOhQRUZG6tZbb9U999yT5y653W5XjRo13F9Vq1b1xeUAAAAAAOA/i6tlZWVpy5YtmjJlinuf1WpV9+7dtXHjxnw/c9NNN2nZsmXavHmz2rdvr71792rt2rW69957PdqlpKSoVq1aCgoKUseOHTV79mzVq1evwFoyMzM93h95+vRpSbm/XeU3qyitLoxdxjBKM8YxygLGccEu/t5YrQXfHypsu+KSk5OjBg0aKCcnxyfnKy0YyygLfDV+CxW8rVarrFarzp07J4fDIavVesVnbywWy1U9B3P8+HE5nU5Vr17dY3/16tX166+/5vuZoUOH6vjx4+rcubMMw1BOTo4efvhhj6nmHTp00JIlS9S0aVOlpqbq6aefVnR0tLZv365KlSrl2+/s2bP19NNP59n/+eefKyQkpNDXBPijDRs2lHQJQJExjlEWMI7zurB42fr162Wz2S7bzmq1XrFdcQoNDdVnn33mk3OVNoxllGbnzp3zyXkKFbxHjBghi8Xi/sF2YbukJSYm6rnnntP8+fPVoUMH7d69WxMnTtQzzzyjadOmSZJuu+02d/uWLVuqQ4cOql+/vlatWqUHHngg336nTJmi2NhY9/bp06dVt25dde3aVeHh4eZeFGCS7OxsbdiwQT169LjiarGAv2IcoyxgHBfMMAzdeuutcjgcV/xvzT59+vioqtw73t98841uuukm2e1+M2G0xDGWURakp6f75DyF+smxZMmSy24Xh6pVq8pms+no0aMe+48ePaoaNWrk+5lp06bp3nvv1ejRoyVJLVq00NmzZzVmzBg99dRT+U4Fqly5spo0aaLdu3cXWEtgYKACAwPz7A8ICOCHCko9xjHKAsYxygLGcV4ul0unTp1SSEjIZad0u1wu7d27V1FRUT6Z+u1yufTVV1+pc+fO/J3lg7GM0sxXY9dvHlJxOBxq166dEhIS3PtcLpcSEhLUsWPHfD9z7ty5PD9sL9yVL+j1EmfOnNGePXtUs2bNYqocAAAAxaGwq5VnZ2dr+fLlPFsMoNQo8lyZc+fOKT09Pd+ge7kFzPITGxur++67TzfccIPat2+vefPm6ezZsxo1apSk3CnutWvX1uzZsyXlTjGaO3eu2rRp455qPm3aNPXp08cdwB977DH16dNH9evX12+//aYZM2bIZrPpnnvuKeKVAwAAAABwZV4Fb5fLpRdeeEH/+Mc/dOTIkQLbXVggo7CGDBmitLQ0TZ8+XUeOHFHr1q21bt0694JrBw4c8LjDPXXqVFksFk2dOlWHDx9WRESE+vTpo7/+9a/uNocOHdI999yj9PR0RUREqHPnzvr2228VERFxlVcNAACA8shqtapNmzasaA7Aa14F78mTJ+ull15S8+bNdeeddxbrgmMTJkzQhAkT8j2WmJjosW232zVjxgzNmDGjwP5WrlxZbLUBAADAXA6H44ptLBaLIiIifLbYb0BAgPr27euTcwEom7wK3suWLVOvXr20du3a4q4HAAAA5VRgYKCmTJlyxXYOh0Pjxo3zQUW5srOz9cknn+i2225jETEAXvFqvszJkyfVr1+/4q4FAAAA5ZjL5dLu3bvlcrku287pdCo5OfmqH2ssSl1bt269Yl0AUBCvgneLFi2Umppa3LUAAIByLjIyUtu2bStSHzNnzlRERIRat26t1q1ba9iwYcVTHExX2NXKc3Jy9OGHHyonJ8dHlQFA0XgVvGfMmKGFCxfq4MGDxV0PAABAkQ0bNkzbtm3Ttm3btHz58pIuBwBQznn1jPeWLVtUv359NWvWTAMGDFCDBg3cr++6wGKxaNq0acVSJAAAKL8+/fRTTZkyRTk5OQoLC9OCBQvUrFmzki4L5YjNZlOXLl3y/PcuABSWV8F75syZ7j8vW7Ys3zYEbwAAUFTHjh3T0KFDlZiYqBYtWmj58uUaNGiQfv7558uuaP3ee+/p888/V3h4uKZNm6auXbv6sGp4q7CrlVssFjVs2NBnq5rb7XbFxMT45FwAyiavgve+ffuKuw4AAIA8Nm3apBYtWqhFixaScqeQjx8/XocPH1adOnXy/czDDz+sp556SgEBAfr66681YMAAfffdd6pfv74vS4cXCrtaucPh0PDhw31QUa6srCytWrVKgwcPLtTrzgDgUl4Fb/7hAgAA/qpGjRruP3fq1Elt2rTR999/z3+/lAJOp1M//PCDWrVqddlp3Tk5Ofrqq6/UuXNn2e1e/efsVTEMQ3v27JFhGKafC0DZ5NXiagAAAL5w44036qefftL27dslSStXrlTt2rVVu3btAj9z6NAh959TUlK0bds29x1z+LfCrlbudDr1xRdf+Ox1YgBQVIX6FeGsWbNksVj01FNPyWq1atasWVf8DM94AwAAb/Ts2VMBAQHu7bi4OI0YMcK9uNp777132Wd7n3rqKW3ZskV2u102m02vvvqqmjRp4ovSAQDIV6GC98yZM2WxWPSXv/xFDofDY3G1ghC8AQDA1dq/f3+++6/med6lS5cWUzVALrvdrj59+vhkWjuAsqlQPz0uLKZ2YTEJFlcDAABlhdPpUlJSilJTM1SzZqiioxvLZuNpvJJQ2NXKrVar2rRpI6vVN39PNptNbdu29cm5AJRNhQrely5GwuIkAACgJK1du1ZPPvlknv1TpkzRkCFDCt1PfHyyJk5cpUOHTrr31akTpri4wRo4kKDla4VdrTwgIEB9+/b1QUW5srKy9M9//lOjR49mVXMAXmG+DAAAKHV69+6t3r17F6mP+PhkDRr0mi5dqPrw4ZMaNOg1rV79EOHbxwq7Wnl2drY++eQT3XbbbR7rAZjFMAylpaWxqjkAr11V8H733XcVFhamW2+9VZL0+++/a9iwYXnaRUVFad68ecVSIAAAQHFzOl2aOHFVntAtSYYhWSzSpEmr1K9fa6ad+9CF1co7dux42eDtcrm0detW9ezZ04fVAYD3Ch28P//8cw0dOlTvv/++e19WVpY++uijPG0tFosGDBigLl26FE+VAAAAxSgpKcVjevmlDEM6ePCkkpJSFBPT1IeVAQDKokL/Cnf58uVq0qSJ+vfvn+fYZ599JpfLJZfLJafTqYYNG+rtt98uzjoBAACKTWpqhse2zSbFxFSSzXb5diifAgICNGzYMJ9MawdQNhU6eH/zzTe64447rtjOYrFo0KBB+uabb4pUGAAAgFlq1gz12LbbLYqJqSS73XLZdjBXYVcrt9ls6tKli2yX/qbExLoaNWrks1XUAZQ9hf7pcfDgQTVp0sTzw1arwsPD86zuWK9ePR08eLB4KgQAAChm0dGNVadOmAp6a5XFItWtG6bo6Ma+Laycu7Ba+ZXuLNvtdsXExPjsvdqZmZmaPXu2MjMzfXI+AGVPoYN3dnZ2nt8qhoWFKS0tTZ07d/bYHxAQoOzs7OKpEAAAlIjIyEht27atSH38/e9/1/XXX68WLVqoZcuWWrZsWfEUV0Q2m1VxcYMlKU/4vrA9b95gFlbzsezsbP373/++4n9HZmVladmyZcrKyvJRZfLpuQCUPYX+1yQiIkJ79+4tVNu9e/cqIiLC66IAAEDZ0Lx5c3399df66aef9PHHH2vSpEnas2dPSZclSRo4sK1Wr35ItWuHyek0lJx8Vk6noTp1wniVWAm5sFq5y+W6bDvDMLRnzx5e7wWg1Ch08G7fvr3i4+Ov2M4wDMXHx6t9+/ZFKgwAAPifTz/9VG3btlXLli3VpUsX/fLLL5dt361bN4WG5j4nXbduXdWoUcOvHkcbOLCt9u9/Ths2xOruuwdpw4ZY7dv3HKEbAFCsCh28R44cqV9//VVTpky5bLspU6Zo165dGjlyZFFrAwAAfuTYsWMaOnSoli5dqh9//FFjxozRoEGDCn3X8bPPPtPJkyf1pz/9yeRKr47NZlWnTlGqUOGIOnWKYno58ggICNDYsWNZ1RyA1wr9L0ufPn3Ur18/vfDCC+rSpYuWLl2qH374Qfv27dMPP/ygpUuX6uabb9aLL76ofv36qU+fPmbWDQAAfGzTpk1q0aKFWrRoIUkaNmyYfvvtNx0+fPiKn/3pp580atQovfvuu6pQoYLZpV61wk5xhrkKu1q53W5Xnz59fLO4msspy84vFJqyXpadX0gup/nnBFDmXNVPqxUrVujhhx/WW2+9pa+++irPccMwNGLECC1cuLDYCgQAAKXbL7/8ojvuuEOLFy/OsyArcLELq5Vfic1mU9u2PngcYEu8tGKisk6laU7dKZp8cLQCK0dI98RJ7Qaaf34AZcZVzaUKCgrSkiVL9MMPP2jatGkaMGCAunXrpgEDBmjatGn64YcftGTJEgUFBZlVLwAAKCE33nijfvrpJ23fvl2StHLlStWuXVu1a9cu8DM7duxQ79699frrr6tHjx6+KhWlVGFXK8/KytL8+fPNXWl8S7w0f5B08pDn/pOHc/dvufLaRwBwgVfzcy6eZgYAAMqunj17ejzXGhcXpxEjRignJ0dhYWF67733ZCnoZdiS/vznPysjI0N/+ctf9Je//EWS9Pzzz6tnz56m1341CjvFGeYq7GrlhmEoLS3NvFXNXU5pxURJ+fVvSLJIKydJbfpJVsYMgCvzwYMxAACgNNq/f3+++4cPH17oPjZs2FBM1ZirsFOcUU7sSsp7p9uDIZ04mNvu2hhfVQWgFGPZTgAA4FecTpcSE3dqxYrNSkzcKafT/AXPCjvFGeVERqrHpsPI1OSDs+UwMi/bDgAKwh1vAABQJGvXrtWTTz6ZZ/+UKVM0ZMiQq+orPj5ZEyeu0qFDJ9376tQJU1zcYFPfrV3YKc4wV2FXKw8ICNCwYcPMe71XaE2PTYukwEtDdz7tAKAgBG8AAFAkvXv3Vu/evYvcT3x8sgYNek2XZt/Dh09q0KDXtHr1Q6aGb5S8wq5WbrVa1ahRI/MKaRIthdXJXUgt3+e8LVKVOrntAKAQmGoOAABKnNPp0sSJq/KEbknufZMmrfLJtHOUnMKuVp6ZmanZs2crMzOfu9DFwWrLfWWYpNz73Rf73/bd81hYDUChEbwBAECJS0pK8ZhefinDkA4ePKmkpBRTzl/YKc4w19WsVm768/jtBkrjVkthl7wur0qd3P28xxvAVeBfFwAAUOJSUzM8tm02KTq6kpKSfpfTWXC74lLYKc4oZ9oNzH1l2K6k3IXUQmvmTi/nTjeAq+RV8I6KirrscYvFouDgYNWrV0+33nqrHnzwQVWoUMGrAgEAQNlXs2aox7bdblFMTCVt3HhGTqdRYLvikpWVpX/+858aPXq0HA6HKedAKWW18cowAEXm1VTzevXqyW63a//+/Tp58qQqV66sypUr6+TJk9q/f7/sdruCg4P17bffKjY2Vu3atVNaWlpx1w4AAMqI6OjGqlMnTJZLH6f9H4tFqls3TNHRjU05/9VMcYZ5CrtaeUBAgMaOHWvequYAUMy8Ct7z5s3TiRMnNH/+fB07dkzJyclKTk5WWlqaXnnlFZ04cUKLFi3S8ePH9Y9//EMpKSmaPn16cdcOAADKCJvNqri4wZKUJ3xf2J43b7BsNpanKcsurFZutV7+79lisSg0NFSWgn5TAwB+xqt/vR577DENGTJEDz/8sMdvGu12u8aNG6e77rpLjz76qKxWq8aPH6977rlHH3/8cbEVDQAAyp6BA9tq9eqHVLt2mJxOQ8nJZ+V0GqpTJ4xXiZUThV2tPCsrS3PmzDF/gTUAKCZePeO9adMm3XXXXQUeb9mypd5++2339k033aT33nvPm1MBAIByZODAturXr7WSklKUmpqhRx4JVXR0Y9PvdBd2ijPMR5gGUBZ59a9YYGCgvvvuuwKPb968WYGBge7tzMxMVaxY0ZtTAQCAcsZms6pTpyhVqHBEnTpF+WR6eWGnOAMA4A2v/nXp27ev3nzzTc2ZM0fnzp1z7z937pxmz56tpUuXqm/fvu7933zzjZo0aVL0agEAQLngcrm0detWuVwun5yvsFOcAQDwhldTzV966SVt3bpVTz75pKZPn65atWpJkn777Tfl5OSoRYsWevHFFyVJ58+fV1BQkMaPH198VQMAABQzpjiXvMKuVu5wODR58mRe/Qag1PAqeFepUkWbNm3SP//5T3300Ufat2+fJKlbt27q06ePxzswg4KCPJ73BgAAAPJTqNXKXU4Zv36pjN/+q6q16sty7c2579oGAD/mVfCWcn/TOG7cOI0bN6446wEAAJDNZlOXLl1ksxGoypMLq5VPnjzZY70gty3x0oqJyj6VpgV1p2jywYcVWDlCuidOajfQ9wUDQCGxgggAAPA7drtdMTExstu9vkdwVQo7xRkmcjmlXUm5f96VlLt9sS3x0vxB0slDnvtPHs7dvyXeN3UCgBe8/tfs7Nmzeuedd5SSkqL09HQZhuFx3GKxaNGiRUUuEAAAlD9ZWVlatWqVBg8e7JPneAs1xRnm+d+dbJ1Kk+pOkeLukC6+k+1y5h6Xkc+HDUkWaeUkqU0/pp0D8EteBe/Nmzfrjjvu0PHjxwtsQ/AGAADeMgxDe/bsyfOLfbNccYozzHPhTrYMyXLR9/7Cnexxq6UKVfLc6Xa4Ll6B3pBOHMy9U35tjC+qBoCr4tVU89jYWPdvoo8fPy6Xy5Xny+l0XrkjAAAAlF+X3Ml2GJmafHC2HEame59WTsoN4RcJNDI15dBsBRqXvP4tI9X0kgHAG14F7y1btujRRx/VoEGDVKVKleKuCQAAAOXBriSPO9mGLMqwXyNDFvcenTgo/Z5WuP5CaxZ/jQBQDLwK3tdcc43Cw8OLuxYAAABJuYur9enTx2eLq6GEXHKH2iWrfglpLtel/4laKUIKqyOpoGfwLVKVulKTaFPKBICi8ip4Dxw4UJ9++mlx1wIAACAp93Vibdu29dnrxBwOhyZPnuyThdxwkUvuUNvlVExGouy65JHFsNq5C61Jyhu+/7d99zwWVgPgt7wK3s8//7yOHTum//u///PpwicAAKB8yMrK0vz585WVleWT8xmGoYyMDP6bxteaRBf+Tna7gbkLrYXV9mxSpU7uft7jDcCPeTV/q3LlyrJYLNq8ebPmz5+fbxuLxaKcnJwiFQcAAMonwzCUlpbmsyCcnZ2tBQsWsKq5r1ltuXey5w9Sbvi++O87nzvZ7QbmvjJsV1LuNPXQmrmhnDvdAPycV8F7xIgRvOcSAAAARXfhTvaKiZ6vDKtSJzd0X3on22rjlWEASh2vgveSJUuKuQwAAACUW9zJBlDGsVQoAADwOwEBARo2bJgCAgJ8dk4WVith3MkGUIYRvAEAgN+xWq1q1KiRz84XGBioKVOm+Ox8AIDypVCrmlutVtntdvfKolarVTab7bJfvHcTAAB4KzMzU7Nnz1ZmZqZPzudyubR79265XC6fnA8AUL4UKh1fWEztwrs0WVwNAACYzVevEpNyVzVfvnw5q5oDAExRqOB96WJqLK4GAAAAAEDhFGqq+aW+/PJLpaWlFXj8+PHj+vLLL70uCgAAAACAssKr4N21a1dt2LChwOMJCQnq2rWr10UBAIDyLSAgQGPHjvXZquYWi0URERE8SgcAMIVXK6AZhnHZ406nU1arV5keAABAFotFoaGhPgvCDodD48aN88m5AADlj9fp+HL/EH7zzTeqWrWqt10DAIByLisrS3PmzPHZAmtOp1PJyclyOp0+OR8AoHwp9B3vuLg4xcXFubcnTZqkp556Kk+7kydP6vTp07r//vuLp0IAAACT5eTk6MMPP1Tz5s3db3EBAKC4FDp4V65cWfXr15ck7d+/X+Hh4apevbpHG4vFouuvv1433nijHnnkkeKtFAAAAACAUqjQwfu+++7TfffdJ0lq0KCB5syZo759+5pWGAAAAAAAZYFXi6vt27evuOsAAABwczgcmjx5shwOh0/OZ7FY1LBhQ1Y1BwCYwqvF1dLT07Vjxw6Pffv27dP//d//adiwYfr000+LpTgAAFA+GYahjIyMK75Jpbg4HA4NHz7cZ0EfAFC+eBW8J06c6J52LklnzpxRdHS0Xn31Va1YsUK33367vvzyy2IrEgAAlC/Z2dlasGCBsrOzfXK+nJwcJSYmKicnxyfnAwCUL14F740bN6p3797u7XfffVe//fab1q5dq99++03XXXedXnjhhWIrEgAAwExOp1NffPEFrxMDAJjCq+B99OhR1a1b1739ySef6IYbblCvXr1Uo0YNjRw5Ulu3bi22IgEAAAAAKK28Ct4BAQH6448/3NtffPGFunTp4t6uXLmy0tPTi14dAAAot3jeGgBQVngVvJs0aaL3339fhmHo3//+t06cOKFu3bq5jx88eFBVqlQptiIBAED5EhgYqClTpigwMNAn57NarWrTpo2sVq/+0wgAgMvy6nVi48eP18iRIxUWFqZz584pKirKI3gnJSWpRYsWxVYkAAAoX1wul/bu3auoqCifhOGAgAD17dvX9PMAAMonr/4lGzFihJYuXapu3bpp+PDh+uSTTxQQECAp91Vjp06d0uDBg4u1UAAAUH5kZ2dr+fLlPlvVPDs7W//+9799dj4AQPni1R1vSbr33nt177335tkfHh6uLVu2FKkoAAAAX3K5XNq6dat69uxZ0qUAAMqgIs/d2r17t77++mtlZGQURz0AAAAAAJQpXgfvjz76SA0bNlTTpk118803u+9yHzt2TI0aNdLq1auLrUgAAFC+WCwWRUREyGKxlHQpAAAUmVfBOzExUQMGDFCVKlU0Y8YMGYbhPlatWjU1bNhQK1euLLYiAQBA+eJwODRu3DifvVLMZrOpS5custlsPjkfAKB88Sp4z5o1S61atdKmTZs0fvz4PMc7duyo5OTkIhcHAADKJ6fTqeTkZDmdTp+cz263KyYmRna718vfAABQIK+C93fffadhw4YV+HqPOnXq6MiRI0UqDAAAlF85OTn68MMPlZOT45PzZWVladmyZcrKyvLJ+QAA5YtXwdvlcikwMLDA48ePH/fZ1DAAAICiMgxDe/bs8Xh8DgCA4uJV8L7uuuuUlJRU4PGPPvpIrVq18rooAAAAAADKikIH7wMHDuiPP/6QJD3wwANavXq1Fi1aJJfLJSl39dFz587pz3/+szZu3KgxY8aYUzEAACjzLBaLGjZsyKrmAIAyodDBu0GDBvrXv/4lSRo7dqyGDBmiBx98UI0bN5bFYtE999yj0NBQvfLKKxo5cqSGDRtmWtEAAKBsczgcGj58uM8eXbPb7erTpw+LqwEATFHo4H3pM0/Lli3T+++/r27duunaa69VlSpV1Lt3b7333ntatGhRsRcKAADKj5ycHCUmJvpscTWbzaa2bdvyOjEAgCmK9GvdAQMGaMCAAcVVCwAAgKTc14l98cUX6tixo0/uQmdlZemf//ynRo8ezQKxAIBi59XiamZ69dVXFRkZqaCgIHXo0EGbN2++bPt58+apadOmCg4OVt26dfXII4/o/PnzReoTAACUL4ZhKC0tjVXNAQCmuKpfISclJV3VlK8RI0ZcVTHvvvuuYmNjtXDhQnXo0EHz5s1Tz549tXPnTlWrVi1P+3feeUeTJ0/W4sWLddNNN2nXrl0aOXKkLBaL5s6d61WfAAAAAAAUp6sK3q+//rpef/31K7YzDEMWi+Wqg/fcuXP14IMPatSoUZKkhQsX6uOPP9bixYs1efLkPO2/+eYbderUSUOHDpUkRUZG6p577tGmTZu87hMAAJQ8q9WqNm3ayGr1u8l5AABctasK3mPGjNGNN95oSiFZWVnasmWLpkyZ4t5ntVrVvXt3bdy4Md/P3HTTTVq2bJk2b96s9u3ba+/evVq7dq3uvfder/sEAAAlLyAgQH379vXp+YYNG6aAgACfnRMAUH5cVfCOjo52310ubsePH5fT6VT16tU99levXl2//vprvp8ZOnSojh8/rs6dO8swDOXk5Ojhhx/Wk08+6XWfkpSZmanMzEz39unTpyVJ2dnZys7O9ur6gJJ2YewyhlGaMY7Lj+zsbK1fv1633nqrz8Jw/fr15XQ65XQ6TT0P4xhlBWMZZYGvxm+pflllYmKinnvuOc2fP18dOnTQ7t27NXHiRD3zzDOaNm2a1/3Onj1bTz/9dJ79n3/+uUJCQopSMlDiNmzYUNIlAEXGOC77nE6nfvrpJ7lcLp+84svpdOrnn39W8+bNffZKMcYxygrGMkqzc+fO+eQ8fhO8q1atKpvNpqNHj3rsP3r0qGrUqJHvZ6ZNm6Z7771Xo0ePliS1aNFCZ8+e1ZgxY/TUU0951ackTZkyRbGxse7t06dPq27duuratavCw8O9vUSgRGVnZ2vDhg3q0aMHUylRajGOy4/MzEz99NNPuvXWWxUYGFimzsc4RlnBWEZZkJ6e7pPz+E3wdjgcateunRISEtS/f39JksvlUkJCgiZMmJDvZ86dO5dn0ZULv6U2DMOrPiUpMDAw3390AwIC+KGCUo9xjLKAcVz2uVwuSb77u/b1+Xx9LsBMjGWUZr4au4UO3hf+QTJTbGys7rvvPt1www1q37695s2bp7Nnz7pXJB8xYoRq166t2bNnS5L69OmjuXPnqk2bNu6p5tOmTVOfPn3cAfxKfQIAAP9js9nUpUsXn037BgDATH5zx1uShgwZorS0NE2fPl1HjhxR69attW7dOvfiaAcOHPC4wz116lRZLBZNnTpVhw8fVkREhPr06aO//vWvhe4TAAD4H7vdrpiYGJ+dLyAgQGPHjuWuHQDAFH4VvCVpwoQJBU4DT0xM9Ni22+2aMWOGZsyY4XWfAADA/2RlZWnVqlUaPHiwHA6H6eezWCwKDQ2VxWIx/VwAgPLHeuUmAAAAvmUYhvbs2SPDMHxyvqysLM2ZM0dZWVk+OR8AoHwheAMAAAAAYCKCNwAAAAAAJiJ4AwAAv2O329WnTx/Z7X63HA0AAFeNf80AAIDfsdlsatu2rc/O53A4NHnyZJ8s5AYAKH+44w0AAPxOVlaW5s+f77PFzgzDUEZGhs8WcwMAlC8EbwAA4HcMw1BaWppvgrDLqeyf/6MFCxYo++f/SC6n+ecEAJQrBG8AAFB+bYmXnoiU4u7I3Y67I3d7S3xJVgUAKGMI3gAAoHzaEi/NHySdPOS5/+Th3P2EbwBAMSF4AwAAvxMQEKBhw4YpICDAnBO4nNKKiZL+/1R2hyvzf3/6376Vk5h2DgAoFgRvAADgd6xWqxo1aiSr1aT/VNmV5HGnO9DI1JRDsxVoXBS+TxzMbQcAQBERvAEAgN/JzMzU7NmzlZmZeeXG3shILd52AABcBsEbAAD4JVNfJRZas3jbAQBwGQRvAABQ/jSJlsLqSLIU0MAiVamb2w4AgCIieAMAgPLHapPuifvfxqXh+3/bd8/LbQcAQBERvAEAgN8JCAjQ2LFjzVvVXJLaDZTGrZbCanvur1Ind3+7geadGwBQrthLugAAAIBLWSwWhYaGymIpaCp4MWk3UGrTL3f18ozU3Ge6m0RzpxsAUKy44w0AAPxOVlaW5syZY+4CaxdYbdK1MVKHe3L/l9ANAChmBG8AAOBfXM7///7sXUm52wAAlGIEbwAA4D+2xEtPREpxd+Rux92Ru70lviSrAgCgSAjeAADAP2yJl+YPkk4e8tx/8nDufsI3AKCUIngDAICS53JKKyZKMiRJDiNTkw/OlsPIdO/TyklMOwcAlEoEbwAAUPJ2JXnc6TZkUYb9Ghnud2wb0omD///ZbwAAShGCNwAAKHkZqR6bLln1S0hzuS79T5VL2gEAUBrwHm8AAFDyQmt6bNrlVExG4hXbAQBQGnDHGwAAlLwm0VJYHck9tfxSFqlK3dx2AACUMgRvAABQ8qw26Z64/21cGr7/t333vNx2AACUMgRvAADgH9oNlMatlsJqe+6vUid3f7uBJVMXAABFxDPeAADAf7QbKLXpl7t6eUZq7jPdTaK50w0AKNUI3gAAwL9YbdK1MSVdBQAAxYap5gAAAAAAmIjgDQAAAACAiQjeAAAAAACYiOANAAAAAICJCN4AAAAAAJiI4A0AAAAAgIkI3gAAAAAAmIjgDQAAAACAiQjeAAAAAACYiOANAAAAAICJCN4AAAAAAJiI4A0AAAAAgIkI3gAAAAAAmIjgDQAAAACAiQjeAAAAAACYiOANAAAAAICJCN4AAAAAAJiI4A0AAAAAgIkI3gAAAAAAmIjgDQAAAACAiQjeAAAAAACYiOANAAAAAICJCN4AAAAAAJiI4A0AAAAAgIkI3gAAAAAAmIjgDQAAAACAiQjeAAAAAACYiOANAAAAAICJCN4AAAAAAJiI4A0AAAAAgIkI3gAAAAAAmIjgDQAAAACAiQjeAAAAAACYiOANAAAAAICJCN4AAAAAAJiI4A0AAAAAgIkI3gAAAAAAmIjgDQAAAACAiQjeAAAAAACYiOANAAAAAICJCN4AAAAAAJiI4A0AAAAAgIkI3gAAAAAAmIjgDQAAAACAiQjeAAAAAACYiOANAAAAAICJCN4AAAAAAJiI4A0AAAAAgIkI3gAAAAAAmIjgDQAAAACAiQjeAAAAAACYiOANAAAAAICJCN4AAAAAAJiI4A0AAAAAgIkI3gAAAAAAmIjgDQAAAACAiQjeAAAAAACYiOANAAAAAICJCN4AAAAAAJjIL4P3q6++qsjISAUFBalDhw7avHlzgW1jYmJksVjyfN1+++3uNiNHjsxzvFevXr64FAAAAABAOWcv6QIu9e677yo2NlYLFy5Uhw4dNG/ePPXs2VM7d+5UtWrV8rSPj49XVlaWezs9PV2tWrXSXXfd5dGuV69eevPNN93bgYGB5l0EAAAAAAD/43d3vOfOnasHH3xQo0aNUrNmzbRw4UKFhIRo8eLF+bavUqWKatSo4f7asGGDQkJC8gTvwMBAj3ZhYWG+uBwAAAAAQDnnV8E7KytLW7ZsUffu3d37rFarunfvro0bNxaqj0WLFunuu+9WhQoVPPYnJiaqWrVqatq0qcaOHav09PRirR0AAAAAgPz41VTz48ePy+l0qnr16h77q1evrl9//fWKn9+8ebO2b9+uRYsWeezv1auXBg4cqAYNGmjPnj168sknddttt2njxo2y2Wx5+snMzFRmZqZ7+/Tp05Kk7OxsZWdne3NpQIm7MHYZwyjNGMcoCxjHKCsYyygLfDV+/Sp4F9WiRYvUokULtW/f3mP/3Xff7f5zixYt1LJlSzVs2FCJiYnq1q1bnn5mz56tp59+Os/+zz//XCEhIcVfOOBDGzZsKOkSgCJjHKMsYByjrGAsozQ7d+6cT87jV8G7atWqstlsOnr0qMf+o0ePqkaNGpf97NmzZ7Vy5UrNmjXriueJiopS1apVtXv37nyD95QpUxQbG+vePn36tOrWrauuXbsqPDy8kFcD+Jfs7Gxt2LBBPXr0UEBAQEmXA3iFcYyygHGMsoKxjLLAV48g+1XwdjgcateunRISEtS/f39JksvlUkJCgiZMmHDZz7733nvKzMzU8OHDr3ieQ4cOKT09XTVr1sz3eGBgYL6rngcEBPBDBaUe4xhlAeMYZQHjGGUFYxmlma/Grl8triZJsbGxeuONN7R06VLt2LFDY8eO1dmzZzVq1ChJ0ogRIzRlypQ8n1u0aJH69++f5470mTNn9Pjjj+vbb7/V/v37lZCQoH79+qlRo0bq2bOnT64JAAAAAFB++dUdb0kaMmSI0tLSNH36dB05ckStW7fWunXr3AuuHThwQFar5+8Ldu7cqa+++krr16/P05/NZtOPP/6opUuX6tSpU6pVq5ZuvfVWPfPMM7zLGwAAAABgOr8L3pI0YcKEAqeWJyYm5tnXtGlTGYaRb/vg4GB9+umnxVkeAAAAAACF5ndTzQEAAAAAKEsI3gAAAAAAmIjgDQAAAACAiQjeAAAAAACYiOANAAAAAICJCN4AAAAAAJiI4A0AAAAAgIkI3gAAAAAAmIjgDQAAAACAiQjeAAAAAACYiOANAAAAAICJCN4AAAAAAJiI4A0AAAAAgIkI3gAAAAAAmIjgDQAAAACAiQjeAAAAAACYiOANAAAAAICJCN4AAAAAAJiI4A0AAAAAgIkI3gAAAAAAmIjgDQAAAACAiQjeAAAAAACYiOANAAAAAICJCN4AAAAAAJiI4A0AAAAAgIkI3gAAAAAAmIjgDQAAAACAiQjeAAAAAACYiOANAAAAAICJCN4AAAAAAJiI4A0AAAAAgIkI3gAAAAAAmIjgDQAAAACAiQjeAAAAAACYiOANAAAAAICJCN4AAAAAAJiI4A0AAAAAgIkI3gAAAAAAmIjgDQAAAACAiQjeAAAAAACYiOANAAAAAICJCN4AAAAAAJiI4A0AAAAAgIkI3gAAAAAAmIjgDQAAAACAiQjeAAAAAACYiOANAAAAAICJCN4AAAAAAJiI4A0AAAAAgIkI3gAAAAAAmIjgDQAAAACAiQjeAAAAAACYiOANAAAAAICJCN4AAAAAAJiI4A0AAAAAgIkI3gAAAAAAmIjgDQAAAACAiQjeAAAAAACYiOANAAAAAICJCN4AAAAAAJiI4A0AAAAAgIkI3gAAAAAAmIjgDQAAAACAiQjeAAAAAACYiOANAAAAAICJCN4AAAAAAJiI4A0AAAAAgIkI3gAAAAAAmIjgDQAAAACAiQjeAAAAAACYiOANAAAAAICJCN4AAAAAAJiI4A0AAAAAgIkI3gAAAAAAmIjgDQAAAACAiQjeAAAAAACYiOANAAAAAICJCN4AAAAAAJiI4A0AAAAAgIkI3gAAAAAAmIjgDQAAAACAiQjeAAAAAACYiOANAAAAAICJCN4AAAAAAJiI4A0AAAAAgIkI3gAAAAAAmIjgDQAAAACAiQjeAAAAAACYiOANAAAAAICJ/DJ4v/rqq4qMjFRQUJA6dOigzZs3F9g2JiZGFoslz9ftt9/ubmMYhqZPn66aNWsqODhY3bt3V0pKii8uBQAAAABQzvld8H733XcVGxurGTNmKDk5Wa1atVLPnj117NixfNvHx8crNTXV/bV9+3bZbDbddddd7jYvvPCC/v73v2vhwoXatGmTKlSooJ49e+r8+fO+uiwAAAAAQDnld8F77ty5evDBBzVq1Cg1a9ZMCxcuVEhIiBYvXpxv+ypVqqhGjRrurw0bNigkJMQdvA3D0Lx58zR16lT169dPLVu21FtvvaXffvtNa9as8eGVAQAAAADKI78K3llZWdqyZYu6d+/u3me1WtW9e3dt3LixUH0sWrRId999typUqCBJ2rdvn44cOeLRZ2hoqDp06FDoPgEAAAAA8Ja9pAu42PHjx+V0OlW9enWP/dWrV9evv/56xc9v3rxZ27dv16JFi9z7jhw54u7j0j4vHLtUZmamMjMz3dsZGRmSpBMnThTuQgA/lJ2drXPnzik9PV0BAQElXQ7gFcYxygLGMcoKxjLKggsZzzAMU8/jV8G7qBYtWqQWLVqoffv2Repn9uzZevrpp/Psb9KkSZH6BQAAAAD4n/T0dIWGhprWv18F76pVq8pms+no0aMe+48ePaoaNWpc9rNnz57VypUrNWvWLI/9Fz539OhR1axZ06PP1q1b59vXlClTFBsb694+deqU6tevrwMHDpj6lwGY6fTp06pbt64OHjyoa665pqTLAbzCOEZZwDhGWcFYRlmQkZGhevXqqUqVKqaex6+Ct8PhULt27ZSQkKD+/ftLklwulxISEjRhwoTLfva9995TZmamhg8f7rG/QYMGqlGjhhISEtxB+/Tp09q0aZPGjh2bb1+BgYEKDAzMsz80NJQfKij1rrnmGsYxSj3GMcoCxjHKCsYyygKr1dzlz/wqeEtSbGys7rvvPt1www1q37695s2bp7Nnz2rUqFGSpBEjRqh27dqaPXu2x+cWLVqk/v37Kzw83GO/xWLRpEmT9Oyzz6px48Zq0KCBpk2bplq1arnDPQAAAAAAZvG74D1kyBClpaVp+vTpOnLkiFq3bq1169a5F0c7cOBAnt9G7Ny5U1999ZXWr1+fb59PPPGEzp49qzFjxujUqVPq3Lmz1q1bp6CgINOvBwAAAABQvvld8JakCRMmFDi1PDExMc++pk2bXnYVOovFolmzZuV5/ruwAgMDNWPGjHynnwOlBeMYZQHjGGUB4xhlBWMZZYGvxrHFMHvddAAAAAAAyjFznyAHAAAAAKCcI3gDAAAAAGAigjcAAAAAACYqt8H71VdfVWRkpIKCgtShQwdt3ry5wLYxMTGyWCx5vm6//XZ3G8MwNH36dNWsWVPBwcHq3r27UlJSfHEpKMeKexyPHDkyz/FevXr54lJQjl3NOJakefPmqWnTpgoODlbdunX1yCOP6Pz580XqEyiq4h7HM2fOzPPz+NprrzX7MlDOXc04zs7O1qxZs9SwYUMFBQWpVatWWrduXZH6BIpDcY/jYvt5bJRDK1euNBwOh7F48WLj559/Nh588EGjcuXKxtGjR/Ntn56ebqSmprq/tm/fbthsNuPNN990t5kzZ44RGhpqrFmzxvjhhx+Mvn37Gg0aNDD++OMPH10VyhszxvF9991n9OrVy6PdiRMnfHRFKI+udhwvX77cCAwMNJYvX27s27fP+PTTT42aNWsajzzyiNd9AkVlxjieMWOG0bx5c4+fx2lpab66JJRDVzuOn3jiCaNWrVrGxx9/bOzZs8eYP3++ERQUZCQnJ3vdJ1BUZozj4vp5XC6Dd/v27Y3x48e7t51Op1GrVi1j9uzZhfr83/72N6NSpUrGmTNnDMMwDJfLZdSoUcN48cUX3W1OnTplBAYGGitWrCje4oH/Ke5xbBi5wbtfv37FXSpQoKsdx+PHjzduueUWj32xsbFGp06dvO4TKCozxvGMGTOMVq1amVIvkJ+rHcc1a9Y0XnnlFY99AwcONIYNG+Z1n0BRmTGOi+vncbmbap6VlaUtW7aoe/fu7n1Wq1Xdu3fXxo0bC9XHokWLdPfdd6tChQqSpH379unIkSMefYaGhqpDhw6F7hO4GmaM4wsSExNVrVo1NW3aVGPHjlV6enqx1g5c4M04vummm7Rlyxb3tLG9e/dq7dq16t27t9d9AkVhxji+ICUlRbVq1VJUVJSGDRumAwcOmHchKNe8GceZmZkKCgry2BccHKyvvvrK6z6BojBjHF9QHD+Py13wPn78uJxOp6pXr+6xv3r16jpy5MgVP79582Zt375do0ePdu+78Dlv+wSulhnjWJJ69eqlt956SwkJCXr++ef1xRdf6LbbbpPT6SzW+gHJu3E8dOhQzZo1S507d1ZAQIAaNmyomJgYPfnkk173CRSFGeNYkjp06KAlS5Zo3bp1WrBggfbt26fo6Gj9/vvvpl4PyidvxnHPnj01d+5cpaSkyOVyacOGDYqPj1dqaqrXfQJFYcY4lorv53G5C95FtWjRIrVo0ULt27cv6VIArxU0ju+++2717dtXLVq0UP/+/fXRRx/pu+++U2JiYskUClwiMTFRzz33nObPn6/k5GTFx8fr448/1jPPPFPSpQGFVphxfNttt+muu+5Sy5Yt1bNnT61du1anTp3SqlWrSrBy4P+Li4tT48aNde2118rhcGjChAkaNWqUrFbiBUqPwozj4vp5XO7+n1G1alXZbDYdPXrUY//Ro0dVo0aNy3727NmzWrlypR544AGP/Rc+502fgDfMGMf5iYqKUtWqVbV79+4i1Qvkx5txPG3aNN17770aPXq0WrRooQEDBui5557T7Nmz5XK5ivT/DcAbZozj/FSuXFlNmjTh5zFM4c04joiI0Jo1a3T27Fn997//1a+//qqKFSsqKirK6z6BojBjHOfH25/H5S54OxwOtWvXTgkJCe59LpdLCQkJ6tix42U/+9577ykzM1PDhw/32N+gQQPVqFHDo8/Tp09r06ZNV+wT8IYZ4zg/hw4dUnp6umrWrFnkmoFLeTOOz507l+duis1mk5T7Wsei/H8D8IYZ4zg/Z86c0Z49e/h5DFMU5WdnUFCQateurZycHL3//vvq169fkfsEvGHGOM6P1z+Pi7w8Wym0cuVKIzAw0FiyZInxyy+/GGPGjDEqV65sHDlyxDAMw7j33nuNyZMn5/lc586djSFDhuTb55w5c4zKlSsbH3zwgfHjjz8a/fr143ViMFVxj+Pff//deOyxx4yNGzca+/btMz777DOjbdu2RuPGjY3z58+bfj0on652HM+YMcOoVKmSsWLFCmPv3r3G+vXrjYYNGxqDBw8udJ9AcTNjHD/66KNGYmKisW/fPuPrr782unfvblStWtU4duyYz68P5cPVjuNvv/3WeP/99409e/YYX375pXHLLbcYDRo0ME6ePFnoPoHiZsY4Lq6fx+UyeBuGYfzjH/8w6tWrZzgcDqN9+/bGt99+6z7WpUsX47777vNo/+uvvxqSjPXr1+fbn8vlMqZNm2ZUr17dCAwMNLp162bs3LnTzEsAinUcnzt3zrj11luNiIgIIyAgwKhfv77x4IMP8o8jTHc14zg7O9uYOXOm0bBhQyMoKMioW7euMW7cOI9/IK/UJ2CG4h7HQ4YMMWrWrGk4HA6jdu3axpAhQ4zdu3f78IpQHl3NOE5MTDSuu+46IzAw0AgPDzfuvfde4/Dhw1fVJ2CG4h7HxfXz2GIYBcxpAgAAAAAARVbunvEGAAAAAMCXCN4AAAAAAJiI4A0AAAAAgIkI3gAAAAAAmIjgDQAAAACAiQjeAAAAAACYiOANAAAAAICJCN4AAAAAAJiI4A0AgJ/bv3+/LBaLZs6c6XUfI0eOlMViKb6ifKi8Xz8AoPQjeAMAcJUsFkuhv/bv31/S5fqlDz/8UAMHDlSdOnUUGBioihUr6rrrrtMDDzyg//znPyVdHgAAxcpiGIZR0kUAAFCaLFu2zGM7KSlJr7/+usaMGaPo6GiPYwMGDFCFChWKdD7DMJSZmSm73S673e5VH9nZ2XI6nQoKCipSLUX1xx9/aOjQoVqzZo2aNm2qgQMHKioqSk6nU7t27dJHH32kXbt26Z133tE999wjqWxdPwCgfCJ4AwBQREuWLNGoUaP05ptvauTIkZdt+/vvv6tSpUq+KcwPjRw5UkuXLtXjjz+uOXPmyGr1nHxnGIb+9a9/KSQkRL169SqhKgEAKF5MNQcAwCSRkZGKiYnR1q1b1bNnT4WGhqply5aScgP41KlT1aFDB1WtWlWBgYFq1KiRJk+erHPnznn0k98zzhfv++ijj/SnP/1JQUFBqlmzph5//HHl5OR49JHfM84X9mVkZGjs2LGqVq2agoKC1KlTJ23atCnP9aSnp+v+++9XeHi4KlasqFtuuUVbt25VTEyMIiMjr/j9+PHHH7V06VJ16tRJzz//fJ7QLeVO4x84cKBH6Dbr+gEA8BXv5msBAIBCOXDggG655RbddddduvPOO3XmzBlJ0uHDh/XPf/5Td955p4YOHSq73a4vvvhCL7zwgrZu3apPP/20UP2vXbtW8+fP18MPP6z7779fH3zwgV566SWFhYXpySefLFQfPXv2VEREhKZPn6709HTNnTtXt99+u/bt2+e+O5+Zmanu3btr27ZtGjlypNq3b68ff/xR3bt3V5UqVQp1nvfff1+S9MADDxRbCC6O6wcAwGwEbwAATLRv3z698cYbGj16tMf+qKgoHTx4UAEBAe5948eP17Rp0/Tss89q8+bNat++/RX7//nnn/Xzzz+77zg//PDDatGihf7xj38UOni2bdtW8+fPd283a9ZMgwcP1jvvvKOHHnpIkrRo0SJt27ZNzz77rJ566il32xYtWmj8+PGqX7/+Fc+zfft2SVLr1q3zHDtx4oRcLpd72+Fw6Jprrrlin8Vx/QAAmI2p5gAAmKhKlSoaNWpUnv0Oh8MdunNycnTy5EkdP35c3bt3l6R8p3rnp3///h7TvC0Wi7p27aojR464765fySOPPOKxfcstt0iSUlJS3Ps+/PBD2Ww2TZw40aPt6NGjFRoaWqjznD59WpLyDdRNmjRRRESE+2vo0KGF6rM4rh8AALNxxxsAABM1bNhQNpst32Pz58/XwoUL9fPPP3vc7ZWkkydPFqr/qKioPPvCw8Ml5T6TXbFixavu4+LPX7Bv3z7VqlUrT38Oh0MNGjQoVL0XAveFAH6x+Ph4ZWVlSZJ69Ohxxb4Kqv3S+gtz/QAAmI3gDQCAiUJCQvLdP3fuXD366KO69dZb9ec//1m1atWSw+HQ4cOHNXLkyDxBvCAFhXopd4XwovRR3C8+uf766xUfH69t27apTZs2Hsduvvlmr/osjusHAMBsTDUHAKAEvP3224qMjNQnn3yi0aNHq3fv3urevbuqV69e0qXlKzIyUr/99lue6dvZ2dnat29fofq48847JeU+L04oBgCUJwRvAABKgM1mk8Vi8QigOTk5mjNnTglWVbA+ffrI6XQqLi7OY/8bb7yhjIyMQvXRsmVLjRgxQl9//bUmT56c7119AjkAoCxiqjkAACVg0KBBmjJlim677TYNHDhQp0+f1jvvvOOxyrk/GT16tF577TVNnTpVu3fvdr9ObNWqVWrUqFGe92YXZOHChcrIyNALL7ygDz74QAMHDlRUVJSys7N14MABrV69WpLUoEEDMy8HAACfIngDAFACHn/8cRmGoUWLFmnixImqUaOGhgwZolGjRqlZs2YlXV4egYGBSkhI0OOPP64PPvhAq1atUocOHZSQkKDRo0fr3LlzheonODhY//rXv/Tvf/9bS5Ys0dKlS5WWlqaAgADVrVtX0dHRev3119W1a1eTrwgAAN+xGMzpAgAAXnI6napatao6dOigdevWlXQ5AAD4JZ7xBgAAhfLHH3/k2bdw4UKdOnXqql4BBgBAecMdbwAAUCjDhw/X+fPnddNNNykwMFAbN27UO++8o4YNGyo5OVmVKlUq6RIBAPBLBG8AAFAob731ll599VXt2rVLZ86cUfXq1dW7d28988wzfvsaNAAA/AHBGwAAAAAAE/GMNwAAAAAAJiJ4AwAAAABgIoI3AAAAAAAmIngDAAAAAGAigjcAAAAAACYieAMAAAAAYCKCNwAAAAAAJiJ4AwAAAABgIoI3AAAAAAAm+n8KpUJu6lj37AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# In-time testing Gini\n",
    "plt.scatter(results_final['In_Time_Train_Gini'], results_final['In_Time_Test_Gini'], color='#000066', label='Out-of-sample')\n",
    "\n",
    "# Out-of-time testing Gini\n",
    "plt.scatter(results_final['In_Time_Train_Gini'], results_final['Out_of_Time_Test_Gini'], color='#ff6200', label='Out-of-time')\n",
    "\n",
    "# Add lollipop lines\n",
    "for index, row in results_final.iterrows():\n",
    "    plt.plot([row['In_Time_Train_Gini'], row['In_Time_Train_Gini']], [row['In_Time_Test_Gini'], row['Out_of_Time_Test_Gini']], color='gray', linestyle='--', linewidth=0.8)\n",
    "    plt.text(row['In_Time_Train_Gini'] - 0.002, row['In_Time_Test_Gini'] + 0.002, row['Model_Name'], ha='right', va='bottom', fontsize=8)\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Training Gini', fontsize=13)\n",
    "plt.ylabel('Testing Gini', fontsize=13)\n",
    "plt.title('Gini coefficient', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "\n",
    "# Set limits for x and y axes\n",
    "plt.xlim(0.7, 0.95)\n",
    "plt.ylim(0.7, 0.95)\n",
    "\n",
    "# Show plot\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapley value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VIF_woe = pd.read_csv('../data/processed/df_VIF_woe.csv')\n",
    "results_vif_LR = pd.read_csv('../data/results/results_vif_LR.csv')\n",
    "results_vif_xgb = pd.read_csv('../data/results/results_vif_xgb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_dict = {'Var_03_woe': 'Cash And Cash Equivalents', \n",
    "            'Var_08_woe': 'Earnings Before Interest And Tax', \n",
    "            'Var_25_woe': 'Quick Ratio', \n",
    "            'Var_04_woe': 'Current Ratio',\n",
    "            'Var_31_woe': 'Sales / Total Assets', \n",
    "            'Var_15_woe': 'Gross Profit Margin', \n",
    "            'Var_05_woe': 'Debt / Net Worth', \n",
    "            'Var_28_woe': 'Return On Total Equity Reserve',\n",
    "            'Var_27_woe': 'Return On Assets', \n",
    "            'Var_37_woe': 'Total Net Debt EBITDA', \n",
    "            'Var_13_woe': 'Financial Leverage', \n",
    "            'Var_35_woe': 'Total Liabilities / Total Assets',\n",
    "            'Var_33_woe': 'Senior Net Debt', \n",
    "            'Var_23_woe': 'Net Profit Margin', \n",
    "            'Var_07_woe': 'EBITDA Margin'\n",
    "            }\n",
    "\n",
    "# change colnames in df_VIF_woe according to var_dict\n",
    "df_VIF_woe.rename(columns=var_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cash And Cash Equivalents</th>\n",
       "      <th>Current Ratio</th>\n",
       "      <th>Debt / Net Worth</th>\n",
       "      <th>EBITDA Margin</th>\n",
       "      <th>Earnings Before Interest And Tax</th>\n",
       "      <th>Financial Leverage</th>\n",
       "      <th>Gross Profit Margin</th>\n",
       "      <th>Net Profit Margin</th>\n",
       "      <th>Quick Ratio</th>\n",
       "      <th>Return On Assets</th>\n",
       "      <th>Return On Total Equity Reserve</th>\n",
       "      <th>Sales / Total Assets</th>\n",
       "      <th>Senior Net Debt</th>\n",
       "      <th>Total Liabilities / Total Assets</th>\n",
       "      <th>Total Net Debt EBITDA</th>\n",
       "      <th>default</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.601304</td>\n",
       "      <td>-0.242243</td>\n",
       "      <td>0.050486</td>\n",
       "      <td>-0.096086</td>\n",
       "      <td>1.927932</td>\n",
       "      <td>0.123224</td>\n",
       "      <td>0.054943</td>\n",
       "      <td>0.971286</td>\n",
       "      <td>-0.572399</td>\n",
       "      <td>0.366856</td>\n",
       "      <td>0.442086</td>\n",
       "      <td>0.663223</td>\n",
       "      <td>-0.000282</td>\n",
       "      <td>0.119413</td>\n",
       "      <td>-0.090043</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.382690</td>\n",
       "      <td>-0.242243</td>\n",
       "      <td>0.050486</td>\n",
       "      <td>-0.096086</td>\n",
       "      <td>2.747950</td>\n",
       "      <td>0.123224</td>\n",
       "      <td>0.054943</td>\n",
       "      <td>-0.120325</td>\n",
       "      <td>-0.144539</td>\n",
       "      <td>0.366856</td>\n",
       "      <td>0.442086</td>\n",
       "      <td>0.663223</td>\n",
       "      <td>-0.000282</td>\n",
       "      <td>0.119413</td>\n",
       "      <td>-0.090043</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.172870</td>\n",
       "      <td>-0.242243</td>\n",
       "      <td>0.050486</td>\n",
       "      <td>-0.096086</td>\n",
       "      <td>2.747950</td>\n",
       "      <td>-0.063886</td>\n",
       "      <td>-0.228062</td>\n",
       "      <td>-0.120325</td>\n",
       "      <td>-0.572399</td>\n",
       "      <td>-0.516452</td>\n",
       "      <td>-0.604011</td>\n",
       "      <td>0.178431</td>\n",
       "      <td>-0.247608</td>\n",
       "      <td>-0.065835</td>\n",
       "      <td>-0.090043</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.172870</td>\n",
       "      <td>-0.242243</td>\n",
       "      <td>-0.128911</td>\n",
       "      <td>-0.096086</td>\n",
       "      <td>2.747950</td>\n",
       "      <td>-0.063886</td>\n",
       "      <td>-0.228062</td>\n",
       "      <td>-0.120325</td>\n",
       "      <td>-0.572399</td>\n",
       "      <td>-0.516452</td>\n",
       "      <td>-0.976500</td>\n",
       "      <td>0.178431</td>\n",
       "      <td>-0.000282</td>\n",
       "      <td>-0.065835</td>\n",
       "      <td>-0.090043</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.382690</td>\n",
       "      <td>-0.242243</td>\n",
       "      <td>-0.467310</td>\n",
       "      <td>-0.096086</td>\n",
       "      <td>1.927932</td>\n",
       "      <td>-0.063886</td>\n",
       "      <td>0.088974</td>\n",
       "      <td>-0.120325</td>\n",
       "      <td>-0.144539</td>\n",
       "      <td>-0.212252</td>\n",
       "      <td>-0.290762</td>\n",
       "      <td>-0.787109</td>\n",
       "      <td>-0.247608</td>\n",
       "      <td>-0.065835</td>\n",
       "      <td>-0.311843</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125753</th>\n",
       "      <td>1.601304</td>\n",
       "      <td>-0.242243</td>\n",
       "      <td>0.044977</td>\n",
       "      <td>-0.096086</td>\n",
       "      <td>1.927932</td>\n",
       "      <td>-0.063886</td>\n",
       "      <td>0.396170</td>\n",
       "      <td>-0.120325</td>\n",
       "      <td>0.117773</td>\n",
       "      <td>-0.004348</td>\n",
       "      <td>-0.290762</td>\n",
       "      <td>-0.787109</td>\n",
       "      <td>-0.247608</td>\n",
       "      <td>-0.065835</td>\n",
       "      <td>-0.311843</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125754</th>\n",
       "      <td>1.601304</td>\n",
       "      <td>-0.242243</td>\n",
       "      <td>0.218554</td>\n",
       "      <td>-0.096086</td>\n",
       "      <td>1.213114</td>\n",
       "      <td>-0.227442</td>\n",
       "      <td>0.396170</td>\n",
       "      <td>0.140213</td>\n",
       "      <td>-0.144539</td>\n",
       "      <td>0.366856</td>\n",
       "      <td>0.442086</td>\n",
       "      <td>-0.787109</td>\n",
       "      <td>-0.247608</td>\n",
       "      <td>-0.231364</td>\n",
       "      <td>-0.090043</td>\n",
       "      <td>0</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125755</th>\n",
       "      <td>-2.027166</td>\n",
       "      <td>0.001806</td>\n",
       "      <td>-0.467310</td>\n",
       "      <td>-0.096086</td>\n",
       "      <td>-2.905559</td>\n",
       "      <td>-0.063886</td>\n",
       "      <td>0.396170</td>\n",
       "      <td>-0.120325</td>\n",
       "      <td>-0.144539</td>\n",
       "      <td>-0.212252</td>\n",
       "      <td>-0.604011</td>\n",
       "      <td>-0.787109</td>\n",
       "      <td>-0.247608</td>\n",
       "      <td>-0.065835</td>\n",
       "      <td>0.091239</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125756</th>\n",
       "      <td>-2.027166</td>\n",
       "      <td>-0.242243</td>\n",
       "      <td>-0.467310</td>\n",
       "      <td>-0.096086</td>\n",
       "      <td>-2.905559</td>\n",
       "      <td>-0.227442</td>\n",
       "      <td>0.396170</td>\n",
       "      <td>-0.120325</td>\n",
       "      <td>-0.144539</td>\n",
       "      <td>-0.004348</td>\n",
       "      <td>-0.976500</td>\n",
       "      <td>-0.787109</td>\n",
       "      <td>-0.247608</td>\n",
       "      <td>-0.231364</td>\n",
       "      <td>-0.311843</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125757</th>\n",
       "      <td>-2.027166</td>\n",
       "      <td>0.413951</td>\n",
       "      <td>-0.467310</td>\n",
       "      <td>-0.096086</td>\n",
       "      <td>-2.905559</td>\n",
       "      <td>0.391178</td>\n",
       "      <td>-0.228062</td>\n",
       "      <td>-0.120325</td>\n",
       "      <td>0.477327</td>\n",
       "      <td>-1.028597</td>\n",
       "      <td>-0.604011</td>\n",
       "      <td>-0.787109</td>\n",
       "      <td>0.187335</td>\n",
       "      <td>0.376593</td>\n",
       "      <td>0.091239</td>\n",
       "      <td>0</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125758 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Cash And Cash Equivalents  Current Ratio  Debt / Net Worth  \\\n",
       "0                        1.601304      -0.242243          0.050486   \n",
       "1                        2.382690      -0.242243          0.050486   \n",
       "2                        1.172870      -0.242243          0.050486   \n",
       "3                        1.172870      -0.242243         -0.128911   \n",
       "4                        2.382690      -0.242243         -0.467310   \n",
       "...                           ...            ...               ...   \n",
       "125753                   1.601304      -0.242243          0.044977   \n",
       "125754                   1.601304      -0.242243          0.218554   \n",
       "125755                  -2.027166       0.001806         -0.467310   \n",
       "125756                  -2.027166      -0.242243         -0.467310   \n",
       "125757                  -2.027166       0.413951         -0.467310   \n",
       "\n",
       "        EBITDA Margin  Earnings Before Interest And Tax  Financial Leverage  \\\n",
       "0           -0.096086                          1.927932            0.123224   \n",
       "1           -0.096086                          2.747950            0.123224   \n",
       "2           -0.096086                          2.747950           -0.063886   \n",
       "3           -0.096086                          2.747950           -0.063886   \n",
       "4           -0.096086                          1.927932           -0.063886   \n",
       "...               ...                               ...                 ...   \n",
       "125753      -0.096086                          1.927932           -0.063886   \n",
       "125754      -0.096086                          1.213114           -0.227442   \n",
       "125755      -0.096086                         -2.905559           -0.063886   \n",
       "125756      -0.096086                         -2.905559           -0.227442   \n",
       "125757      -0.096086                         -2.905559            0.391178   \n",
       "\n",
       "        Gross Profit Margin  Net Profit Margin  Quick Ratio  Return On Assets  \\\n",
       "0                  0.054943           0.971286    -0.572399          0.366856   \n",
       "1                  0.054943          -0.120325    -0.144539          0.366856   \n",
       "2                 -0.228062          -0.120325    -0.572399         -0.516452   \n",
       "3                 -0.228062          -0.120325    -0.572399         -0.516452   \n",
       "4                  0.088974          -0.120325    -0.144539         -0.212252   \n",
       "...                     ...                ...          ...               ...   \n",
       "125753             0.396170          -0.120325     0.117773         -0.004348   \n",
       "125754             0.396170           0.140213    -0.144539          0.366856   \n",
       "125755             0.396170          -0.120325    -0.144539         -0.212252   \n",
       "125756             0.396170          -0.120325    -0.144539         -0.004348   \n",
       "125757            -0.228062          -0.120325     0.477327         -1.028597   \n",
       "\n",
       "        Return On Total Equity Reserve  Sales / Total Assets  Senior Net Debt  \\\n",
       "0                             0.442086              0.663223        -0.000282   \n",
       "1                             0.442086              0.663223        -0.000282   \n",
       "2                            -0.604011              0.178431        -0.247608   \n",
       "3                            -0.976500              0.178431        -0.000282   \n",
       "4                            -0.290762             -0.787109        -0.247608   \n",
       "...                                ...                   ...              ...   \n",
       "125753                       -0.290762             -0.787109        -0.247608   \n",
       "125754                        0.442086             -0.787109        -0.247608   \n",
       "125755                       -0.604011             -0.787109        -0.247608   \n",
       "125756                       -0.976500             -0.787109        -0.247608   \n",
       "125757                       -0.604011             -0.787109         0.187335   \n",
       "\n",
       "        Total Liabilities / Total Assets  Total Net Debt EBITDA  default  year  \n",
       "0                               0.119413              -0.090043        0  2015  \n",
       "1                               0.119413              -0.090043        1  2016  \n",
       "2                              -0.065835              -0.090043        0  2015  \n",
       "3                              -0.065835              -0.090043        1  2016  \n",
       "4                              -0.065835              -0.311843        1  2015  \n",
       "...                                  ...                    ...      ...   ...  \n",
       "125753                         -0.065835              -0.311843        0  2020  \n",
       "125754                         -0.231364              -0.090043        0  2021  \n",
       "125755                         -0.065835               0.091239        0  2017  \n",
       "125756                         -0.231364              -0.311843        0  2016  \n",
       "125757                          0.376593               0.091239        0  2019  \n",
       "\n",
       "[125758 rows x 17 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_VIF_woe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete observations after first iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['Net Profit Margin', 'Return On Total Equity Reserve']\n",
    "df_VIF_woe_dropped = df_VIF_woe.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_VIF_woe_dropped.to_csv('../data/processed/df_VIF_woe_dropped.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VIF_woe_dropped = pd.read_csv('../data/processed/df_VIF_woe_dropped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_config \u001b[38;5;241m=\u001b[39m results_vif_LR\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel_to_shapley\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_VIF_woe_dropped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 16\u001b[0m, in \u001b[0;36mmodel_to_shapley\u001b[0;34m(df, model, target, time)\u001b[0m\n\u001b[1;32m     12\u001b[0m X_in_time_train, X_in_time_test, y_in_time_train, y_in_time_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m     13\u001b[0m     X_in_time, y_in_time, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, stratify\u001b[38;5;241m=\u001b[39my_in_time, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m(X_in_time_train, y_in_time_train)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Create a Linear explainer and calculate Shapley values\u001b[39;00m\n\u001b[1;32m     19\u001b[0m explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mLinearExplainer(model, X_in_time_train, feature_perturbation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minterventional\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "model_config = results_vif_LR.iloc[0,0]\n",
    "model_to_shapley(df_VIF_woe_dropped, model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"LogisticRegression(C=1, class_weight='balanced', penalty='l1', random_state=42,\\n                   solver='liblinear')\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
